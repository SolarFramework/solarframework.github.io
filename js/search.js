// builds lunr
var index = lunr(function () {
  this.field('title');
  this.field('content', {boost: 10});
  this.ref('id');
});
 


  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "GIT",
      "content": "GIT\n\nTable of Contents\n\nGIT repositories\nAccess to code source here\n\nThe direct links to repositories\n\n\nAccess to Pull Requests\nAccess to issues\n\n\n\nGIT repositories\n\n\n\n\n\nAccess to code source here\n\n\nTo get access to the list of all SolAR repositories: https://github.com/SolarFramework\n\n\nThe direct links to repositories\n\n\n\n\n\n\n\n\n\nName\nGitHub url\n\n\nFramework\nSolARFramework\nhttps://github.com/SolarFramework/SolARFramework\n\n\nSolARFrameworkGRPCRemote\nhttps://github.com/SolarFramework/SolARFrameworkGRPCRemote\n\n\nSolARPipelineManager\nhttps://github.com/SolarFramework/SolARPipelineManager\n\n\nSolARWrapper\nhttps://github.com/SolarFramework/SolARWrapper\n\n\nModules\nSolARModuleCeres\nhttps://github.com/SolarFramework/SolARModuleCeres\n\n\nSolARModuleFBOW\nhttps://github.com/SolarFramework/SolARModuleFBOW\n\n\nSolARModuleG2O\nhttps://github.com/SolarFramework/SolARModuleG2O\n\n\nSolARModuleNonFreeOpenCV\nhttps://github.com/SolarFramework/SolARModuleNonFreeOpenCV\n\n\nSolARModuleOpenCV\nhttps://github.com/SolarFramework/SolARModuleOpenCV\n\n\nSolARModuleOpenGL\nhttps://github.com/SolarFramework/SolARModuleOpenGL\n\n\nSolARModuleOpenGV\nhttps://github.com/SolarFramework/SolARModuleOpenGV\n\n\nSolARModulePCL\nhttps://github.com/SolarFramework/SolARModulePCL\n\n\nSolARModulePopSift\nhttps://github.com/SolarFramework/SolARModulePopSift\n\n\nSolARModuleRealSense\nhttps://github.com/SolarFramework/SolARModuleRealSense\n\n\nSolARModuleTools\nhttps://github.com/SolarFramework/SolARModuleTools\n\n\nSamples\nSample-FiducialMarker\nhttps://github.com/SolarFramework/Sample-FiducialMarker\n\n\nSample-NaturalImageMarker\nhttps://github.com/SolarFramework/Sample-NaturalImageMarker\n\n\nSample-DepthCamera\nhttps://github.com/SolarFramework/Sample-DepthCamera\n\n\nSample-MapUpdate\nhttps://github.com/SolarFramework/Sample-MapUpdate\n\n\nSample-Mapping\nhttps://github.com/SolarFramework/Sample-Mapping\n\n\nSample-Relocalization\nhttps://github.com/SolarFramework/Sample-Relocalization\n\n\nSample-Triangulation\nhttps://github.com/SolarFramework/Sample-Triangulation\n\n\nSample-Slam\nhttps://github.com/SolarFramework/Sample-Slam\n\n\nServices\nService-Mapping\nhttps://github.com/SolarFramework/Service-Mapping\n\n\nService-MapUpdate\nhttps://github.com/SolarFramework/Service-MapUpdate\n\n\nService-Relocalization\nhttps://github.com/SolarFramework/Service-Relocalization\n\n\n\n\n\n\n\nAccess to Pull Requests\n\n\nhttps://solarframework.github.io/SolAR-githubPullRequests.html\n\n\n\n\nAccess to issues\n\n\n\n\nFor open issues visit https://github.com/issues?q=user%3ASolarFramework+is%3Aopen\n\n\n\n\n",
      "id": 0
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "About",
      "content": "About\n\nAbout us\n\n\n\n\n\nWho initiated Solar ?\n\n\nThe SolAR initiative was launched by the b&lt;&gt;com Institute of Research and Technology, and is open to any contributors or users who share the SolAR goals.\n\n\n\n\n\n\n\n\n\nOur first Contributors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContact US\n\n\nFor any requests, please contact us.\n\n\n",
      "id": 1
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "for Android development",
      "content": "\nTable of Contents\n\nInstall for Android\nTest SolAR\nInstall development environment\n\nInstall Qt Creator IDE\nInstall Android SDK and NDK\nConfigure Qt Android Kit\n\n\nBuild SolAR\n\nClone SolAR\nBuild with scripts\nBuild on Qt Creator\n\n\nUnity SolAR Android\n\n\n\nInstall for Android\n\n\n\n\n\nTest SolAR\n\n\nWe provide you with an Android demo app to showcase 3 pipelines: fiducial and natural image marker-based pipelines and a SLAM.\n\n\nTable 1. Available Samples\n\n\n\n\n\n\nSolAR Demo (Fiducial, Natural, SLAM)\nv0.11.0 download link\n\n\n\n\n\n\n\n\n\n\nYou will find the required markers for you to print on the link above.\n\n\n\n\n\n\n\nInstall development environment\n\n\nInstall Qt Creator IDE\n\nThe SolAR framework uses a dedicated pipeline to link and compile code as well as internal tools for third party downloads which should make your job much easier.\nAs you will see later, the SolAR framework is based on QMake originally created by the Qt Company, but compliant with most IDE.\n\n\nRefer to your OS section to install your development environment, whether on Windows or Linux.\n\n\n\n\n\n\n\n\nDo not forget to install Qt Android component. SolAR supports arm64-v8a architecture only.\n\n\n\n\n\n\n\n\n\n\n\nIt is recommended to install IDE for your own OS to test directly your pipeline on your system and then compile it for Android\n\n\n\n\n\n\nInstall Android SDK and NDK\n\nOnce Qt Creator installed we can install dependencies required by Qt Creator for Android :\n\n\n\n\n\n\n\n\nWe recommend you to install these dependencies using Unity Android build support module (available from Unity HUB, see build support)\nOtherwise you can also directly download the dependencies in Qt Creator from SDKManager in Android tab of Tools&gt;Options&gt;Devices\n\n\n\n\n\n\n\nJava SDK\n\n\nAndroid SDK\n\n\nAndroid NDK\n\n\n\n\nYou can find more information in Qt documentation : Android building\n\n\n\nConfigure Qt Android Kit\n\nIn Qt Creator navigation bar go to Tools &gt; Options &gt; Devices.\nThen fill required path and check if your Android kit is valid :\n\n\n\n\n\nFigure 1. Android kit configuration\n\n\nInstall SolAR dependencies\n\nRefer to your OS section to install Remaken, whether on Windows or Linux.\n\n\n\nSet Remaken for Android\n\nYou need to configure your Remaken profile according to your development environment. To do so, run the following command in a command prompt:\n\n\n\nremaken profile init --cpp-std 17 -b clang -o android -a arm64-v8a\n\n\n\nYou can check if the change have been done in your Remaken profile using :\n\n\n\nremaken profile display\n\n\n\n\nSet Conan profile for Android\n\nAs we are using Conan to download dependencies, you need to configure a conan profile for Android. Remaken uses the following naming convention for the conan profile: os-compiler-arch. Thus, when Remaken calls conan install, it can pass the good profile to conan according to the targeted os, compiler and architecture.\nIf you want to create a conan profile for android, with clang compiler and arm64_v8a architecture, run the following command:\n\n\n\nconan profile new --force --detect android-clang-arm64-v8a ;\nconan profile update settings.os=Android android-clang-arm64-v8a ;\nconan profile update settings.arch=armv8 android-clang-arm64-v8a ;\nconan profile update settings.compiler=clang android-clang-arm64-v8a ;\nconan profile update settings.compiler.version=8 android-clang-arm64-v8a ;\nconan profile update settings.compiler.libcxx=libc++ android-clang-arm64-v8a ;\nconan profile update settings.os.api_level=21 android-clang-arm64-v8a ;\nconan profile update settings.compiler.cppstd=17 android-clang-arm64-v8a\nconan profile update settings.ceres-solver.build_type=Release\n\n\n\n\n\n\n\n\n\nos_build should match your OS (Windows or Linux)\n\n\n\n\n\nIf you show your profile by running:\n\n\n\nconan profile show android-clang-arm64-v8a\n\n\n\nYou should obtain the following result:\n\n\n\nConfiguration for profile android-clang-arm64-v8a:\n\n[settings]\nos=Android\nos_build=Linux\narch=armv8\narch_build=x86_64\ncompiler=clang\ncompiler.version=8\ncompiler.libcxx=libc++\nbuild_type=Release\nos.api_level=21\ncompiler.cppstd=17\n[options]\n[conf]\n[build_requires]\n[env]\n\n\n\nNow, you are ready to download your dependencies with Remaken.\n\n\n\n\n\n\n\n\nceres_solver package has problems with glog in debug mode, that is why Release build_type is forced for it in the conan profile.\n\n\n\n\n\n\nUse Remaken\n\nRemaken uses a file called packagedependencies.txt to describe which depedencies to install, in which version, where to install them, where to download them, with which package manager and with which configuration.\n\n\nA global packagedepedencies.txt defining the common dependencies with specific packagedependencies-&lt;os&gt;.txt files defining dependencies which are specific for each os are available in the parent GIT repository SolAR, and can be downloaded on the following link:\n\n\n\n\npackagedependencies.txt\n\n\npackagedependencies-android.txt\n\n\n\n\nCopy these files where you want on your computer, open a command prompt in the folder where you have copied the packagedependencies.txt and packagedependencies-&lt;os&gt;.txt files, and run remaken with the following command:\n\n\n\nremaken install packagedependencies.txt\n\n\n\nThis command will install all SolAR dependencies in release mode.\n\n\nTo download the dependencies in debug mode, run the following command:\n\n\n\nremaken install -c debug packagedependencies.txt\n\n\n\nThis is done, all your dependencies are downloaded and ready to use !\nSome of the module will download and build third parties using Conan which requires CMake (minimum version 3.10).\n\n\n\n\n\n\nBuild SolAR\n\n\nClone SolAR\n\nSolAR is made up of a multitude of projects (SolAR Framework, SolAR pipeline manager, modules, samples, etc.). To help you, we have created a parent repository with sub-modules regrouping all source codes of SolAR projects. You can clone it from the following url:\nhttps://github.com/SolarFramework/SolAR.git\n\n\n\n\n\n\n\n\nNo space in the path of the folder where you are cloning SolAR !\n\n\n\n\n\n\ngit clone --recurse-submodules https://github.com/SolarFramework/SolAR.git\n\n\n\nIf you forgot to use --recurse-submodules, you can initialize the submodules afterwards with:\n\n\n\ncd SolAR\ngit submodule update --init --recursive\n\n\n\nIf you want to move all submodules on HEAD of master, launch the following commands\n\n\n\ngit submodule foreach --recursive git checkout master\ngit submodule foreach --recursive git pull\n\n\n\nIf you do not want to download all the source codes of SolAR, you can have a look to the different repositories available on Github on the Community/Git page.\n\n\n\nBuild with scripts\n\nBuild scripts are available in the scripts folder.\nJust run build_all.* to rebuild the SolAR framework, the modules and their tests, the pipelines and their tests, the samples, the services and their tests. If you want to rebuild them seprately, use the corresponding script. Check with build_all.* --help the different options such as the cross-build for Android, the number of processors to use, the version of Qt, the path to SolAR root folder, etc.\n\n\n\n\n\n\n\n\nLinux is the prefered platform to perform an Android cross-build.\n\n\n\n\n\n\nBuild on Qt Creator\n\nCounting the framework, the pipeline manager, modules, module tests, samples, pipelines, pipeline tests, there are more than 60 QT projects on GitHub. In order to ease the building of all these projects, they are grouped in the following parent QT projects available into the root folder of SolAR:\n\n\n\n\nSolARCore\n\n\nSolARAllModules\n\n\nSolARAllModulesTests\n\n\nSolARAllSamples\n\n\nSolARAllPipelines\n\n\nSolARAllPipelineTests\n\n\nSolARAllServices\n\n\nSolARAllServicesTests\n\n\n\n\nYou can open one of them or all in QT Creator.\n\n\nCheck by clicking on the Projects tab, and then on the Manage Kits&#8230;&#8203; button that your Qt x.x.x Android for arm64-v8a kit is well configured\n\n\nThe build step  will copy the built binaries into ${HOME}/.remaken/packages.\n\n\n\n\n\n\n\n\nDisable step Make install and Build Android APK\n\n\n\n\n\n\n\n\n\n\n\nSince QT Creator 4.14.0, we highly recommand to set the project option qmake system() behavior when parsing to Ignore to highly reduce the project loading time. But do not forget to run qmake on projects before building them.\n\n\n\n\n\n\n\n\nFigure 2. Edit build settings\n\n\nIf you open several projects, you will have to set their build order. You can do it in QT Creator by defining the dependencies of each project in the Projects menu, select your project, click on Dependencies, and check the projects that depend directly on the selected project (checking `Synchronize configuration' will synchronize all projects in Debug or Release configuration).\n\n\n\n\n\nFigure 3. Project dependencies settings\n\n\nTable 2. Project dependencies\n\n\n\n\n\n\nProject\nProject dependencies\n\n\n\n\nSolARCore\nNo dependency\n\n\nSolARAllModules\nSolARCore\n\n\nSolARAllModulesCuda\nSolARCore\n\n\nSolARAllModulesTests\nSolARAllModules\n\n\nSolARAllPipelines\nSolARAllModules\n\n\nSolARAllPipelineTests\nSolARAllPipelines\n\n\nSolARAllSamples\nSolARAllModules\n\n\nSolARAllServices\nSolARAllPipelines\n\n\nSolARAllServicesTests\nSolARAllServices\n\n\n\n\n\n\n\n\n\n\nIf you have not yet run a qmake on each project, some error could appear concerning the fact that conanbuildinfo.pri files do not exist. This is normal, run qmake on each project will create these files, and the error message will no longer appear.\n\n\n\n\n\nThen, in the Build menu, click on Rebuild All Projects for All Configurations, and go get a cup of coffee.\n\n\nNow your shared libraries can be use as a SolAR module in Unity to build an Android application.\n\n\n\n\n\nUnity SolAR Android\n\n\nTo build a SolAR Android application you need to have Android build support module installed.\n\n\n\n\n\nFigure 4. Add Android build support module from Unity HUB\n\n\nSet your Unity Android environment. Then you need to set your current target platform of Unity Editor File &gt; Build settings to Android. In the player settings you must specify a package name (ex : com.company.appname) and set the configuration to support SolAR Framework.\n\n\n\n\n\n\n\n\nSolAR modules provide support for AArch64 instruction sets through Application Binary Interface arm64-v8a. This is the most common architecture used by Android devices.\n\n\n\n\n\n\n\n\nFigure 5. Set player settings for arm64-v8a and SolAR\n\n\nNow you can import your SolAR modules and their dependencies in your Unity project in the dedicated directory ./Assets/Plugins/Android/.\n\n\n\n\nIf you haven&#8217;t built or downloaded modules and dependencies for Android, execute ./plugin/unity/SolARUnityPlugin/Install.bat.\n\n\nOtherwise you could simply execute ./plugin/unity/SolARUnityPlugin/Bundle.bat to bundle all of your Remaken packages and wrap SWIG.\n\n\n\n\n\n\n\n\n\n\nYou cannot directly test your pipeline in Unity Editor with modules built for Android. It is recommended to develop for your OS in a first time and then provide your module for Android.\n\n\n\n\n\nCongratulation, your configuration is ready to use and you can build your .apk with Unity to play it on your Android device. If necessary you can set Unity build settings options and use ADB to debug your app.\n\n\nMoreover you can edit your pipeline configuration on your device. XML is stored in your public path of the application in your internal memory (ie : /storage/emulated/0/Android/data/com.company.appname/files/StreamingAssets/SolAR/Pipelines/*.xml)\n\n\nYou can find more information for SolAR Android Unity pipeline in Unity Android Deployment\n\n\nWindows\n\n\n",
      "id": 2
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "api",
      "content": "\nTable of Contents\n\nAPI\nAPI\n\n\n\nAPI\n\n\n\n\n\nAPI\n\n\nIn order to create new components, please refer to the list of available API in the SolAR framework :\n\n\n\n",
      "id": 3
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "services",
      "content": "\nTable of Contents\n\nServices\nIntroduction\nOverview\nGlobal processing of ARCloud Services\nContents of the ARCloud Services packages\n\nThe Map Update Service package\nThe Relocalization Service package\nThe Relocalization Markers Service package\nThe Mapping Service package\nThe Mapping No Drop Service package\nThe Mapping and Relocalization Front End Service package\nThe test applications package\n\n\nOverview of computer vision pipelines\n\nThe Map Update Pipeline\nThe Relocalization Pipeline\nThe Relocalization Markers Pipeline\nThe Mapping Pipeline\n\n\nARCloud Services deployment\n\nHelm charts\nThe Map Update Service\nThe Map Update Cuda Service (GPU version)\nThe Relocalization Service\nThe Relocalization Cuda Service (GPU version)\nThe Relocalization Markers Service\nThe Mapping Service\nThe Mapping Cuda Service (GPU version)\nThe Mapping No Drop Service\nThe Mapping No Drop Cuda Service (GPU version)\nThe Mapping and Relocalization Front End Service\nkubectl commands\n\n\nARCloud Services on a local computer (with Docker engine)\nTest applications\n\nThe Map Update Service test application\nThe Map Update Display Map test application\nThe Mapping Service test application: Producer client\nThe Mapping Service test application: Viewer client\nThe Relocalization Service test application\nThe Relocalization, Mapping (and Map Update) Services test application\nThe Mapping and Relocalization Front End test application\n\n\n\n\n\nServices\n\n\n\n\n\nIntroduction\n\n\nThe purpose of this documentation is to present the ARCloud Services: a set of computer vision pipelines dedicated to Augmented Reality (AR) applications and ready to be deployed on a Cloud architecture.\n\n\nFirst, we will present the pipelines on which these remote services are based. Then, we will describe how to configure and deploy the services on your Cloud architecture. And finally, we will explain how to test these services using sample applications.\n\n\n\n\nOverview\n\n\nCurrently, ARCloud Services consist of six services, working together to provide a complete vision processing to AR client applications:\n\n\n\n\nMapping and Mapping No Drop: calculate the point cloud (sparse map) corresponding to a series of images captured by a camera, as well as the pose of the camera during its journey\n\n\nMap Update: builds and consolidates a global map (sparse map) from all the partial maps resulting from the Mapping services\n\n\nRelocalization: allows precise re-estimation of camera poses from captured images, based on the sparse submap provided by the Map Update service\n\n\nRelocalization Markers: allows precise re-estimation of camera poses from captured images, based on markers detection (fiducial markers or QR Codes)\n\n\nMapping and Relocalization Front End: front end service that relies on Map Update, Relocalization, Relocalization Markers and Mapping services to calculate the sparse map corresponding to an end user journey (Mapping), and to transpose the corresponding point cloud into the SolAR coodinate system (Relocalization/Relocalization Markers), to finally merge it into the global map (Map Update).\n\n\n\n\nThese services are therefore strongly linked, which results in data exchanges presented in the following diagram:\n\n\n\n\n\nFigure 1. ARCloud Services\n\n\nThe advantage of this set of services is to allow a user to be precisely located in the 3D environment in which he evolves, from a camera integrated into the device he uses (mixed reality device such as Hololens headset, tablet, mobile phone, etc.). More than that, this set of services is capable of gradually building an accurate map of the real world to enable increasingly precise and fast localization of AR application users. And above all, this computer vision service greatly reduces the processing costs of the user&#8217;s device, since it runs in the Cloud, on servers with much higher processing capacities.\n\n\n\n\n\n\n\n\nIn this lastest version, some services have been enhanced by GPU processing based on the Compute Unified Device Architecture (CUDA) interface provided by Nvidia. Thus, the ARCloud services set now contains the Cuda versions of the MapUpdate, Relocalization, Mapping and Mapping No Drop services. These services have been tested on Nvidia A100 and Tesla T4 GPU.\n\n\n\n\n\n\n\nGlobal processing of ARCloud Services\n\n\nThis diagram shows, in chronological order, all the processing that is carried out to process a sequence of images captured by a camera:\n\n\n\n\n\nFigure 2. ARCloud Services processing\n\n\n\n\nInitializes services by giving, among other things, the camera parameters.\n\n\nCalculates the first camera pose and the initial transformation matrix. If the global map is empty, tries to detect a marker in the captured images to determine the first camera pose (marker based reloc.) Else, tries to determine the first camera pose from captured images using point matching (sparse map based reloc.)  Deduces the initial transformation matrix from client’s coordinate system and  that of ARCloud and determines the sparse submap to use for mapping.\n\n\nDetermines the submap for mapping. Initializes the mapping service with the sparse submap previously determined to help the map merge at the end of process.\n\n\nIteratively calculates the transformation matrix. Every “n” images, tries to calculate the transformation matrix using markers based and sparse map based Relocalization services.\n\n\nSends images and transformed poses to mapping. Provides the mapping service with the images and poses given by the client, by applying the transformation matrix, to process for mapping.\n\n\nMerges the new partial sparse map. At the end of the mapping process, sends the new partial sparse map to the map update service to merge to current global map\n\n\n\n\n\n\nContents of the ARCloud Services packages\n\n\nThis set of services is delivered as six packages (one for each service) containing all the files you need to deploy these computer vision pipelines on your own Cloud architecture.\n\n\nThese packages are completed by Helm charts files giving all the information to deploy each ARCloud service on a Cloud architecture using Kubernetes, and script files to launch these services on a local computer.\n\n\nAdditionally, some test applications are also available for testing the ARCloud Services once deployed, and will be described in this document.\n\n\n\n\n\n\n\n\nThe ARCloud Service Docker images, based on Ubuntu 18.04, are completely independent from other external resources, and can be deployed on any Cloud infrastructure that supports Docker containers.\n\n\n\n\n\nThe Map Update Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-service:x.y.z)\n\n\nthe Docker image of the CUDA version of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-cuda-service:x.y.z)\n\n\n\n\n\nThe Relocalization Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-service:x.y.z)\n\n\nthe Docker image of the CUDA version of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-cuda-service:x.y.z)\n\n\n\n\n\n\n\n\n\n\nThe Relocalization Service depends on the Map Update Service, and this one must therefore already be deployed on your Cloud infrastructure (in Cuda version if needed).\n\n\n\n\n\n\nThe Relocalization Markers Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-markers-service:x.y.z)\n\n\n\n\n\nThe Mapping Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-service:x.y.z)\n\n\nthe Docker image of the CUDA version of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-cuda-service:x.y.z)\n\n\n\n\n\n\n\n\n\n\nThe Mapping Service depends on the Map Update Service and the Relocalization Service, and these must therefore already be deployed on your Cloud infrastructure (in Cuda versions if needed).\n\n\n\n\n\n\nThe Mapping No Drop Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-service:x.y.z)\n\n\nthe Docker image of the CUDA version of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-cuda-service:x.y.z)\n\n\n\n\n\n\n\n\n\n\nThe Mapping No Drop Service depends on the Map Update Service and the Relocalization Service, and these must therefore already be deployed on your Cloud infrastructure (in Cuda versions if needed).\n\n\n\n\n\n\nThe Mapping and Relocalization Front End Service package\n\nThis package includes:\n\n\n\n\nthe Docker image of the remote service, available on a public Docker repository (solar-docker-local.artifact.b-com.com/relocalization/0.11.0/mappingandrelocalizationfrontend-service:x.y.z)\n\n\n\n\n\n\n\n\n\n\nThe Mapping and Relocalization Front End Service depends on Map Update Service, Relocalization Service, Relocalization Markers Service and Mapping Service, and these must therefore already be deployed on your Cloud infrastructure (this Front End Service can refer to non Cuda or Cuda versions of other services).\n\n\n\n\n\n\nThe test applications package\n\nThis package includes:\n\n\n\n\nthe Docker images of the test applications, available on a public Docker repository (solar-docker-local.artifact.b-com.com/tests/0.11.0/&#8230;&#8203;)\n\n\nthe script files for Linux and Windows OS to launch the test applications on your own computer (available on https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/)\n\n\n\n\n\n\n\n\n\n\nThe test application images, based on Ubuntu 18.04, are completely independent from other external resources, and can be run on any computer that supports Docker engine.\n\n\n\n\n\n\n\n\nOverview of computer vision pipelines\n\n\nThese computer vision pipelines have been developed using the SolAR Framework, an open-source framework under Apache v2 license dedicated to Augmented Reality.\n\n\nThe Map Update Pipeline\n\nThe objective of this map update pipeline is to build and maintain a global map from all local maps provided by each AR device, using the Mapping Service.\n\n\nThis pipeline is able to process sparse maps made up of the following data:\n\n\n\n\nidentification: to uniquely identify a map\n\n\ncoordinate system: the coordinate system used by the map\n\n\npoint cloud: set of 3D points constituting the sparse map\n\n\nkeyframes: set of keyframes selected among captured images to approximately cover all viewpoints of the navigation space\n\n\ncovisibility graph: graph consisting of keyframes as nodes so that connections between keyframes exist if they share a minimum of common map points\n\n\nkeyframe retriever: set of nearest images (the most similar to a reference image) used to accelerate feature matching\n\n\n\n\nThis data is defined in a specific data structure Map that is detailed on the SolAR web site, in the API part: https://solarframework.github.io/create/api/\n\n\nFor each local map, the processing of this pipeline is divided into three main steps:\n\n\n\n\nThe overlap detection: If no relative transformation is known to go from the local map coordinate system to the global map coordinate system (in this case, the local map is considered as a floating map), the pipeline tries to detect overlap area between the two maps, in order to be able to calculate the transformation to be applied to process the next step.\n\n\nThe map fusion: This step allows to merge the local map into the global map, using the same coordinate system. For that, the pipeline performs the following treatments:\n\n\n\nchange local map coordinate system to that of the global map\n\n\ndetect of duplicated cloud points between the two maps\n\n\nmerge cloud points and fuse duplicated points\n\n\nmerge keyframes and update the covisibility graph\n\n\nmerge keyframe retriever models\n\n\nprocess a global bundle adjustment\n\n\n\n\n\nThe map update: This step ensures maximum consistency between the global map and the current state of the real world, by regularly updating the map to adjust to real-world evolutions (taking into account lighting conditions, moving objects, etc.).\n\n\n\n\nTo initialize the map update pipeline processing, a device must give the characteristics of the camera it uses (resolution, focal).\n\n\nThen, the pipeline is able to process maps. To do this, the local map calculated from the images and poses captured by the camera of the device must be sent to it.\n\n\nAt any time, the current global map can be requested from the Map Update pipeline, which then returns a map data structure.\n\n\nTo facilitate the use of this pipeline by any client application embedded in a device, it offers a simple interface based on the SolAR::api::pipeline::IMapUpdatePipeline class (see https://solarframework.github.io/create/api/ for interface definition and data structures).\nThis interface is defined as follows:\n\n\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode setCameraParameters(const datastructure::CameraParameters &amp; cameraParams) override;\n\n\n\n\n    /// @brief Start the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the stard succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n\n\n\n    /// @brief Stop the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the stop succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode stop() override;\n\n\n\n\n    /// @brief Request to the map update pipeline to update the global map from a local map\n    /// @param[in] map: the input local map to process\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode mapUpdateRequest(const SRef&lt;datastructure::Map&gt; map) override;\n\n\n\n\n    /// @brief Request to the map update pipeline to get the global map\n    /// @param[out] map: the output global map\n    /// @return FrameworkReturnCode::_SUCCESS if the global map is available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getMapRequest(SRef&lt;SolAR::datastructure::Map&gt; &amp; map) const override;\n\n\n\n\n          /// @brief Request to the map update pipeline to get a submap based on a query frame.\n                /// @param[in] frame the query frame\n                /// @param[out] map the output submap\n                /// @return FrameworkReturnCode::_SUCCESS if submap is found, else FrameworkReturnCode::_ERROR_\n                FrameworkReturnCode getSubmapRequest(const SRef&lt;SolAR::datastructure::Frame&gt; frame,\n                                                                                         SRef&lt;SolAR::datastructure::Map&gt; &amp; map) const override;\n\n\n\n\n        /// @brief Reset the map stored by the map update pipeline\n        /// @return FrameworkReturnCode::_SUCCESS if the map is correctly reset, else FrameworkReturnCode::_ERROR_\n        FrameworkReturnCode resetMap() override;\n\n\n\nThe map update pipeline project is available on the SolAR Framework GitHub: https://github.com/SolarFramework/Sample-MapUpdate/tree/0.11.0/SolARPipeline_MapUpdate\n\n\n\nThe Relocalization Pipeline\n\nThe goal of the Relocalization Pipeline is to estimate the pose of a camera from an image it has captured. For that, this service must carry out the following steps:\n\n\n\n\nGet a submap: This submap is managed by the Map Update Service (which must therefore already be deployed on your Cloud infrastructure). Thus, the relocalization pipeline requests the Map Update service to obtain the submap, and all associated data, corresponding to the frame given by the camera of the client. This step will be repeated until a submap is given.\n\n\nKeyframe retrieval: Each time the relocalization pipeline receives an image for which is has to estimate its pose, it will try to find the similar keyframes stored in the global map data. Several candidate keyframes, which should be captured from viewpoints close to the viewpoint of the input image, can be retrieved if the similarity score is high enough.\n\n\nKeypoint matching: For each candidate keyframe, the relocalization pipeline will try to match their keypoints with the one detected in the input image. The matches are found thanks to the similarity of the descriptors attached to the keypoints.\n\n\n\n\nTo initialize the relocalization pipeline processing, a device must give the characteristics of the camera it uses (resolution, focal).\n\n\nThen, the pipeline is able to process images. To do this, input data is required:\n\n\n\n\nthe images captured by the device (images are sent to the pipeline one by one)\n\n\n\n\nAnd, if the processing is successful, the output data is:\n\n\n\n\nthe new pose calculated by the pipeline\n\n\nthe confidence score of this new pose\n\n\n\n\nTo facilitate the use of this pipeline by any client application embedded in a device, it offers a simple interface based on the SolAR::api::pipeline::IRelocalizationPipeline class (see https://solarframework.github.io/create/api/ for interface definition and data structures).\nThis interface is defined as follows:\n\n\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode setCameraParameters(const SolAR::datastructure::CameraParameters &amp; cameraParams) override;\n\n\n\n\n    /// @brief Get the camera parameters\n    /// @param[out] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly returned, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getCameraParameters(SolAR::datastructure::CameraParameters &amp; cameraParams) const override;\n\n\n\n\n    /// @brief Start the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the stard succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n\n\n\n    /// @brief Stop the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the stop succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode stop() override;\n\n\n\n\n    /// @brief Request the relocalization pipeline to process a new image to calculate the corresponding pose\n    /// @param[in] image: the image to process\n    /// @param[out] pose: the new calculated pose\n    /// @param[out] confidence: the confidence score\n    /// @return FrameworkReturnCode::_SUCCESS if the processing is successful, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode relocalizeProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image, SolAR::datastructure::Transform3Df&amp; pose, float_t &amp; confidence) override;\n\n\n\n\n    /// @brief Request to the relocalization pipeline to get the map\n          /// @param[out] map the output map\n          /// @return FrameworkReturnCode::_SUCCESS if the map is available, else FrameworkReturnCode::_ERROR_\n          FrameworkReturnCode getMapRequest(SRef&lt;SolAR::datastructure::Map&gt; &amp; map) const override;\n\n\n\nThe relocalization pipeline project is available on the SolAR Framework GitHub (multithreading version): https://github.com/SolarFramework/Sample-Relocalization/tree/0.11.0/SolARPipeline_Relocalization\n\n\n\nThe Relocalization Markers Pipeline\n\nThe goal of the Relocalization Markers Pipeline is to estimate the pose of a camera from an image it has captured, based on markers detection such as fiducial markers or QR Codes. For that, this service must carry out the following steps:\n\n\n\n\nMarker detection: Each time the relocalization markers pipeline receives an image, it will try to detect a predefined marker (fiducial or QR Code) in this image. If it happens, the service will use the known position of the marker to calculate the precise pose of the camera.\n\n\n\n\nTo initialize the relocalization markers pipeline processing, a device must give the characteristics of the camera it uses (resolution, focal).\n\n\nThen, the pipeline is able to process images. To do this, input data is required:\n\n\n\n\nthe images captured by the device (images are sent to the pipeline one by one)\n\n\n\n\nAnd, if the processing is successful, the output data is:\n\n\n\n\nthe new pose calculated by the pipeline\n\n\nthe confidence score of this new pose\n\n\n\n\nTo facilitate the use of this pipeline by any client application embedded in a device, it offers a simple interface based on the SolAR::api::pipeline::IRelocalizationPipeline class (see https://solarframework.github.io/create/api/ for interface definition and data structures).\nThis interface is defined as follows:\n\n\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode setCameraParameters(const SolAR::datastructure::CameraParameters &amp; cameraParams) override;\n\n\n\n\n    /// @brief Get the camera parameters\n    /// @param[out] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly returned, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getCameraParameters(SolAR::datastructure::CameraParameters &amp; cameraParams) const override;\n\n\n\n\n    /// @brief Start the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the stard succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n\n\n\n    /// @brief Stop the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the stop succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode stop() override;\n\n\n\n\n    /// @brief Request the relocalization pipeline to process a new image to calculate the corresponding pose\n    /// @param[in] image: the image to process\n    /// @param[out] pose: the new calculated pose\n    /// @param[out] confidence: the confidence score\n    /// @return FrameworkReturnCode::_SUCCESS if the processing is successful, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode relocalizeProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image, SolAR::datastructure::Transform3Df&amp; pose, float_t &amp; confidence) override;\n\n\n\n\n    /// @brief Request to the relocalization pipeline to get the map\n          /// @param[out] map the output map\n          /// @return FrameworkReturnCode::_SUCCESS if the map is available, else FrameworkReturnCode::_ERROR_\n          FrameworkReturnCode getMapRequest(SRef&lt;SolAR::datastructure::Map&gt; &amp; map) const override;\n\n\n\nThe relocalization markers pipeline project is available on the SolAR Framework GitHub (multithreading version): https://github.com/SolarFramework/Sample-Relocalization/tree/0.11.0/SolARPipeline_RelocalizationMarker\n\n\n\nThe Mapping Pipeline\n\nThe objective of this mapping pipeline is to build a 3D sparse map surrounding the AR device for visualization and relocalization, based on images and their poses provided by the tracking system of the AR device.\n\n\nThe processing of this pipeline is divided into three main steps:\n\n\n\n\nThe bootstrap: This step aims to define the first two keyframes and to triangulate the initial point cloud from the matching keypoints between these keyframes.\n\n\nThe mapping: Firstly, this step updates the visibilities of the features in each input frame with the 3D point cloud. Then, it tries to detect new keyframes based on these visibilities. Once a new keyframe is found, it is triangulated with its neighboring keyframes to create new 3D map points. Finally, a local bundle adjustment is performed to optimize the local map points and the poses of keyframes.\n\n\nThe loop closure: Although the use of the tracking system built into the AR device provides high accuracy of the poses, the pose errors still accumulate over time. Therefore, the loop closure process allows to optimize the map and the keyframe poses, as well as to avoid creating a redundant map when AR devices return to the pre-built area.\n\n\n\n\nTo initialize the mapping pipeline processing, a device must give the characteristics of the camera it uses (resolution, focal).\n\n\nThen, the pipeline is able to process images and poses. To do this, input data is required:\n\n\n\n\nthe images captured by the device (images are sent to the pipeline one by one)\n\n\nthe relative position and orientation of the capture device, for each image\n\n\n\n\nAnd finally, after the pipeline processing, the output data is:\n\n\n\n\nthe current map of the place calculated by the pipeline from the first image to the current one (in fact, a point cloud)\n\n\nthe recalculated positions and orientations of the capture device, inside this point cloud, from the first image to the current one (in fact, only for some keyframes determined by the pipeline).\n\n\n\n\nTo facilitate the use of this pipeline by any client application embedded in a device, it offers a simple interface based on the SolAR::api::pipeline::IMappingPipeline class (see https://solarframework.github.io/create/api/ for interface definition and data structures).\nThis interface is defined as follows:\n\n\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode setCameraParameters(const datastructure::CameraParameters &amp; cameraParams) override;\n\n\n\n\n    /// @brief Start the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the stard succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n\n\n\n    /// @brief Stop the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the stop succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode stop() override;\n\n\n\n\n    /// @brief Request to the mapping pipeline to process a new image/pose\n    /// Retrieve the new image (and pose) to process, in the current pipeline context\n    /// (camera configuration, fiducial marker, point cloud, key frames, key points)\n    /// @param[in] image the input image to process\n    /// @param[in] pose the input pose in the device coordinate system\n    /// @param[in] transform the transformation matrix from the device coordinate system to the world coordinate system\n    /// @param[out] updatedTransform the refined transformation by a loop closure detection\n    /// @param[out] status the current status of the mapping pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode mappingProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image,\n                                              const SolAR::datastructure::Transform3Df &amp; pose,\n                                              const SolAR::datastructure::Transform3Df &amp; transform,\n                                              SolAR::datastructure::Transform3Df &amp; updatedTransform,\n                                              MappingStatus &amp; status) override;\n\n\n\n\n    /// @brief Provide the current data from the mapping pipeline context for visualization\n    /// (resulting from all mapping processing since the start of the pipeline)\n    /// @param[out] outputPointClouds: pipeline current point clouds\n    /// @param[out] keyframePoses: pipeline current keyframe poses\n    /// @return FrameworkReturnCode::_SUCCESS if data are available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getDataForVisualization(std::vector&lt;SRef&lt;datastructure::CloudPoint&gt;&gt; &amp; outputPointClouds,\n                                                std::vector&lt;datastructure::Transform3Df&gt; &amp; keyframePoses) const override;\n\n\n\nTo complete the description of the mapping pipeline, the following diagram shows the different steps it implements, from initialization to the constitution of the point cloud:\n\n\n\n\n\nFigure 3. Mapping pipeline processing\n\n\nThe mapping pipeline project is available on the SolAR Framework GitHub (multithreading version): https://github.com/SolarFramework/Sample-Mapping/tree/0.11.0/Mapping/SolARPipeline_Mapping_Multi\n\n\n\n\n\nARCloud Services deployment\n\n\nIn fact, these services are remote versions of the pipelines presented previously, designed to be deployed in a Cloud architecture, to provide AR device applications with an easy way to benefit from this powerful SolAR Framework spatial computing algorithms.\n\n\nTo help you easily deploy the services on your own infrastructure, we have already performed some preliminary steps:\n\n\n\n\ncreation of a remote version of each pipeline:\n\n\n\nby adding a gRPC server based on the interface of the pipeline (which becomes the interface of the service), as described in the previous paragraph\n\n\nby managing the serialization and deserialization of all the data structures that can be used to request the service through its interface\n\n\n\n\n\nencapsulation of the remote pipeline in a Docker container, which exposes its interface through a default port\n\n\ngeneration of the corresponding Docker image, which is available on a public docker repository\n\n\n\n\n\n\n\n\n\n\nYou must check the available versions of the ARCloud Services on our public repository to be able to deploy them to your servers:\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-docker-local\n\n\n\n\n\nSo, you are ready to deploy the services on your Cloud infrastructure, using Kubernetes.\n\n\nKubernetes is an open source system for automating the deployment, scaling and management of containerized applications.\n\n\nBut before that, you must have prepared your Cloud architecture: set up the Kubernetes environment with a traffic manager, a load balancer, an application gateway, a firewall, servers, clusters, nodes&#8230;&#8203;\n\n\n\n\n\n\n\n\nIn order to be able to perform the instructions presented in this part, you must have installed the Kubernetes command-line tool, kubectl, on your computer:\nhttps://kubernetes.io/docs/tasks/tools/\n\n\n\n\n\nHelm charts\n\nTo help you deploy the ARCloud services on your own cloud architecture, we provide you with some Helm packages (charts) giving all the information needed to deploy each of these services.\n\n\nHelm is a tool that helps you manage Kubernetes applications - Helm charts help you define, install, and upgrade even the most complex Kubernetes application. Helm is the package manager supported and recommended by Kubernetes.\n\n\nYou will find these Helm charts at this public URL: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-helm-virtual\n\n\n\n\n\n\n\n\nIn order to be able to perform the instructions presented in this part, you must have installed the Helm CLI on your computer:\nhttps://helm.sh/docs/intro/install/\n\n\n\n\n\nThe ARCloud Helm charts are organized as follows:\n\n\n\n[Helm charts name-version]\n|_ Chart.yaml\n|_ values.yaml\n|_ templates\n  |_ mapupdate\n  |_ mapupdatecuda\n  |_ relocalization\n  |_ relocalizationcuda\n  |_ relocalizationmarkers\n  |_ mapping\n  |_ mappingcuda\n  |_ mappingnodrop\n  |_ mappingnodropcuda\n  |_ mappingandrelocalizationfrontend\n  |_ mappingandrelocalizationfrontendcuda\n\n\n\nChart.yaml\n\n\nThis yaml file defines, among other things, the Helm charts name, version and description.\n\n\n\napiVersion: v2\nappVersion: &quot;2.0&quot;\ndescription: [description]\nname: [Helm charts name]\ntype: application\nversion: [Helm charts version]\n\n\n\nvalues.yaml\n\n\nThis yaml file defines the default values to use for all ARCloud services during deployment. These values will be described later for each service.\n\n\ntemplates\n\n\nThis folder contains a subfolder for each service to deploy (with the name of this service). Each subfolder is organized identically:\n\n\n\n[service name]\n|_ NOTES.txt              =&gt; defines some variables needed for deployment\n|_ _helpers.tpl           =&gt; defines labels used in the 'values.yaml' file\n|_ deployment.yaml        =&gt; gives deployment information such as Docker image,\n                          environment variables, ports, replicas, etc.\n|_ ingress.yaml           =&gt; sets ingress configuration (host, path,\n                          service name and port, etc.)\n|_ service.yaml           =&gt; gives service information such as name, labels,\n                          internal/external ports, network protocol, etc.\n|_ serviceaccount.yaml    =&gt; gives service account information (name, labels, annotations)\n\n\n\nTo use these Helm charts to deploy the ARCloud services, here are the commands you need to run on your computer:\n\n\n\n\nfirst, to set your Kubernetes configuration (distant server IP and port, user, certificates, cluster, etc):\n\n\nexport KUBECONFIG=[/path/to/config/file]\n\n\n\n\nto add ARCloud Helm charts repository to your Helm environment:\n\n\nhelm repo add solar-helm-virtual https://solar-docker-local.artifact.b-com.com/solar-helm-virtual\n\n\n\n\nthen, to sync your local Helm environment with ARCloud distant repository (to run before each new deployment):\n\n\nhelm repo update\n\n\n\n\nand, finally, to deploy the ARCloud services on your server:\n\n\nhelm upgrade --install --namespace [your name space] [Helm charts name] solar-helm-virtual/[Helm charts package] --version [Helm charts version]\n\n\n\n\n\n\n\n\n\n\n\n\nYou must check the available versions of the Helm charts on our public repository to be able to deploy the ARCloud services to your servers:\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-helm-virtual\n\n\n\n\n\n\nThe Map Update Service\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-service:x.y.z\n\n\nThe Map Update Service default values are defined in the values.yaml file included in the Helm charts, in the 'mapUpdateService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmapUpdateService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32050 (*)\n  ...\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO             (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;    (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;-1&quot;    (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Map Update Cuda Service (GPU version)\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-cuda-service:x.y.z\n\n\nThe Map Update Cuda Service default values are defined in the values.yaml file included in the Helm charts, in the 'mapUpdateCudaService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmapUpdateCudaService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-cuda-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.3&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32150 (*)\n  ...\n\n\n\nGPU configuration:\n\n\nfor NVIDIA Tesla 4 (virtual GPU):\n\n\n\n  resources:\n    limits:\n      k8s.kuartis.com/vgpu: '1'\n\n\n\nfor NVIDIA A100 (GPU card partitioning):\n\n\n\n  resources:\n    limits:\n      nvidia.com/mig-1g.5gb: &quot;1&quot;\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO             (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;    (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;-1&quot;    (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Relocalization Service\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-service:x.y.z\n\n\nThe Relocalization Service default values are defined in the values.yaml file included in the Helm charts, in the 'relocalizationService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nrelocalizationService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32051 (*)\n  ...\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO             (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;    (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;-1&quot;    (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Relocalization Cuda Service (GPU version)\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-cuda-service:x.y.z\n\n\nThe Relocalization Cuda Service default values are defined in the values.yaml file included in the Helm charts, in the 'relocalizationCudaService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nrelocalizationCudaService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-cuda-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32151 (*)\n  ...\n\n\n\nGPU configuration:\n\n\nfor NVIDIA Tesla 4 (virtual GPU):\n\n\n\n  resources:\n    limits:\n      k8s.kuartis.com/vgpu: '1'\n\n\n\nfor NVIDIA A100 (GPU card partitioning):\n\n\n\n  resources:\n    limits:\n      nvidia.com/mig-1g.5gb: &quot;1&quot;\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO             (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;    (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;-1&quot;    (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Relocalization Markers Service\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-markers-service:x.y.z\n\n\nThe Relocalization Markers Service default values are defined in the values.yaml file included in the Helm charts, in the 'relocalizationMarkersService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nrelocalizationMarkersService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-markers-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32052 (*)\n  ...\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;20000&quot;   (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Mapping Service\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-service:x.y.z\n\n\nThe Mapping Service default values are defined in the values.yaml file included in the Helm charts, in the 'mappingService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmappingService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32053 (*)\n  ...\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;      (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Mapping Cuda Service (GPU version)\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-cuda-service:x.y.z\n\n\nThe Mapping Cuda Service default values are defined in the values.yaml file included in the Helm charts, in the 'mappingCudaService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmappingCudaService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-cuda-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32153 (*)\n  ...\n\n\n\nGPU configuration:\n\n\nfor NVIDIA Tesla 4 (virtual GPU):\n\n\n\n  resources:\n    limits:\n      k8s.kuartis.com/vgpu: '1'\n\n\n\nfor NVIDIA A100 (GPU card partitioning):\n\n\n\n  resources:\n    limits:\n      nvidia.com/mig-1g.5gb: &quot;1&quot;\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;      (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Mapping No Drop Service\n\nThis service behaves in much the same way as the * Mapping Service * (it is also based on the Mapping Pipeline), except that it has a buffer of images to store those sent by the client, before processing them. This mechanism allows it to process these images at its own rate, without loss (no drop), even if the client sends its requests at a higher rate (within the limit of the maximum size of the buffer).\n\n\nThe idea is to make this service more robust in terms of loss of tracking compared to the mapping service presented above. On the other hand, using this service, the processing of the images is no longer done in real time.\n\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-service:x.y.z\n\n\nThe Mapping No Drop Service default values are defined in the values.yaml file included in the Helm charts, in the 'mappingNoDropService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmappingNoDropService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32054 (*)\n  ...\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;      (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Mapping No Drop Cuda Service (GPU version)\n\nThe Docker image of this service is available on this public Docker repository: solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-cuda-service:x.y.z\n\n\nThe Mapping No Drop Cuda Service default values are defined in the values.yaml file included in the Helm charts, in the 'mappingNoDropCudaService:' part.\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmappingNoDropCudaService:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment:\n\n\n\n  ...\n  image:\n    solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-nodrop-cuda-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    nodePort: 32154 (*)\n  ...\n\n\n\nGPU configuration:\n\n\nfor NVIDIA Tesla 4 (virtual GPU):\n\n\n\n  resources:\n    limits:\n      k8s.kuartis.com/vgpu: '1'\n\n\n\nfor NVIDIA A100 (GPU card partitioning):\n\n\n\n  resources:\n    limits:\n      nvidia.com/mig-1g.5gb: &quot;1&quot;\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;-1&quot;      (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\nThe Mapping and Relocalization Front End Service\n\nThis service is quite different from those presented above. In fact, it is not based on a computer vision processing embedded in a SolAR pipeline. As its name suggests, this service is used as a \"front end\" to receive requests from end-user applications (running on Hololens glasses for example). It then relies on the other ARCloud services (Map Update, Relocalization, Relocalization Markers and Mapping) to process the received data and to return some results to the calling application.\n\n\nThe processing of this service is the following:\n\n\n\n\nit receives pairs of images (captured by the camera of the client) and poses (calculated by the client in its own coordinate system)\n\n\nit tries to get an initial pose of the camera in the SolAR coordinate system, sending images to Relocalization Service (sparse map based) and Relocalization Markers Service (marker based), for the mapping process which will start just after\nCurrently, the marker definitions are embedded in the Relocalization Markers Service image (it will be handled differently in next versions)\nFor your tests, a fiducial marker image is available here: https://github.com/SolarFramework/Service-Relocalization/blob/0.11.0/SolARService_MappingAndRelocalizationFrontend/markerA.png\n\n\nonce the origin of the coordinate system has been determined, it uses the Relocalization Service and the Relocalization Merkers Service to attempt to relocalize some images in the ARCloud map to determine the corrected pose of these images in the SolAR coordinate system, and calculate the 3D transformation matrix between the client coordinate system and that of SolAR\n\n\nin the same time, it sends each pair of image and pose (corrected using the 3D transformation matrix) to the Mapping Service to calculate the point cloud of the client journey\n\n\nwhen no more images are sent (end of the client journey), the point cloud calculated by the Mapping Service is sent to the Map Update Service to merge it with the current global map (thus making the ARCloud services more and more efficient)\n\n\n\n\nClient applications based on the SolAR Framework\n\nThis service can be directly requested by C++ applications based on the SolAR Framework. For that, it exposes a specific interface, IAsyncRelocalizationPipeline, on a dedicated port (see the values.yaml description\" bellow). This interface is defined like this:\n\n\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n\n\n\n    /// @brief Init the pipeline and specify the mode for the pipeline processing\n    /// @param[in] pipelineMode: mode to use for pipeline processing\n    /// @return FrameworkReturnCode::_SUCCESS if the mode is correctly initialized, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init(PipelineMode pipelineMode) override;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    virtual FrameworkReturnCode setCameraParameters(const SolAR::datastructure::CameraParameters &amp; cameraParams) = 0;\n\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode setCameraParameters(const SolAR::datastructure::CameraParameters &amp; cameraParams) override;\n\n\n\n\n    /// @brief Get the camera parameters\n    /// @param[out] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly returned, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getCameraParameters(SolAR::datastructure::CameraParameters &amp; cameraParams) const override;\n\n\n\n\n    /// @brief Start the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the stard succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n\n\n\n    /// @brief Stop the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the stop succeed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode stop() override;\n\n\n\n\n    /// @brief Request the asynchronous relocalization pipeline to process a new image to calculate\n    /// the corresponding 3D transformation to the SolAR coordinates system\n    /// @param[in] image: the image to process\n    /// @param[in] pose: the original pose in the client coordinates system\n    /// @param[in] timestamp: the timestamp of the image\n    /// @param[out] transform3DStatus: the status of the current 3D transformation matrix\n    /// @param[out] transform3D : the current 3D transformation matrix (if available)\n    /// @param[out] confidence: the confidence score of the 3D transformation matrix\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode relocalizeProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image,\n                                                 const SolAR::datastructure::Transform3Df &amp; pose,\n                                                 const std::chrono::system_clock::time_point &amp; timestamp,\n                                                 TransformStatus &amp; transform3DStatus,\n                                                 SolAR::datastructure::Transform3Df &amp; transform3D,\n                                                 float_t &amp; confidence) override;\n\n\n\n\n    /// @brief Request the asynchronous relocalization pipeline to get the 3D transform offset\n    /// between the device coordinate system and the SolAR coordinate system\n    /// @param[out] transform3DStatus: the status of the current 3D transformation matrix\n    /// @param[out] transform3D : the current 3D transformation matrix (if available)\n    /// @param[out] confidence: the confidence score of the 3D transformation matrix\n    /// @return FrameworkReturnCode::_SUCCESS if the 3D transform is available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode get3DTransformRequest(TransformStatus &amp; transform3DStatus,\n                                              SolAR::datastructure::Transform3Df &amp; transform3D,\n                                              float_t &amp; confidence) override;\n\n\n\n\n    /// @brief Return the last pose processed by the pipeline\n    /// @param[out] pose: the last pose if available\n    /// @param[in] poseType: the type of the requested pose\n    ///            - in the SolAR coordinate system (by default)\n    ///            - in the device coordinate system\n    /// @return FrameworkReturnCode::_SUCCESS if the last pose is available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getLastPose(SolAR::datastructure::Transform3Df &amp; pose,\n                                    const PoseType poseType = SOLAR_POSE) const override;\n\n\n\n\n    /// @brief Reset the map stored by the map update pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the map is correctly reset, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode resetMap() const override;\n\n\n\n\nMapping And Relocalization Frontend Proxy\n\nFor clients that are unable to use the SolAR framework directly (e.g. unsupported hardware platforms), SolAR provides a proxy for the Mapping and Relocalization Front End service that can be called via a dedicated gRPC interface. Here is an excerpt of the proto file defining the gRPC service:\n\n\n\nservice SolARMappingAndRelocalizationProxy\n{\n    rpc Init(PipelineModeValue) returns (Empty);\n    rpc Start(Empty) returns (Empty);\n    rpc Stop(Empty) returns (Empty);\n    rpc SetCameraParameters(CameraParameters) returns (Empty);\n    rpc RelocalizeAndMap(Frame) returns (RelocalizationResult);\n    rpc Get3DTransform(Empty) returns (RelocalizationResult);\n    rpc Reset(Empty) returns (Empty);}\n\n\n\nThe proxy comes as a C++ gRPC service, handling the client requests and passing them to the SolAR frontend service.\n\n\nThe gRPC proto can be used to generate clients for any gRPC-supported language, which opens the access to the Mapping and Relocalization Front End service to a wide range of platforms. The generated C# client can for instance be used in Unity projects, so that applications embedded in mobile devices (Android, iOS, VR/AR headsets,&#8230;&#8203;) can use the Mapping and Relocalization Front End service without the need to have the SolAR framework as a dependency.\n\n\nThe proxy is packaged alongside the frontend service in the same Docker image, and is accessible via another port than the one used for direct access to the service, as shown in next figure.\n\n\n\nHololens 2 Unity plugin\n\nFor Hololens 2 AR glasses, SolAR provides a native C++ Unity plugin which is able to retrieve sensors data. With it, one can collect images, poses and timestamps from the RGB (PV) camera, as well as from the tracking (VLC) cameras and depth sensors, provided that Research Mode is enabled for the device. This native library can then be installed and used in a Unity project, thanks to a C# wrapper of the plugin API.\n\n\nFor convenience, SolAR also provides a Unity package containing a prefab encapsulating the use of the aforementioned plugin and the C# gRPC client for the frontend proxy.\n\n\nThis prefab is parameterized by:\n\n\n\n\nThe URL of the Mapping And Relocalization frontend\n\n\nThe Hololens 2 camera(s) to enable\n\n\nA Unity game object of a scene on which the returned transformation will be applied to\n\n\n\n\nThe prefab can then be controlled to start and stop the collect of sensor data which are sent to the SolAR Mapping And Relocalization Front End service, which in turn returns the transformation to apply to the given GameObject, so that the scene is properly aligned.\n\n\nHere is the global architecture of the Mapping and Relocalization Front End service:\n\n\n\n\n\nFigure 4. Global architecture of the Mapping and Relocalization Front End Service\n\n\n\nService configuration\n\nThe Docker image of this service is available on this public Docker repository: _solar-docker-local.artifact.b-com.com/relocalization/0.11.0/mappingandrelocalizationfrontend-service:x.y.z\n\n\nThe Mapping and Relocalization Front End Service default values are defined in the values.yaml file included in the Helm charts, in the 'mappingAndRelocalizationFrontend:' part and in the 'mappingAndRelocalizationFrontendCuda' part (for ARCloud Services using GPU).\n\n\nYou can modify some of these values according to your own configuration:\n\n\nenable/disable the service deployment:\n\n\n\nmappingAndRelocalizationFrontend:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\nor (GPU)\n\n\n\nmappingAndRelocalizationFrontendCuda:\n  enabled: true (*)\n  replicaCount: 1\n...\n\n\n\npath and version of the Docker image to use for deployment (same image for Cuda/GPU version):\n\n\n\n  ...\n  image:\n    repository: solar-docker-local.artifact.b-com.com/relocalization/0.11.0/mappingandrelocalizationfrontend-service (*)\n    pullPolicy: Always\n    tag: &quot;3.0.2&quot; (*)\n  ...\n\n\n\nexternal port for remote clients:\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    httpNodePort: 32055   (*) Port for C++ app clients\n    proxyNodePort0: 32060 (*) Ports port for Hololens/Unity clients\n    proxyNodePort1: 32061 (*)\n    proxyNodePort2: 32062 (*)\n    proxyNodePort3: 32063 (*)\n    proxyNodePort4: 32064 (*)\n    proxyNodePort5: 32065 (*)\n    proxyNodePort6: 32066 (*)\n    proxyNodePort7: 32067 (*)\n    proxyNodePort8: 32068 (*)\n    proxyNodePort9: 32069 (*)\n  ...\n\n\n\nor (GPU)\n\n\n\n  ...\n  service:\n    type: NodePort\n    port: 80\n    httpNodePort: 32155   (*) Port for C++ app clients\n    proxyNodePort0: 32160 (*) Ports port for Hololens/Unity clients\n    proxyNodePort1: 32161 (*)\n    proxyNodePort2: 32162 (*)\n    proxyNodePort3: 32163 (*)\n    proxyNodePort4: 32164 (*)\n    proxyNodePort5: 32165 (*)\n    proxyNodePort6: 32166 (*)\n    proxyNodePort7: 32167 (*)\n    proxyNodePort8: 32168 (*)\n    proxyNodePort9: 32169 (*)\n  ...\n\n\n\nGPU configuration (for GPU version):\n\n\nfor NVIDIA Tesla 4 (virtual GPU):\n\n\n\n  resources:\n    limits:\n      k8s.kuartis.com/vgpu: '1'\n\n\n\nfor NVIDIA A100 (GPU card partitioning):\n\n\n\n  resources:\n    limits:\n      nvidia.com/mig-1g.5gb: &quot;1&quot;\n\n\n\nenvironment variables\n\n\n\n  ...\n  settings:\n    solarLogLevel: INFO               (*) Log level\n                                        (DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING)\n    xpcfGRPCMaxSendMsgSize: &quot;20000&quot;   (*) maximum size of gRPC sent messages\n                                        (-1 for maximum value)\n    xpcfGRPCMaxRecvMsgSize: &quot;7000000&quot; (*) maximum size of gRPC received messages\n                                        (-1 for maximum value)\n\n\n\n\n\nkubectl commands\n\nTo deploy or re-deploy a service in your infrastructure, when your manifest file is correctly filled in, you can use the kubectl command-line tool to:\n\n\n\n\ndefine your Kubernetes context in a .conf file (cluster name, namespace, server address and port, user, certificates, etc.) like this:\n\n\n\n\n\n    apiVersion: v1\n    clusters:\n    - cluster:\n        certificate-authority-data: ...\n        server: https://...\n    name: kubernetes\n    contexts:\n    - context:\n        cluster: kubernetes\n        namespace: artwin\n        user: kubernetes-admin\n    name: cluster-mapping\n    current-context: cluster-mapping\n    kind: Config\n    preferences: {}\n    users:\n    - name: kubernetes-admin\n    user:\n        client-certificate-data: ...\n        client-key-data: ...\n\n\n\nand set it as default configuration:\n\n\n\nexport KUBECONFIG=path/to/your_configuration.conf\n\n\n\n\n\nset the context to use:\n\n\nkubectl config use-context [context name]\n\n\n\n\n\n\nyou can also specify the default namespace:\n\n\n\nkubectl config set-context --current --namespace=[namespace]\n\n\n\n\n\ndeploy your service:\n\n\nkubectl apply -f [path/to/your_manifest.yaml]\n\n\n\n\ncheck your deployments:\n\n\nkubectl get deployments\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\nmap-update-service        1/1     1            1           27d\nrelocalization-service    1/1     1            1           25d\nmapping-service           1/1     1            1           21d\n\n\n\n\ncheck your services:\n\n\nkubectl get services\nNAME                      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nmap-update-service        NodePort   10.110.144.63    &lt;none&gt;        80:31888/TCP     27d\nrelocalization-service    NodePort   10.108.177.214   &lt;none&gt;        80:31889/TCP     28d\nmapping-service           NodePort   10.107.117.85    &lt;none&gt;        80:31887/TCP     30d\n\n\n\n\ncheck your pods:\n\n\nkubectl get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\nmap-update-service-57ff9b9888-65hth        1/1     Running   0          14d\nrelocalization-service-598bc78d96-rnwmx    1/1     Running   0          21d\nmapping-service-84bc74d954-6vc7v           1/1     Running   1          7d15h\n\n\n\n\nvisualize the logs of a pod:\n\n\nkubectl logs -f [pod name]\n\n\n\n\nrestart a pod: to do that, you can for example change an environment variable of the pod, to force it to restart\n\n\nkubectl set env deployment [deployment name] SOLAR_LOG_LEVEL=INFO\n\n\n\n\n\n\n\n\n\n\n\n\nYou can find more kubectl commands on this web site: https://kubernetes.io/docs/reference/kubectl/cheatsheet/\n\n\n\n\n\n\n\n\nARCloud Services on a local computer (with Docker engine)\n\n\nIf you want to test our ARCloud Services on your own computer, without any Cloud deployment, you can easily pull their images on your local Docker Engine, and then launch these services using predefined scripts.\n\n\n\n\n\n\n\n\nTo run these services, you must first install a Docker engine to manage the Docker images (https://docs.docker.com/engine/install/).\nThen, you have to add our repository to the \"insecure-registries\" of your Docker Engine configuration, like this:\n\n\n\n\n\n\n{\n  &quot;builder&quot;: {\n    &quot;gc&quot;: {\n      &quot;defaultKeepStorage&quot;: &quot;20GB&quot;,\n      &quot;enabled&quot;: true\n    }\n  },\n  &quot;debug&quot;: true,\n  &quot;experimental&quot;: false,\n  &quot;features&quot;: {\n    &quot;buildkit&quot;: true\n  },\n  &quot;insecure-registries&quot;: [\n    ...\n    &quot;solar-docker-local.artifact.b-com.com&quot;\n  ],\n  &quot;registry-mirrors&quot;: []\n}\n\n\n\n1- Download the Service images you need to test on your own Docker engine\n\n\n\n\n\n\n\n\nYou must check the available versions of the ARCloud Services on our public repository to be able to pull their images to your local Docker engine:\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-docker-local\n\n\n\n\n\nFor example:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-service:x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-service:x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-markers-service:x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-service:x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/mappingandrelocalizationfrontend-service:x.y.z\n\n\n\n2- Get the launch scripts corresponding to your Operating System (Windows or Linux) from the public repository\n\n\n\n\n\n\n\n\nYou must check the available scripts on our public repository to be able to download them to your local computer:\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services\n\n\n\n\n\nFor example:\n\n\nLinux:\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mapupdate.sh\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_relocalization.sh\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_relocalization_markers.sh\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mapping_multi.sh\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mappingandrelocalizationfrontend.sh\n\n\nWindows:\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mapupdate.bat\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_relocalization.bat\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_relocalization_markers.bat\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mapping_multi.bat\n\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services/launch_mappingandrelocalizationfrontend.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service ('latest') by the current one.\n\n\n\n\n\n3- If needed, modify the launch script of the ARCloud service you want to test\n\n\n\n\n\n\n\n\nBy default, the log level of each service is set to INFO, but you can change it in the corresponding script file (available values are DEBUG, CRITICAL, ERROR, INFO, TRACE, WARNING):\n\n\n\n\n\n\nexport SOLAR_LOG_LEVEL=INFO\n\n\n\n\n\n\n\n\n\nBy default, a specific port is set for each service, but you can change it in the corresponding script file (last line):\n\n\n\n\n\n\ndocker run -d -p default_port:8080 ...\n\n\n\nThe default ports currently defined for the ARCloud Services are:\n\n\nServices without GPU (no Cuda):\n\n\nMap Update: 50053\n\n\nRelocalization: 50052\n\n\nRelocalization Markers: 50050\n\n\nMapping/Mapping No Drop: 50051\n\n\nFront End: 50055 / 5000&#8594;5009 (for Unity clients)\n\n\n\n\nServices with GPU (Cuda):\n\n\nMap Update: 60053\n\n\nRelocalization: 60052\n\n\nMapping/Mapping No Drop: 60051\n\n\nFront End: 60055 / 5100&#8594;5109 (for Unity clients)\n\n\n\n\n4- Execute the launch script of the ARCloud service you want to test\n\n\n\n\n\n\n\n\nARCloud Services depend on each other and you must launch them in a specific order to test them all:\nMap Update &#8594; Relocalization &#8594; Relocalization Markers &#8594; Mapping &#8594; Front End\n\n\n\n\n\nAs some ARCloud Services depend on others, sometimes you need to give some services URL (IP:port) as script parameters. To check if a script needs such parameters, just run it and you will see a help message if needed:\n\n\n\nlaunch_relocalization.sh\nYou need to give Map Update Service URL as parameter!\n\n\n\nA specific message will be prompted for each necessary parameter.\n\n\nTo give the local URL of a service, use the Docker bridge IP address (and the port defined in the launch script) given by this command (\"Gateway\" value):\n\n\n\ndocker network inspect bridge\n[\n    {\n        &quot;Name&quot;: &quot;bridge&quot;,\n        &quot;Id&quot;: &quot;7ec85993b0ab533552db22a0a61794f0051df5782703838948cdac82d4069674&quot;,\n        &quot;Created&quot;: &quot;2021-09-22T06:51:14.2139067Z&quot;,\n        &quot;Scope&quot;: &quot;local&quot;,\n        &quot;Driver&quot;: &quot;bridge&quot;,\n        &quot;EnableIPv6&quot;: false,\n        &quot;IPAM&quot;: {\n            &quot;Driver&quot;: &quot;default&quot;,\n            &quot;Options&quot;: null,\n            &quot;Config&quot;: [\n                {\n                    &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,\n                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;\n                }\n            ]\n        },\n...\n\n\n\nFor example:\n\n\n\nlaunch_relocalization.sh 172.17.0.1:50053\n\n\n\n5- Check the logs of the service:\n\n\nYou can display the logs of each service using its Docker name:\n\n\n\ndocker logs [-f] [service_name]\n\n\n\nFor example:\n\n\n\ndocker logs -f solarservicemapupdate\n\n\n\nYou should see something like this:\n\n\n\n[2021-09-22 07:24:56:772970] [info] [    7] [main():137] Load modules configuration file: /.xpcf/SolARService_MapUpdate_modules.xml\n[2021-09-22 07:24:56:789855] [info] [    7] [main():146] Load properties configuration file: /.xpcf/SolARService_MapUpdate_properties.xml\n[2021-09-22 07:25:00:716371] [info] [    7] [main():161] LOG LEVEL: INFO\n[2021-09-22 07:25:00:716410] [info] [    7] [main():163] GRPC SERVER ADDRESS: 0.0.0.0:8080\n[2021-09-22 07:25:00:716429] [info] [    7] [main():165] GRPC SERVER CREDENTIALS: 0\n[2021-09-22 07:25:00:716432] [info] [    7] [main():171] GRPC MAX RECEIVED MESSAGE SIZE: 18446744073709551615\n[2021-09-22 07:25:00:716435] [info] [    7] [main():178] GRPC MAX SENT MESSAGE SIZE: 18446744073709551615\n[2021-09-22 07:25:00:716438] [info] [    7] [main():182] XPCF gRPC server listens on: 0.0.0.0:8080\n\n\n\nTo get all service names (for running containers), use this command:\n\n\n\ndocker container ls\n\n\n\nHere is the exhaustive list of ARCloud services (Docker containers):\n\n\n\n\nsolarservicemapupdate\n\n\nsolarservicemapupdatecuda\n\n\nsolarservicerelocalization\n\n\nsolarservicerelocalizationcuda\n\n\nsolarservicerelocalizationmarkers\n\n\nsolarservicemappingmulti\n\n\nsolarservicemappingmulticuda\n\n\nsolarservicemappingmultinodrop\n\n\nsolarservicemappingmultinodropcuda\n\n\nsolarservicemappingandrelocalizationfrontend\n\n\nsolarservicemappingandrelocalizationfrontendcuda\n\n\n\n\n\n\nTest applications\n\n\nTo verify if your services are deployed correctly and are running as expected, we provide you with test client applications that are embedded in Docker containers and delivered as pre-built Docker images, so that you can run them on any computer with Linux or Windows operating system.\n\n\n\n\n\n\n\n\nYou must check the available versions of the ARCloud test applications on our public repository to be able to pull them on your own computer:\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-docker-local/tests/0.11.0\n\n\n\n\n\nThese test images are available on a public Docker repository:\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-client:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-display-map-client:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-producer:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-viewer:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/relocalization-client:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-relocalization-client:x.y.z\n\n\nsolar-docker-local.artifact.b-com.com/tests/0.11.0/mappingandrelocalizationfrontend-client:x.y.z\n\n\n\n\n\n\n\n\nTo run these test applications, you must first install a Docker engine to manage the Docker images (https://docs.docker.com/engine/install/).\nThen, you have to add our repository to the \"insecure-registries\" of your Docker Engine configuration, like this:\n\n\n\n\n\n\n{\n  &quot;builder&quot;: {\n    &quot;gc&quot;: {\n      &quot;defaultKeepStorage&quot;: &quot;20GB&quot;,\n      &quot;enabled&quot;: true\n    }\n  },\n  &quot;debug&quot;: true,\n  &quot;experimental&quot;: false,\n  &quot;features&quot;: {\n    &quot;buildkit&quot;: true\n  },\n  &quot;insecure-registries&quot;: [\n    ...\n    &quot;solar-docker-local.artifact.b-com.com&quot;\n  ],\n  &quot;registry-mirrors&quot;: []\n}\n\n\n\n\n\n\n\n\n\nYou must have a X-server running on your system to manage the graphical outputs of the test applications (example for Windows: https://sourceforge.net/projects/vcxsrv/)\n\n\n\n\n\n\n\n\n\n\n\nTo run these test applications on a Linux Virtual Machine hosted on a Windows computer, you need to disable some Windows features (Programs and Features&#8594;Turn Windows features on or off) as shown in this screenshot:\n\n\n\n\n\n\n\n\nFigure 5. Windows features to turn off for Linux VM configurations\n\n\nThe Map Update Service test application\n\nOverview\n\nThis sample application is used to test the processing of local maps carried out remotely by the Map Update Service. It uses a predefined local map dataset, included in this sample package.\n\n\nThe algorithm in this example is quite simple:\n\n\n1- It requests the Map Update service for the first time to get its current global map, and displays it in a dedicated window if available (in the form of a point cloud). To do that, the application uses this method from the pipeline interface:\n\n\n\n    /// @brief Request to the map update pipeline to get the global map\n    /// @param[out] map: the output global map\n    /// @return FrameworkReturnCode::_SUCCESS if the global map is available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getMapRequest(SRef&lt;SolAR::datastructure::Map&gt; &amp; map) const override;\n\n\n\n2- Then, it reads the local map dataset stored in the ./data/maps folder, and sends it for processing to the Map Update Service using this method from its interface:\n\n\n\n    /// @brief Request to the map update pipeline to update the global map from a local map\n    /// @param[in] map: the input local map to process\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode mapUpdateRequest(const SRef&lt;datastructure::Map&gt; map) override;\n\n\n\n3- And to finish, it requests again the Map Update service to get its new global map, and displays it in a dedicated window if available (in the form of a point cloud).\n\n\nTraces are displayed at runtime to follow the progress of the application.\n\n\n\nLaunch the test application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-client:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapupdate_client.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mapupdate_client.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapupdate_client.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Map Update Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch.sh [map_update_service_url] [host_IP_address]\nlaunch_vm.sh [map_update_service_url]\nlaunch.bat [map_update_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50053 192.168.56.1\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemapupdateclient\n\n\n\nAnd you will see something like this:\n\n\n\nMAPUPDATE_SERVICE_URL defined: 0.0.0.0:50053\nTry to replace the MapUpdate Service URL in the XML configuration file...\nXML configuration file ready\n[2021-07-28 16:19:29:451534] [info] [12204] [main():87] Get component manager instance\n[2021-07-28 16:19:29:451821] [info] [12204] [main():91] Load Client Remote Map Update Service configuration file: SolARServiceTest_MapUpdate_conf.xml\n[2021-07-28 16:19:29:452551] [info] [12204] [main():99] Resolve IMapUpdatePipeline interface\n[IMapUpdatePipeline_grpcProxy::onConfigured()]Check if remote map update service URL is defined in MAPUPDATE_SERVICE_URL\n[2021-07-28 16:19:29:482257] [info] [12204] [main():102] Resolve other components\n[2021-07-28 16:19:29:506398] [info] [12204] [main():106] Client components loaded\n[2021-07-28 16:19:29:506478] [info] [12204] [main():110] Initialize map update service\n[2021-07-28 16:19:30:481731] [info] [12204] [main():118] Set camera parameters\n[2021-07-28 16:19:30:482769] [info] [12204] [main():125] Start map update service\n[2021-07-28 16:19:30:596887] [info] [12204] [main():136] Try to get initial global map from Map Update remote service\n[2021-07-28 16:19:37:196074] [info] [12204] [main():151]\n==&gt; Display initial global map\n\n\n\nYou will also see the global map given by the Map Update Service in a dedicated window:\n\n\n\n\n\nFigure 6. Image viewer of the first global map\n\n\n\n[2021-07-28 16:32:11:961857] [info] [12264] [main():163] Load local map\n[2021-07-28 16:32:11:961959] [info] [12264] [loadFromFile():289] Loading the map from file...\n[2021-07-28 16:32:12:945752] [info] [12264] [loadFromFile():346] Load done!\n[2021-07-28 16:32:12:945831] [info] [12264] [main():170] Send map request for local map\n[2021-07-28 16:32:12:945871] [info] [12264] [main():174] Nb points: 8338\n[2021-07-28 16:32:31:572424] [info] [12264] [main():188]\n==&gt; Display new global map (after local map processing): press Ctrl+C to stop test\n\n\n\nAnd you will see the new global map given by the Map Update Service in a dedicated window:\n\n\n\n\n\nFigure 7. Image viewer of the new global map\n\n\n\n\nThe Map Update Display Map test application\n\nOverview\n\nThis sample application is just used to display the map currently stored in the Map Update service. To do that, it requests the service to get its global map, and displays it in a dedicated window if available (in the form of a point cloud).\n\n\n\nLaunch the test application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-display-map-client:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapupdate_display_map_client.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mapupdate_display_map_client.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapupdate_display_map_client.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Map Update Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch.sh [map_update_service_url] [host_IP_address]\nlaunch_vm.sh [map_update_service_url]\nlaunch.bat [map_update_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50053 192.168.56.1\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemapupdatedisplaymapclient\n\n\n\nAnd you will see something like this:\n\n\n\nMAPUPDATE_SERVICE_URL defined: 0.0.0.0:50053\nTry to replace the MapUpdate Service URL in the XML configuration file...\nXML configuration file ready\n[2021-12-09 14:27:40:754955] [info] [    9] [main():120] Get component manager instance\n[2021-12-09 14:27:40:755155] [info] [    9] [main():124] Load Client Remote Map Update Pipeline configuration file: /.xpcf/SolARServiceTest_MapUpdate_DisplayMap_conf.xml\n[2021-12-09 14:27:40:755522] [info] [    9] [main():132] Resolve IMapUpdatePipeline interface\n[2021-12-09 14:27:40:758871] [info] [    9] [main():135] Resolve other components\n[2021-12-09 14:27:40:766994] [info] [    9] [main():137] Client components loaded\n[2021-12-09 14:27:40:767020] [info] [    9] [main():142] Initialize map update pipeline\n[2021-12-09 14:27:40:782673] [info] [    9] [main():150] Set camera parameters\n[2021-12-09 14:27:40:783056] [info] [    9] [main():157] Start map update pipeline\nlibGL error: No matching fbConfigs or visuals found\nlibGL error: failed to load driver: swrast\n[2021-12-09 14:27:40:900158] [info] [    9] [main():168] Try to get current global map from Map Update remote service\n[2021-12-09 14:27:43:430034] [info] [    9] [main():177] Map Update request terminated\n[2021-12-09 14:27:43:452630] [info] [    9] [main():186] ==&gt; Display current global map: press ESC on the map display window to end test\n\n\n\nYou will also see the global map given by the Map Update Service in a dedicated window:\n\n\n\n\n\nFigure 8. Image viewer of the first global map\n\n\n\n\nThe Mapping Service test application: Producer client\n\nOverview\n\nThis application is used to test the processing of images and poses carried out remotely by the Mapping Service. It uses a full set of pre-captured images taken from a Hololens device, included in this sample package.\n\n\nThe algorithm in this example is quite simple: it reads each pair of images and poses from the Hololens data files and requests the Mapping Service to process them, using this method from its interface:\n\n\n\n    /// @brief Request to the mapping pipeline to process a new image/pose\n    /// Retrieve the new image (and pose) to process, in the current pipeline context\n    /// (camera configuration, point cloud, key frames, key points)\n    /// @param[in] image: the input image to process\n    /// @param[in] pose: the input pose to process\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode mappingProcessRequest(const SRef&lt;datastructure::Image&gt; image,\n                                              const datastructure::Transform3Df &amp; pose) override;\n\n\n\nTraces are displayed at runtime to follow the progress of the application. In addition, it displays each image read in a specific window to be able to visually follow the path of the Hololens device.\n\n\n\nLaunch the producer application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-producer:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_producer.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mapping_multi_producer.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_producer.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Mapping Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch_producer.sh [mapping_service_url] [host_IP_address]\nlaunch_producer_vm.sh [mapping_service_url]\nlaunch_producer.bat [mapping_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch_producer.bat 172.17.0.1:50051 192.168.56.1\n\n\n\nYou can also change the default Hololens image data set used for the test by specifying the \"B\" data set (\"A\" used by default)\n\n\nFor example:\n\n\n\nlaunch_producer.bat 172.17.0.1:50051 192.168.56.1 B\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemappingmultiproducer\n\n\n\nAnd you will see something like this:\n\n\n\nMAPPING_SERVICE_URL defined: 172.17.0.1:50051\nTry to replace the Mapping Service URL in the XML configuration file...\nHOLOLENS_DATA_SET defined: B\nPath to image set to: ./data/data_hololens/loop_desktop_B\nReplace the path to image data in the XML configuration file...\nXML configuration file ready\n[2021-07-09 17:02:58:985680] [info] [  824] [main():197] Get component manager instance\n[2021-07-09 17:02:58:985973] [info] [  824] [main():201] Load Client Remote Mapping Pipeline configuration file: SolARServiceTest_Mapping_Multi_Producer_conf.xml\n[2021-07-09 17:02:59:000145] [info] [  824] [main():205] Resolve IMappingPipeline interface\n[2021-07-09 17:02:59:094552] [info] [  824] [main():216] Remote producer client: AR device component created\n[2021-07-09 17:02:59:094801] [info] [  824] [main():219] Remote producer client: AR device component created\n[2021-07-09 17:03:00:335424] [info] [  824] [main():230] Remote producer client: Init mapping pipeline result = SUCCESS\n[2021-07-09 17:03:00:336157] [info] [  824] [main():234] Remote producer client: Set mapping pipeline camera parameters result = SUCCESS\n[2021-07-09 17:03:00:336257] [info] [  824] [main():236] Remote producer client: Start remote mapping pipeline\n[2021-07-09 17:03:00:338818] [info] [  824] [main():239] Start remote producer client thread\n[2021-07-09 17:03:00:338965] [info] [  824] [main():253]\n***** Control+C to stop *****\n[2021-07-09 17:03:00:726830] [info] [  835] [operator()():73] Producer client: Send (image, pose) num 1 to mapping pipeline\n[2021-07-09 17:03:02:717961] [info] [  835] [operator()():73] Producer client: Send (image, pose) num 2 to mapping pipeline\n[2021-07-09 17:03:03:075171] [info] [  835] [operator()():73] Producer client: Send (image, pose) num 3 to mapping pipeline\n[2021-07-09 17:03:03:465519] [info] [  835] [operator()():73] Producer client: Send (image, pose) num 4 to mapping pipeline\n[2021-07-09 17:03:03:783061] [info] [  835] [operator()():73] Producer client: Send (image, pose) num 5 to mapping pipeline\n...\n\n\n\nAnd you will see the images loaded from the Hololens dataset in a dedicated window:\n\n\n\n\n\nFigure 9. Image viewer of the producer application\n\n\n\n\nThe Mapping Service test application: Viewer client\n\nOverview\n\nThis sample application is used to check the result of the processing of images and poses carried out remotely by the Mapping Service. It continually requests this service to get the current point cloud and keyframe poses resulting from all previously processed images, using this method from its interface:\n\n\n\n    /// @brief Provide the current data from the mapping pipeline context for visualization\n    /// (resulting from all mapping processing since the start of the pipeline)\n    /// @param[out] outputPointClouds: pipeline current point clouds\n    /// @param[out] keyframePoses: pipeline current keyframe poses\n    /// @return FrameworkReturnCode::_SUCCESS if data are available, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode getDataForVisualization(std::vector&lt;SRef&lt;datastructure::CloudPoint&gt;&gt; &amp; outputPointClouds,\n                                                std::vector&lt;datastructure::Transform3Df&gt; &amp; keyframePoses) const override;\n\n\n\nTraces are displayed at runtime to follow the progress of the application. In addition, it displays current point cloud and keyframe poses in a specific window to visualize the Mapping Service processing result.\n\n\n\nLaunch the viewer application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-viewer:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_viewer.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mapping_multi_viewer.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_viewer.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Mapping Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch_viewer.sh [mapping_service_url] [host_IP_address]\nlaunch_viewer_vm.sh [mapping_service_url] [host_IP_address]\nlaunch_viewer.bat [mapping_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch_viewer.bat 172.17.0.1:50051 192.168.56.1\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemappingmultiviewer\n\n\n\nAnd you will see something like this:\n\n\n\nMAPPING_SERVICE_URL defined: 172.17.0.1:50051\nTry to replace the Mapping Service URL in the XML configuration file...\nXML configuration file ready\n[2021-07-09 17:51:17:466319] [info] [20783] [main():134] Get component manager instance\n[2021-07-09 17:51:17:466693] [info] [20783] [main():138] Load Client Remote Mapping Pipeline configuration file: SolARServiceTest_Mapping_Multi_Viewer_conf.xml\n[2021-07-09 17:51:17:467416] [info] [20783] [main():142] Resolve IMappingPipeline interface\n[2021-07-09 17:51:17:579575] [info] [20783] [main():150] Start viewer client thread\n[2021-07-09 17:51:17:579735] [info] [20783] [main():155]\n***** Control+C to stop *****\n[2021-07-09 17:51:18:052272] [info] [20807] [operator()():69] Viewer client: I3DPointsViewer component created\n...\n\n\n\nAnd you will see the current point cloud and keyframe poses in a dedicated window:\n\n\n\n\n\nFigure 10. Image viewer of the viewer application\n\n\n\n\nThe Relocalization Service test application\n\nOverview\n\nThis application is used to test the pose calculation carried out remotely by the Relocalization Service. It uses a full set of pre-captured images taken from a Hololens device, included in this sample package.\n\n\nTo test the service, this application reads each image from the Hololens data files and, every \"x\" images, requests the Relocalization Service to process the current one, using this method from its interface:\n\n\n\n    /// @brief Request the relocalization pipeline to process a new image to calculate the corresponding pose\n    /// @param[in] image: the image to process\n    /// @param[out] pose: the new calculated pose\n    /// @param[out] confidence: the confidence score\n    /// @return FrameworkReturnCode::_SUCCESS if the processing is successful, else FrameworkReturnCode::_ERROR_\n    virtual FrameworkReturnCode relocalizeProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image, SolAR::datastructure::Transform3Df&amp; pose, float_t &amp; confidence) override;\n\n\n\nTraces are displayed at runtime to follow the progress of the application. In addition, it displays each image read in a specific window to be able to visually follow the path of the Hololens device.\n\n\n\nLaunch the test application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/relocalization-client:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_relocalization_client.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_relocalization_client.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_relocalization_client.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Relocalization Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch.sh [relocalization_service_url] [host_IP_address]\nlaunch_vm.sh [relocalization_service_url]\nlaunch.bat [relocalization_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50052 192.168.56.1\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicerelocalizationclient\n\n\n\nAnd you will see something like this:\n\n\n\nRELOCALIZATION_SERVICE_URL defined: 172.17.0.1:50052\nTry to replace the Relocalization Service URL in the XML configuration file...\nXML configuration file ready\n[2021-10-20 14:52:34:711132] [info] [    8] [main():101] Get component manager instance\n[2021-10-20 14:52:34:711374] [info] [    8] [main():105] Load Client Remote Relocalization Service configuration file: /.xpcf/SolARServiceTest_Relocalization_conf.xml\n[2021-10-20 14:52:34:711910] [info] [    8] [main():109] Resolve IRelocalizationPipeline interface\n[2021-10-20 14:52:35:270449] [info] [    8] [main():114] Resolve components used\n[2021-10-20 14:52:35:302757] [info] [    8] [main():121] Set relocalization service camera parameters\n[2021-10-20 14:52:35:307598] [info] [    8] [main():129] Start relocalization pipeline process\n[2021-10-20 14:52:35:310974] [info] [    8] [main():136]\n***** Control+C to stop *****\n[2021-10-20 14:52:35:716387] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:52:43:432279] [info] [    8] [main():169] Failed to calculate pose for this image\n[2021-10-20 14:52:57:486997] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:52:58:317535] [info] [    8] [main():169] Failed to calculate pose for this image\n[2021-10-20 14:53:11:988718] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:53:12:932932] [info] [    8] [main():169] Failed to calculate pose for this image\n[2021-10-20 14:53:26:661736] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:53:27:699770] [info] [    8] [main():169] Failed to calculate pose for this image\n[2021-10-20 14:53:41:562936] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:53:42:334819] [info] [    8] [main():169] Failed to calculate pose for this image\n[2021-10-20 14:53:48:776327] [info] [    8] [main():162] Send an image to relocalization pipeline\n[2021-10-20 14:53:49:454503] [info] [    8] [main():165] New pose calculated by relocalization pipeline\n...\n\n\n\nAnd you will see the images loaded from the Hololens dataset in a dedicated window.\n\n\n\n\nThe Relocalization, Mapping (and Map Update) Services test application\n\nOverview\n\nThis application can be used to test both Relocalization and Mapping services. In addition, as these two services rely on the Map Update service, this third one can also be tested using this example. This program uses a full set of pre-captured images taken from a Hololens device, included in the sample package.\n\n\nFirst of all, this test program tries to determine the transformation matrix between the Hololens coordinate system and that of the global map. To do that, it uses the 'n' first images from the Hololens dataset to request the Relocalization service for pose calculation, based on the global map provided by the Map Update service. If this treatment is successful, the program is able to compare the original Hololens pose with the new pose from the Relocalization service, and to deduce the transformation.\n\n\nFor this step, this method of the Relocalization Service interface is called:\n\n\n\n    /// @brief Request the relocalization pipeline to process a new image to calculate the corresponding pose\n    /// @param[in] image: the image to process\n    /// @param[out] pose: the new calculated pose\n    /// @param[out] confidence: the confidence score\n    /// @return FrameworkReturnCode::_SUCCESS if the processing is successful, else FrameworkReturnCode::_ERROR_\n    virtual FrameworkReturnCode relocalizeProcessRequest(const SRef&lt;SolAR::datastructure::Image&gt; image, SolAR::datastructure::Transform3Df&amp; pose, float_t &amp; confidence) override;\n\n\n\nThen, the test application sends each pair of image and pose from the Hololens dataset to the Mapping service, applying the transformation matrix if known. Thus, the Mapping pipeline will be able to constitute a new local sparse map completed by the precise poses corresponding to the movements of the Hololens device.\n\n\nFor this step, this method of the Mapping Service interface is called:\n\n\n\n    /// @brief Request to the mapping pipeline to process a new image/pose\n    /// Retrieve the new image (and pose) to process, in the current pipeline context\n    /// (camera configuration, point cloud, key frames, key points)\n    /// @param[in] image: the input image to process\n    /// @param[in] pose: the input pose to process\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode mappingProcessRequest(const SRef&lt;datastructure::Image&gt; image,\n                                              const datastructure::Transform3Df &amp; pose) override;\n\n\n\nAt the end of the program, when all the Hololens images have been analyzed, the Mapping service sends the new local map to the Map Update service, to consolidate and enrich the global sparse map.\n\n\nTraces are displayed at runtime to follow the progress of the application. In addition, it displays each image read in a specific window to be able to visually follow the path of the Hololens device.\n\n\n\n\n\n\n\n\nAt any time during the execution of this example, you can use the Mapping Service viewer client to visualize the current local map being creating.\n\n\n\n\n\n\n\n\n\n\n\nAt the end of this application, you can use the Map Update Service test application to view the new global map.\n\n\n\n\n\n\nLaunch the test application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-relocalization-client:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_relocalization_client.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mapping_multi_relocalization_client.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mapping_multi_relocalization_client.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Relocalization Service URL, your Mapping Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch.sh [relocalization_service_url] [mapping_service_url] [host_IP_address]\nlaunch_vm.sh [relocalization_service_url] [mapping_service_url]\nlaunch.bat [relocalization_service_url] [mapping_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50052 172.17.0.1:50051 192.168.56.1\n\n\n\nYou can also change the default Hololens image data set used for the test by specifying the \"B\" data set (\"A\" used by default)\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50052 172.17.0.1:50051 192.168.56.1 B\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemappingmultirelocalizationclient\n\n\n\nAnd you will see something like this:\n\n\n\nRELOCALIZATION_SERVICE_URL defined: 172.17.0.1:50052\nReplace the Relocalization Service URL in the XML configuration file...\nMAPPING_SERVICE_URL defined: 172.17.0.1:50051\nReplace the Mapping Service URL in the XML configuration file...\nHOLOLENS_DATA_SET defined: A\nPath to image set to: ./data/data_hololens/loop_desktop_A\nReplace the path to image data in the XML configuration file...\nXML configuration file ready\n[2021-08-02 16:42:29:986811] [info] [24091] [main():228] Get component manager instance\n[2021-08-02 16:42:29:987168] [info] [24091] [main():232] Load configuration file: SolARServiceTest_Mapping_Multi_Relocalization_conf.xml\n[2021-08-02 16:42:29:987681] [info] [24091] [main():236] Resolve IMappingPipeline interface\n[2021-08-02 16:42:30:018976] [info] [24091] [main():239] Resolve IRelocalizationPipeline interface\n[2021-08-02 16:42:30:041947] [info] [24091] [main():250] Remote producer client: AR device component created\n[2021-08-02 16:42:30:042243] [info] [24091] [main():253] Remote producer client: AR device component created\n[2021-08-02 16:42:30:055965] [info] [24091] [main():264] Remote mapping client: Init mapping pipeline result = SUCCESS\n[2021-08-02 16:42:30:056461] [info] [24091] [main():273] Remote mapping client: Set mapping pipeline camera parameters result = SUCCESS\n[2021-08-02 16:42:30:487307] [info] [24091] [main():277] Remote relocalization client: Init relocalization pipeline result = SUCCESS\n[2021-08-02 16:42:30:489220] [info] [24091] [main():286] Remote relocalization client: Set relocalization pipeline camera parameters result = SUCCESS\n[2021-08-02 16:42:30:489261] [info] [24091] [main():288] Remote mapping client: Start remote mapping pipeline\n[2021-08-02 16:42:30:490200] [info] [24091] [main():292] Start remote mapping client thread\n[2021-08-02 16:42:30:490391] [info] [24091] [main():297] Start remote relocalization client thread\n[2021-08-02 16:42:30:490508] [info] [24091] [main():302] Read images and poses from hololens files\n[2021-08-02 16:42:30:490538] [info] [24091] [main():304]\n***** Control+C to stop *****\n[2021-08-02 16:42:31:082000] [info] [24091] [main():330] Add image to input drop buffer for relocalization\n[2021-08-02 16:42:31:082029] [info] [24091] [main():341] Add pair (image, pose) to input drop buffer for mapping\n[2021-08-02 16:42:31:082150] [info] [24102] [operator()():82] Mapping client: Send (image, pose) to mapping pipeline\n[2021-08-02 16:42:31:082599] [info] [24103] [operator()():107] Relocalization client: Send image to relocalization pipeline\n[2021-08-02 16:42:31:389366] [info] [24091] [main():341] Add pair (image, pose) to input drop buffer for mapping\n[2021-08-02 16:42:31:389448] [info] [24102] [operator()():82] Mapping client: Send (image, pose) to mapping pipeline\n...\n\n\n\nIf the relocalization processing is successful, you will see the following trace:\n\n\n\n[2021-08-02 16:42:32:544970] [info] [24103] [operator()():111] =&gt; Relocalization succeeded\n[2021-08-02 16:42:32:544999] [info] [24103] [operator()():113] Hololens pose:    0.996649  -0.0777834   0.0252523  -0.0062405\n  -0.045278   -0.781977   -0.621658 -0.00958775\n  0.0681019    0.618432   -0.782881   -0.143014\n          0           0           0           1\n[2021-08-02 16:42:32:545107] [info] [24103] [operator()():114] World pose:    0.996321  -0.0810918    0.027727 -0.00972971\n -0.0469172   -0.786846   -0.615364  -0.0186298\n  0.0717178    0.611799   -0.787756   -0.138227\n          0           0           0           1\n[2021-08-02 16:42:32:545174] [info] [24103] [operator()():118] Transformation matrix from Hololens to World:    0.999993   0.0010635 -0.00400569 -0.00405193\n  -0.001096    0.999969 -0.00805077  -0.0102006\n 0.00399664  0.00805406     0.99996  0.00488381\n          0           0           0           1\n\n\n\nAt any time, you will see the images loaded from the Hololens dataset in a dedicated window:\n\n\n\n\n\nFigure 11. Image viewer of the test application\n\n\n\n\nThe Mapping and Relocalization Front End test application\n\nOverview\n\nThis application is used to test the Mapping and Relocalization Front End service. In addition, as this service relies on the Relocalization service, the Mapping service, and also the Map Update service, these can also be tested using this example. This program uses a full set of pre-captured images taken from a Hololens device, included in the sample package.\n\n\nThe algorithm of this test application is very simple: it first initializes the Mapping and Relocalization Front End, giving it the parameters of the camera used to capture the Holones set of images.\n\n\nFor this step, this method of the Front End Service interface is called:\n\n\n\n    /// @brief Set the camera parameters\n    /// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n    /// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\n    virtual FrameworkReturnCode setCameraParameters(const SolAR::datastructure::CameraParameters &amp; cameraParams) = 0;\n\n\n\nThen, the test application sends each pair of image and pose from the Hololens dataset to the Front End service.\n\n\nFor this step, this method of the Mapping And Relocalization Front End Service interface is called:\n\n\n\n    /// @brief Request the asynchronous relocalization pipeline to process a new image to calculate\n    /// the corresponding 3D transformation to the SolAR coordinates system\n    /// @param[in] image: the image to process\n    /// @param[in] pose: the original pose in the client coordinates system\n    /// @param[in] timestamp: the timestamp of the image\n    /// @param[out] transform3DStatus: the status of the current 3D transformation matrix\n    /// @param[out] transform3D : the current 3D transformation matrix (if available)\n    /// @param[out] confidence: the confidence score of the 3D transformation matrix\n    /// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\n    virtual FrameworkReturnCode relocalizeProcessRequest(\n                  const SRef&lt;SolAR::datastructure::Image&gt; image,\n                  const SolAR::datastructure::Transform3Df &amp; pose,\n                  const std::chrono::system_clock::time_point &amp; timestamp,\n                  TransformStatus &amp; transform3DStatus,\n                  SolAR::datastructure::Transform3Df &amp; transform3D,\n                  float_t &amp; confidence) = 0;\n\n\n\nTraces are displayed at runtime to follow the progress of the application. In addition, it displays each image read in a specific window to be able to visually follow the path of the Hololens device.\n\n\n\n\n\n\n\n\nAt any time during the execution of this example, you can use the Mapping Service viewer client to visualize the current local map being creating.\n\n\n\n\n\n\n\n\n\n\n\nAt the end of this application, you can use the Map Update Service test application to view the new global map.\n\n\n\n\n\n\nLaunch the test application\n\nFirst, you have to install the test application image on your Docker engine:\n\n\n\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mappingandrelocalizationfrontend-client:x.y.z\n\n\n\nThen, get the script file used to launch the Docker container of the test application from one of these URL:\n\n\nLinux: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mappingandrelocalizationfrontend_client.sh\n\n\nLinux VM: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_vm_mappingandrelocalizationfrontend_client.sh\n\n\nWindows: https://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients/launch_mappingandrelocalizationfrontend_client.bat\n\n\n\n\n\n\n\n\nAt the end of the \"docker run\" command (last line of the scripts), you should replace the default version of the service by the current one.\n\n\n\n\n\nYou can now launch the Docker container using the script file, giving your Mapping and Relocalization Front End Service URL and, if you do not use a Linux Virtual Machine, your computer local IP address (to export graphical display) as parameter:\n\n\n\nlaunch.sh [mapping_and_relocalization_frontend_service_url] [host_IP_address]\nlaunch_vm.sh [mapping_and_relocalization_frontend_service_url]\nlaunch.bat [mapping_and_relocalization_frontend_service_url] [host_IP_address]\n\n\n\nFor example:\n\n\n\nlaunch.bat 172.17.0.1:50055 192.168.56.1\n\n\n\n\n\n\n\n\n\nOn Windows host, do not forget to start the X Server manager before running the test application\n\n\n\n\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n\n\ndocker logs [-f] solarservicemappingandrelocalizationfrontendclient\n\n\n\nAnd you will see something like this:\n\n\n\nMAPPINGANDRELOCALIZATIONFRONTEND_SERVICE_URL defined: 0.0.0.0:50055\nTry to replace the MappingAndRelocalizationFrontend Service URL in the XML configuration file...\nXML configuration file ready\n[2021-12-09 15:20:32:915391] [info] [    9] [main():106] Get component manager instance\n[2021-12-09 15:20:32:915620] [info] [    9] [main():114] Load Client Remote Service configuration file: /.xpcf/SolARServiceTest_MappingAndRelocalizationFrontend_conf.xml\n[2021-12-09 15:20:32:919253] [info] [    9] [main():121] Mapping and relocalization front end service component created\n[2021-12-09 15:20:32:919272] [info] [    9] [main():128] Initialize the service\n[2021-12-09 15:20:32:931482] [info] [    9] [main():136] Producer client: AR device component created\n[2021-12-09 15:20:32:931621] [info] [    9] [main():139] Remote producer client: AR device component created\n[2021-12-09 15:20:32:939194] [info] [    9] [main():147] Set camera paremeters for the service\n[2021-12-09 15:20:32:940457] [info] [    9] [main():154] Start the service\n[2021-12-09 15:20:32:941359] [info] [    9] [main():161] Read images and poses from hololens files\n[2021-12-09 15:20:32:941386] [info] [    9] [main():162]\n***** Control+C to stop *****\n[2021-12-09 15:20:33:055380] [info] [    9] [main():179] Send image and pose to service\n[2021-12-09 15:20:33:248617] [info] [    9] [main():179] Send image and pose to service\n[2021-12-09 15:20:33:397523] [info] [    9] [main():179] Send image and pose to service\n...\n\n\n\nIf the 3D transformation matrix between the client coordinate system and the SolAR coordinate systeme if found (by the Relocalization Service), you will see the following trace:\n\n\n\n[2021-12-09 15:20:34:810304] [info] [    9] [main():189] New 3D transformation =\n  -0.989591    0.00656944  -0.143773    -0.153553\n   0.143782    0.000883323 -0.989609    -0.716952\n  -0.00637388 -0.99998     -0.00181631  -0.796267\n   0           0            0            1\n\n\n\nAt any time, you will see the images loaded from the Hololens dataset in a dedicated window.\n\n\n\n\n\n\n\n\nThe complete projects of these test applications are available on the SolAR Framework GitHub:\nhttps://github.com/SolarFramework/Service-MapUpdate/tree/0.11.0/SolARService_MapUpdate/tests/SolARServiceTest_MapUpdate\nhttps://github.com/SolarFramework/Service-MapUpdate/tree/0.11.0/SolARService_MapUpdate/tests/SolARServiceTest_MapUpdate_DisplayMap\nhttps://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi/tests/SolARServiceTest_Mapping_Multi_Producer\nhttps://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi/tests/SolARServiceTest_Mapping_Multi_Viewer\nhttps://github.com/SolarFramework/Service-Relocalization/tree/0.11.0/SolARService_Relocalization/tests/SolARServiceTest_Relocalization\nhttps://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi/tests/SolARServiceTest_Mapping_Multi_Relocalization\nhttps://github.com/SolarFramework/Service-Relocalization/tree/0.11.0/SolARService_MappingAndRelocalizationFrontend/tests/SolARServiceTest_MappingAndRelocalizationFrontend\n\n\n\n\n\n\n\n",
      "id": 4
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "best practises",
      "content": "BEST practises\n\nTable of Contents\n\nBest practices\nSolAR guidelines\n\nComponent Interfaces\nModules and Components\nComponent Implementations\nModules\nPipepline Management\n\n\nLogs to help debugging\ncoding rules\n\nProject organization\nFiles\nC / C++ Coding Rules\nLanguage features\nLibraries and headers\nNaming conventions\nDesign conventions\nLayout conventions\nTracing and debugging\nError handling\nMiscellaneous conventions\nDocumentation\nC/C++ Performance rules\nTools\n\n\nAppendix A.\tRules management\n\nI.\tResponsibility\nII.\tDeviation\nIII.\tTraining\nIV.\tControl\n\n\n\n\n\nBest practices\n\n\n\n\n\nSolAR guidelines\n\n\nThe following rules shall be used for every addition/modification to the SolAR project.\nThis encompasses the SolAR framework and the GUI interface and unless otherwise specified, these rules shall apply to both.\n\n\nComponent Interfaces\n\n1.1. Solar component interfaces are virtual base classes. They shall inherit from class org::bcom::xpcf::IComponentIntrospect\n\n\n1.2. SolAR Interfaces are defined in a dedicated header file (.h) whose name shall begin with a capital I followed with the name of the abstract class it refers to, e.g. ICamera.h.\n\n\n1.3. SolAR Interfaces shall not contain member variables, it is an abstract class without committing to a particular implementation of the class. If you need member variables, declare them in the implementation of the component\n\n\n1.4. A component interface must be a abstract class, meaning that all its methods must be virtual.\n\n\n1.5. Solar Framework is organized hierarchically via dedicated directories and namespaces. Currently, concerning the interfaces, this organization is as follows :\n\n\n\n\n\n\n\n\nDirectory\nnamespace\n\n\n\n\nSolARFramework/interfaces/api/display\nSolAR::api::display\n\n\nSolARFramework/interfaces/api/features\nSolAR::api::features\n\n\nSolARFramework/interfaces/api/fusion\nSolAR::api::fusion\n\n\nSolARFramework/interfaces/api/geom\nSolAR::api::geom\n\n\nSolARFramework/interfaces/api/image\nSolAR::api::image\n\n\nSolARFramework/interfaces/api/input/devices\nSolAR::api::input::devices\n\n\nSolARFramework/interfaces/api/input/files\nSolAR::api::input::files\n\n\nSolARFramework/interfaces/api/loop\nSolAR::api::input::loop\n\n\nSolARFramework/interfaces/api/output/files\nSolAR::api::input::output::files\n\n\nSolARFramework/interfaces/api/pipeline\nSolAR::api::input::pipeline\n\n\nSolARFramework/interfaces/api/pointcloud\nSolAR::api::input::pointcloud\n\n\nSolARFramework/interfaces/api/reloc\nSolAR::api::input::reloc\n\n\nSolARFramework/interfaces/api/segm\nSolAR::api::input::segm\n\n\nSolARFramework/interfaces/api/sink\nSolAR::api::sink\n\n\nSolARFramework/interfaces/api/slam\nSolAR::api::slam\n\n\nSolARFramework/interfaces/api/solver/map\nSolAR::api::solver::map\n\n\nSolARFramework/interfaces/api/solver/pose\nSolAR::api::solver::pose\n\n\nSolARFramework/interfaces/api/source\nSolAR::api::solver::source\n\n\nSolARFramework/interfaces/api/tracking\nSolAR::api::solver::tracking\n\n\n\n\n1.6. Namespaces should use lower case.\n\n\n1.7. Any new interface must fall into one of these categories. Yet, if needed, one may ask the SolAR Team to add a new one to fulfill a particular need not covered by the current organization.\n\n\n1.8. If possible, an abstract interface of a component must define only one processing method. Exception may be allowed if your processing method need take as an input or output only one or a collection of several objects, as for instance:\n\n\n\nvirtual void drawCircle(SRef&lt;Point2Df&gt; point, unsigned int radius, int thickness, std::vector&lt;unsigned int&gt; &amp; bgrValues, SRef&lt;Image&gt; displayImage) = 0;\nvirtual void drawCircles(std::vector&lt;SRef&lt;Point2Df&gt;&gt;&amp; points, unsigned int radius, int thickness, SRef&lt;Image&gt; displayImage) = 0;\n\n\n\nor if the processing method can take as input our output parameter an object or an inherited object, as for instance:\n\n\n\nvirtual void drawCircles(std::vector&lt;SRef&lt;Point2Df&gt;&gt;&amp; points, unsigned int radius, int thickness, SRef&lt;Image&gt; displayImage) = 0;\nvirtual void drawCircles(std::vector&lt;SRef&lt;Keypoint&gt;&gt;&amp; keypoints, unsigned int radius, int thickness, SRef&lt;Image&gt; displayImage) = 0;\n\n\n\n1.9. A 128-bit UUID (Universal Unique IDentifier) shall be associated to any virtual interface and explicitly quoted in the interface header file, preferably after the class definition.\n\n\nThe syntax is the following :\n\n\n\nXPCF_DEFINE_INTERFACE_TRAITS(SolARnamespaces::IInterfaceClassName,\n                             &quot;aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee&quot;,\n                             &quot;Interface name&quot;,\n                             &quot;Interface description&quot;);\n\n\n\nAn example :\n\n\n\nXPCF_DEFINE_INTERFACE_TRAITS(SolAR::api::display::I2DOverlay,\n                             &quot;62b8b0b5-9344-40e6-a288-e609eb3ff0f1&quot;,\n                             &quot;I2DOverlay&quot;,\n                             &quot;SolAR::I2DOverlay interface&quot;);\n\n\n\n1.10. The header file shall contain Doxygen documentation code in order to automatically generate the interface description web page during the continuous integration process used by the Solar Integration Team.\nIn particular the purpose of the interface shall be documented, as well as each virtual method with its input/ouput parameters.\nFor instance for class documentation:\n\n\n\n/**\n * @class I2DOverlay\n * @brief Drawing interface to overlay 2D information on top of an image.\n *\n * This class provides drawing methods to overlay 2D debug informations on top of an image.\n */\n\n\n\nand for method documentation:\n\n\n\n/// @brief Draw a Squared Binary Pattern.\n/// @param[in] pattern The squared binary pattern to display.\n/// @param[in,out] displayImage The image on which the squared binary pattern will be drawn (on the whole image).\nvirtual void drawSBPattern (SRef&lt;SquaredBinaryPattern&gt; pattern, SRef&lt;Image&gt; displayImage) = 0;\n\n\n\n\nModules and Components\n\nModules are the placeholders for the components. Basically, they are defined to reflect a particular type of implementation, based e.g. on a particular technology or a particular provider, etc &#8230;&#8203;\n\n\nA component cannot exist by itself. It must be included in a module but a module may contain only one component if needed.\n\n\nThey are are few rules that modules and components must conform to in order to be usable by SolAR. This is explained in the following.\n\n\n\n\nModules are delivered as shared libraries (windows or linux)\n\n\nThe recommended naming convention is  ModuleName, where name should reflect a characteristic of the module, e.g. ModuleOpencvFree.\n\n\nComponents are declared inside a namespace according to the following naming convention : SolAR::MODULES::NAMEOFMODULE\n\n\nA 128-bit UUID (Universal Unique IDentifier) shall be associated to every module\n\n\nA 128-bit UUID (Universal Unique IDentifier) shall be associated to every component included in a module\n\n\nA Module exports its components via a dedicated Export API defined in a header file named NameOfModuleAPI.h, such :\n\n\n\n\n\n/**\n * Copyright and license notice ......\n */\n\n#ifndef NAME_API_H\n    #define NAMEOFMODULE_API_H\n    #if _WIN32\n        #ifdef NameOfModule_API_DLLEXPORT\n            #define NAMEOFMODULE_EXPORT_API __declspec(dllexport)\n        #else //NameOfModule_API_DLLEXPORT\n            #define NAMEOFMODULE_EXPORT_API __declspec(dllimport)\n        #endif //NameOfModule_API_DLLEXPORT\n    #else //_WIN32\n        #define NAMEOFMODULE_EXPORT_API\n    #endif //_WIN32\n    #include \"NameOfModule_traits.h\"\n#endif //NAMEOFMODULE_API_H\n\n\n\nand where 'NameOfModule_traits.h'  exposes the list of components contained in the module. Follows a generic example of this file.\n\n\n\n#define NAMEOFMODULE_TRAITS_H\n\n#include &quot;xpcf/component/ComponentTraits.h&quot;\n\nnamespace SolAR {\nnamespace MODULES {\nnamespace NAME {\nclass Component1;\nclass Component2;\n}\n}\n}\n\nXPCF_DEFINE_COMPONENT_TRAITS(SolAR::MODULES::NAME::Component1,\n                             &quot;aaaaaaaa-bbbb-cccc-dddd-eeeeeeee&quot;,\n                             &quot;Component1&quot;,\n                             &quot;SolAR::MODULES::NAME::Component1 definition&quot;)\n\n\nXPCF_DEFINE_COMPONENT_TRAITS(SolAR::MODULES::NAME::Component1,\n                             &quot;ffffffff-gggg-hhhh-iiii-jjjjjjjj&quot;,\n                             &quot;Component2&quot;,\n                              &quot;SolAR::MODULES::NAME::Component2 definition&quot;)\n\n\n#endif // NAMEOFMODULE_TRAITS_H\n\n\n\n\n\nThere will be a code file named NameOfModule.cpp that shall contain the Module UUID as well as which components are included and exposed in the module. The syntax is as follows.\nNameOfModule.cpp:\n\n\n\n\n\n#include \"xpcf/module/ModuleFactory.h\"\n\n#include \"component1.h\"\n#include \"component2.h\"\n\nnamespace xpcf=org::bcom::xpcf;\n\nXPCF_DECLARE_MODULE(\"kkkkkkkk-llll-mmmm-nnnn-oooooooo\", \"ModuleName\", \"ModuleDescription\")\n\n\nextern \"C\" XPCF_MODULEHOOKS_API xpcf::XPCFErrorCode XPCF_getComponent(const boost::uuids::uuid&amp; componentUUID,SRef&lt;xpcf::IComponentIntrospect&gt;&amp; interfaceRef)\n{\n     xpcf::XPCFErrorCode errCode = xpcf::XPCFErrorCode::_FAIL;\n     errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::NAME::component1&gt;(componentUUID,interfaceRef);\n     if (errCode != xpcf::XPCFErrorCode::_SUCCESS)\n     {\n         errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::NAME::component2&gt;(componentUUID,interfaceRef);\n     }\n    return errCode;\n}\n\nXPCF_BEGIN_COMPONENTS_DECLARATION\nXPCF_ADD_COMPONENT(SolAR::MODULES::Name::component1)\nXPCF_ADD_COMPONENT(SolAR::MODULES::Name::component2)\nXPCF_END_COMPONENTS_DECLARATION\n\n\n\nIn the above mentioned code, a two-component module is considered.\n\n\nEach component is implemented via a class (.cpp/.h).\n\n\nComponents should be implemented for processing.\nIt should take in parameters and deliver out components and data.\n\n\n2.1. Data structures should be implemented for the data flow, meaning the data that will be exchanged between components at runtime thanks to the processing methods of the components. To optimize vision pipelines developped thanks to the SolAR Framework, it is of real need to take care to data structure optimization (reduce the memory copy, favour a quick access to data, etc.)\n\n\n2.2. A data structure must be defined in a namespace SolAR::datastructure\n\n\n2.3. The header and cpp files must be put under the datastructure directory.\n\n\n2.4. The header file shall contains Doxygen documentation code in order to automatically generate the data structure description web page during the continuous integration process used by the Solar Integration Team.\nFor instance for class documentation:\n\n\n\n/**\n  * @class Image\n  * @brief Specifies the Image base class.\n  *\n  * This class provides an image abstraction for SolAR\n  */\n\n\n\nand for method documentation:\n\n\n\n/** @brief  reserves new space depending on the image layers and bitspercomponent infos\n  *  @param width: width of the image\n  *  @param height: height of the image\n  */\n  void setSize(uint32_t width, uint32_t height);\n\n\n\n\n\n\n\n\n\nPlease refer to existing SolAR component interfaces and take them as examples.\n\n\n\n\n\n\nComponent Implementations\n\nA component implements a low-level vision processing in order to offer much more flexibility. They can be interconnected together to create  high-level and real-time vision pipeline (localization, 3D reconstruction, etc.).\n\n\nThat is why a component should take in parameters and deliver out data structures to ease their connection (data flow).\n\n\nWhen you intent to  create a component, first verify that :\n\n\n\n\na SolAR interface already exists for your kind of components. If so, please use the specified interface.\nif not, or if you think the existing interfaces do not totally fit your need, please contact the SolAR team.\n\n\n\n\nPlease look at the organization of existing modules and components.\n\n\n3.1. Any component must be embedded in a module. Modules are used to easily publish one or a group of components to the solAR community. If you want to create a new module, please copy the existing structure of SolAR modules.\n\n\n3.2. Your components should have explicit names (that means, that ideally, we do not need to read documentation to understands what it is for and on which third party it is based). Please refer to existing components for naming examples.\n\n\n3.3. Your components must inherit from component interfaces defined by the SolARFramework. If no component interface fits your need or if you think an existing interface should be extended, please contact the SolAR team.\n\n\n3.4. Your components can define their own attributes which should be defined as private. No attributes can be shared by other components. If you want to share data between components, pass them as attributes of the processing method.\n\n\n3.5. Your components can define their own methods which should be defined as private. The only public methods should be the ones defined by the abstract interfaces.\n\n\n3.6. Processing methods should not pass control or configuration parameters, but only the parameters representing the input and output dataflow of the component. If a parameter does not have vocation to change at each pass of the pipeline, this parameter must be moved as a private attribute of the component.\n\n\n3.7. All methods defined by the abstract interface you are inheriting must be implemented in your component.\n\n\n3.8. Your component should inherit from ConfigurableBase if you want to configure it thanks to an external file (please refer to other components, for instance ImageFilter). So you won&#8217;t have to recompile your pipeline if you want to change the configuration of its components.\n\n\nexample :\n\n\n\nSolARImageFilterBinaryOpencv::SolARImageFilterBinaryOpencv():ConfigurableBase(xpcf::toUUID&lt;SolARImageFilterBinaryOpencv&gt;())\n{\n    declareInterface&lt;api::image::IImageFilter&gt;(this);\n}\n\n\n\n3.9. If you want to initialize the value of component attributes at runtime thanks to an external file, you need to wrap it to a naming string. Thus, you will be able to configure your pipeline by editing an external file defining this attribute by its given name.\nexample :\n\n\n\n    declareProperty(&quot;min&quot;, m_min);\n\n\n\n3.10. There is no need to implement a setParameter and a getParameter method in a SolAR component. To configure a parameter at runtime in your code, get the wanted property of your component through the IConfigurable interface and set it to the desired value.\n\n\nexample :\n\n\n\nauto imageFilterBinary =xpcfComponentManager-&gt;resolve&lt;image::IImageFilter&gt;();\nimageFilterBinary-&gt;bindTo&lt;xpcf::IConfigurable&gt;()-&gt;getProperty(&quot;min&quot;)-&gt;setIntegerValue(-1);\n\n\n\n3.11. All Dataflow parameters of a processing method that are not modified by the function must be const.\n\n\n3.12. All Dataflow parameters of a processing method that are SRef and that can be instantiated by the method must be a reference on a SRef, otherwise do not use a reference on a SRef. In the following example, the ouptut image of a ImageConvertor can be auotmatically instantiated according to the size of the input image:\n\n\n\nvirtual FrameworkReturnCode convert(const SRef&lt;Image&gt; imgSrc, SRef&lt;Image&gt;&amp; imgDst) = 0;\n\n\n\n3.13. The namespace of your component must respect the following form: modules::module_name, where module_name refers to the name of your module.\n\n\n3.14. The namespaces of your component should use lower case.\n\n\n\n\n\n\n\n\nPlease refer to existing SolAR component implementations and take them as examples.\n\n\n\n\n\n\n\n\n\n\n\nPlease refer to existing SolAR components and take them as examples.\n\n\n\n\n\n3.15. Each parameter of each method of your component should be tested in your code, to detect bad parameter values. When an incorrect value is detected, the method should return an errorcode. This will help developers for implementing and debugging their pose estimation solution.\n\n\n3.16. Each component should have a corresponding \"simple\" unit test . The unit test should test component creation, configuration, and simple use of this component, with normal cases. The unit test should be commented, to help other contributors to understand your component.\n\n\n3.17. Each component should have unit tests with limit cases, as bad instanciation (bad values of attributes), bad use of components (for example: try to load an image that does not exist), and test every error code of each method.\n\n\n\nModules\n\n4.1. A Module is a group of components with the same implementation basis :\n\n\n\n\nsame third parties\n\n\nsame authors\n\n\nsame Intellectual Properties\n\n\n\n\n4.2. For each module, there should be an XML file describing the components with their UID. Please refer to SolarModuleTools example, and XPCF documentation to do it.\n\n\n4.3. In each module, there should be a Module unit test project, gathering the unit tests of all components.\n\n\n\nPipepline Management\n\nSolAR architecture is based on a pipeline manager called XPCF and implemented by b&lt;&gt;com.\nPlease refer to the XPCF github project link: XPCF GITHUB repository to know more about XPCF.\n\n\n\n\n\n\n\n\nPlease be aware that components must be interoperable thanks to XPCF, and that is why thoses guidelines should be used.\n\n\n\n\n\n\n\n\nLogs to help debugging\n\n\nA SolARLog tool has been defined in order to help you to debug and test your programs.\n\n\nSolarLog is based on spdlog, and is managed as a singleton, so that you will have at maximum 2 loggers : 1 console and 1 file.\n\n\nYou have  2 log modes\n\n\n\n\nconsole\n\n\nfile\n\n\n\n\nPlease initiate your console logger with  LOG_ADD_LOG_TO_CONSOLE or file logger thanks to  LOG_ADD_LOG_TO_FILE.\n\n\nYou will easily find examples in SolAR sample codes.\n\n\nPlease use one of this macro to log your data, depending on the severity you want :\n\n\n\n\nLOG_TRACE: create a TRACE of INFO level\n\n\nLOG_INFO(fmt, &#8230;&#8203;) : create a log of INFO level\n\n\nLOG_DEBUG(fmt, &#8230;&#8203;) : create a log of DEBUG level\n\n\nLOG_CRITICAL(fmt, &#8230;&#8203;) : create a log of CRITICIAL level\n\n\nLOG_WARNING(fmt, &#8230;&#8203;) : create a log of WARNING level\n\n\nLOG_ERROR(fmt, &#8230;&#8203;)  : create a log of ERROR level\n\n\nLOG_FLUSH : can be used to force logs flush (console or file mode)\n\n\nLOG_RELEASE : is used to release the logger (should be used at the end of a program).\n\n\n\n\n\n\ncoding rules\n\n\nProject organization\n\nIn order to ease the source code management, we should follow the same hierarchy for each module. A module typically becomes one dynamic or static library (dll or lib). To make the code more accessible and friendly for everyone, developers should follow the rules below:\n\n\n\nFiles\n\n\n\n\n\n\n\nC++/C modules\n- Project-wide definitions must be in a dedicated header file (for instance definitions.h)\n- Each module may have a common .h file that contains all common constants, macros, enum, structures… It should not contain elements that are not common to classes in the module.\n\n\nC++/C source files\nC++/C source files should contain the implementation of only one class (except for very small private utility classes related to that class).\nSee Naming conventions for naming conventions.\n\n\nC++/C headers files\nC++/C headers files should contain the declaration of only one class (except for very small public utility classes related to that class).\nSee Naming conventions for naming conventions.\n\n\nDirectory layout for each module\nThe directory layout for each module should be as described in:\n coding_rules.adoc chapter Project organisation\n\n\n\n\n\nC / C++ Coding Rules\n\nWhy restricting C++?\n\n\nEven if compilers now correctly compile even the most advanced C++ language features, some advanced features make the code overly complex and difficult to maintain.\n\n\nWhy restricting C?\n\n\nC can be written in many ways to do the same things but some ways are more obfuscated and offer less robustness.\n\n\nThen, what language to use?\n\n\nb&lt;&gt;com is using a mix of C and C++ based on existing code, external dependencies (like platform types, SDK, etc).\n\n\nThat’s why the following rules make sense in our environment and, in order to facilitate porting and code review, developers must use the set of rules defined below. An example of code and header can be found in annexes A1 and A2.\n\n\n\nLanguage features\n\nAs it is very easy to make unreadable and non-understandable C code, here are a few rules/restrictions to follow for the C language itself:\n\n\n\n\n\n\n\n\nTemplates\nTemplates should be used following the \"KISS\" principle. Extreme template programming must be avoided and replaced with ad-hoc design to ensure code maintainability.\n\n\nExceptions\nExceptions must not be used outside package boundary (i.e. outside a static or dynamic library no exception must be thrown).\n\n\nOperator overload\nOperator overloading should be used appropriately.\n\n\nWeird language features of C++\nWeird language features of C++ must not be used, especially:\n- static variables that invoke constructors at initialization time  (except for some very special cases, such as the singleton pattern)\n- run-time type information (‘casts’ can fail at run-time)\n\n\nBit fields\nBit fields must not be used for the following reasons: they are not portable because the implementation of bit fields is left to the compiler manufacturer according to the platform; and usage of bit fields is usually inefficient in terms of code size. Use one variable instead of each bit field.\nConsider using the STL bitset template class instead.\n\n\nNamespaces\nNamespaces may be used for std classes to avoid the full Class::methName statement. But for internal classes with ambiguous names, try to always use their full class name.\n\n\n'goto' keyword\nThe 'goto' keyword should not be used, and if it is, it can only be used to jump to the end of a method for error recovery.\nBy considering the architecture of a method, this keyword can nearly always be avoided.\nSee annex A3 for examples.\n\n\n‘continue’ and ‘break’ keywords\nThe ‘continue’ statement should not be used; the ‘break’ statement should not be used except inside switch statements.\n\n\n‘return’\nThe 'return' keyword may be used anywhere in the code.\nHowever, it requires that the no dynamic allocation rule is respected (see below) and that no vital code is skipped.\nIt also requires that all synchronization is made through C++11 lock_guard objects.\n\n\nC++ types\nTypes such as bool, etc. may be used if they are not platform dependant.\n\n\nC++ iostreams\nIostreams should be used.\n\n\nDynamic memory allocations\nDynamic memory allocation should be avoided.\nMost of the time, C++ offers semantics that allows no dynamic allocation design.\nmalloc/free, new/delete should be used during initialization sequence (in the class constructors for instance)\nDuring run time, explicit memory allocations should not be used to avoid memory fragmentation and leaks.\nIf an array is needed at some point during the execution of the program, this need should preferably be planned and reserved at the initialization sequence.\nLocal arrays are recommended if they are small in size (no more than 16-32 values).\n\n\nArrays\nuse STL&#8217;s vector&lt;T&gt; and array&lt;T&gt; instead of old C-style arrays, as C-style arrays don&#8217;t behave as expected with C++ objects.\n\n\nDynamically allocation of member (aggregated/composed) object\ndynamic allocation of a \"local\" object must occur only when the inner object lifetime is different from the \"hosting\" class (aggregation case) OR when the used framework doesn&#8217;t allow the creation of the object upon class creation (for instance, when no default constructor is available).\nWhen dynamically allocating inner objects, prefer the use of STL&#8217;s shared_ptr or unique_ptr (depending on the inner object lifetime), to ensure proper behavior upon exception throwing &#8230;&#8203;\n\n\nClass instantiations during run time\nAll the necessary classes, arrays, structures should be present, allocated and initialized before run time (during the initialization sequence) except for transient objects (objects operated by a pipeline should be created at the beginning and destroyed at the end for instance)\n\n\nConstants\nConstants must be declared using static const or enum for enumeration of constants. #define must be avoided (language evolution tends to avoid #statements).\n\n\nconst\nconst keyword MUST be used. It must be used appropriately.\nUsed on method parameters, it clearly shows when a parameter is an input, input/output or output parameter.\nUsed for methods, it clearly shows that const methods leave the underlying object members unmodified. (typically getters should be const methods).\n\n\n\n\nC++11\n\n\n\n\n\n\n\n\nMove semantics\nMove semantics must not be used. In most cases, move semantics can be replaced with designing the method using C++ references upon output parameters, or with the use of STL shared_ptr.\n\n\nLambdas expressions\n\n\n\nThreads\nC++11 threads and related facilities (mutex, scoped lock_guard, future &#8230;&#8203;) must be used\n\n\nLiterals\n\n\n\nRange for\nRange for must be used to work on containers as it improves the code readibility\n\n\nauto\nauto keyword use is recommended when it simplifies the code readibility.\nIt allows to avoid explicit typing of objects when there&#8217;s not a strong interest to :\n- for iterators\n- for temporary objects\n\n\nsmart pointers\nSTL&#8217;s smart pointers must be used. When possible, it should replace most of old C-style pointers (DLL boundary issue ?)\n\n\nFunction objects\nstd::function, std::bind, std::mem_fn &#8230;&#8203; readibility, maintainability issues ?\n\n\nSTL containers initializer list\n\n\n\nDate and time\nSTL chrono, useful also for performance counters\n\n\nSTL\narray&lt;T&gt;, bitset&lt;T&gt;\n\n\n\n\n\nLibraries and headers\n\n\n\n\n\n\n\nSTL containers\nSTL should be used for container types, such as vectors, lists, maps, etc. (but must not be used across DLL boundaries).\n\n\nC++ strings\nThe C++ string object should be used for string manipulation (but must not be used across DLL boundaries).\n\n\nC++ 'cin', 'cout', 'cerr'\nThe C++ 'cin', 'cout', 'cerr' must not be used (except inside unit test code and command line tools).\n\n\nC 'stdin', 'stdout', 'stderr'\nThe C 'stdin', 'stdout', 'stderr' must not be used (except inside unit test code and command line tools).\n\n\nC headers/libraries\nC headers/libraries may be used.\n\n\nSystem specific headers/libraries\nSystem specific headers/libraries must not be used\n(except in system specific source code – in that case it should be clearly isolated and identified). The code should use as little as possible the windows SDKs (tradeoff between using existing code and code created from scratch).\n\n\nMultiple header include\nTo avoid multiple definitions, each header must have:\n#ifndef HEADERNAME_H\n#define HEADERNAME_H\n&lt;header&gt;\n#endif // HEADERNAME_H\n\n\nInclude inside header files\n#include should not be inside header files in order to avoid include files obfuscation, and to prevent some cases of bad build of a project which shares dependencies with a non-rebuilt project.\n\n\nFunction and variable declaration\nFunction and variable declarations must be done in header files (and not in other files).\n\n\n#pragma once\nUse of #pragma once is prohibited :\n- even if it is supported by a vast majority of c++ compilers, it is not a standard directive of the language\n- although it protects from header naming conflict, it doesn&#8217;t prevent from ncluding a header twice if it exists in more than one location in a project as files are excluded based on their filesystem-level identity.\n\n\n\n\n\nNaming conventions\n\n\n\n\n\n\n\nAbout names\n- Words must be in English.\n- Words inside the name must start with an uppercase letter. Other letters of the word must be lowercase letters (except for constants).\n- Names should not contain underscores '_' (except for constants and the prefixes as specified bellow).\n- Names should not contain abbreviations (except if the abbreviation is widely used in the particular field, such as ESDescriptor for “elementary stream descriptor”).\n- Names should be explicit according to what they will do, avoid generic names (like i, a, x…).\n\n\nC++ source files\nC++ source files must begin with the name of the class followed by ‘.cpp’.\n\n\nC source files\nC source files must begin with the name of the class followed by ‘.c’.\n\n\nC++/C headers files\nC++/C header files must begin with the name of the class followed by ‘.h’.\n\n\nC++ template headers files\nC++ template header files must begin with the name of the class followed by ‘.[inl|tpl]’.\n\n\nC++ template source files\nC++ template source files must begin with the name of the class followed by ‘.[ipp|tcc]’.\n\n\nClasses,\nstructures,\nglobal functions, structure tags, typedefs,\nenumerated values\nClass names, structure names, global functions, structure tags, typedefs, enumerated values must have their name beginning with an uppercase.\nExample MatrixBase\n\n\nMethods\nMethod names must begin with a lowercase letter (except for constructors and destructors).\nExample\treadAccessUnit()\n\n\nPrivate members\nPrivate member variable names must be prefixed with 'm_' and start with a lowercase letter.\nExample\tm_accessUnitList\n\n\nPrivate static members\nIf used, private static member variable names must be prefixed with 's_' and start with a lowercase.\nExample\ts_socketCounter\n\n\nLocal variables\nLocal variable names must start with a lowercase letter.\nExample\tdataLength\n\n\nConstants\nConstants must be all uppercase with each word separated by “_”.\nExample\tMAX_LENGTH\n\n\n\n\n\nDesign conventions\n\n\n\n\n\n\n\nMultiple inheritance\nPolymorphism\nMultiple inheritances should not be used, except if the additional classes are pure virtual (equivalent to Java interfaces).\n\n\nClasses with public virtual methods\nClasses with public virtual methods must have a virtual destructor (or else the destructor will not be called). When possible, use the appropriate compiler warning to be warn when destructor isn&#8217;t declared virtual while some public methods are.\n\n\nStatic member variables\nStatic member variables must not be used (these are basically “global variables”). (except for singleton design pattern)\n\n\nPublic member variables\nPublic member variables must not be used (except in pure “struct-like” classes). Instead, getter and/or setter methods should be provided to access member variables.\nExample\tint getMember()\n{\n&#8230;&#8203;.return m_member;\n}\nError setMember(int variable)\n{\n&#8230;&#8203;.if (variable&#8230;&#8203;)\n&#8230;&#8203;.{\n&#8230;&#8203;&#8230;&#8203;..m_variable = variable;\n&#8230;&#8203;&#8230;&#8203;..return NoErr;\n&#8230;&#8203;.}\n&#8230;&#8203;.return Error_NUMBER;\n}\n\n\nUnsigned/signed types\nSigned and unsigned computations should not be mixed. Signed and unsigned doesn’t work well together and are, in many cases, not comparable one another.\nSituations like “comparing unsigned values with potentially negative values” or “use signed computations to be casted into unsigned variables” makes the code vulnerable.\n\n\nSigned types\nUnsigned types  should be used.\nSigned types should only be used when the value for the variable or parameter in question could sensibly be negative.\n\n\n'enum' type\nFor variables or parameters that may take one of a set of values whose representation is arbitrary, the enum type should be used.\nExample\tenum CM_Colors { CM_RED, CM_GREEN, CM_BLUE };\n\n\nDynamic length structure\nIt is recommended to avoid structures with dynamic length. However, if they are used, the size should be bounded in size in order to avoid unlimited memory occupation.\n\n\nPreprocessor definitions\nThe definition and use of preprocessing flags (#ifdef/#ifndef) in the source code should be limited; in particular, there should not be any OS or compiler specific code.\nHowever, if specific code is present, it should be isolated and clearly identified.\nMost of the time, a different design approach allows to avoid inlined OS preprocessor definitions (namespace or inheritance usage for instance).\n\n\nCode under conditional compilation flags\nCode under #if, #ifdef, #ifndef should be limited. Theses sections, if not build with the rest of the code, can easily be broken without notice.\n\n\nInline\nInline may be used instead of macro for functions that are called often and when they are more than one line long.\n\n\nRange of variables\nConsider the range of each variable: each variable should remain local to a code block as much as possible.\nVariable like the for iterator can remain local to the loop. If the if condition statement block needs a local variable: declare it inside the statement block. This variable will not be visible outside the block, preventing misuse.\nNote for Intel compilers: before ICC11, declaring a variable into a for statement for (int myVariable;…) resulted in having the variable defined locally to the function containing the for. With ICC11, this variable exists only with the for statement code block.\nExample\tif (myCondFct())\n{\n&#8230;&#8203;.\tint myLocalVar = methodVar * m_aMember;\n&#8230;&#8203;.useMyLocalVar(myLocalVar);\n}\naMethodThatCanNotUseMyLocalVarHere();\nmyLocalVar is only used in the if statement block. If someone attempts to use it outside, the project will not build. This variable only serves that code block and it is not useful outside. The code is easier to read, no need to monitor myLocalVar, or wonder if it is used elsewhere…\n\n\nScope of variables\nAvoid using one variable for multiple purposes (the compiler handles this optimization process better than anybody).\n\n\nCode organization\nIt is recommended to differentiate:\n- Functions dedicated to computing.\n- Functions dedicated to schedule and control the computing functions.\n- Functions dedicated to data flow management.\nExample\tError computeFunc(UInt32* res, UInt32* sourceTable)\n{\n&#8230;&#8203;.// compute code\n&#8230;&#8203;.res = sourceTable[0] * sourceTable[1] + MY_CONST;\n&#8230;&#8203;.return NoErr;\n}\nError dataFlowFunc(MyStruct destStruct, MyStruct* sourceStruct)\n{\n&#8230;&#8203;.// copy struct\n&#8230;&#8203;.memcpy(destStruct, sourceStruct, sizeof(destStruct));\n&#8230;&#8203;.return NoErr;\n}\nError controlFunc(MyStruct* destStruct, MyStruct* sourceStruct, UInt32* sourceTable)\n{\n&#8230;&#8203;.Error err;\n&#8230;&#8203;.UInt32 res;\n&#8230;&#8203;.err = computeFunc(&amp;res, sourceTable);\n&#8230;&#8203;.if (err == NoErr)\n&#8230;&#8203;.{\n&#8230;&#8203;&#8230;&#8203;..err = dataFlowFunc(destStruct, sourceStruct);\n&#8230;&#8203;.}\n&#8230;&#8203;.return err;\n}\n\n\nThread concurrency\nUse threads with caution. It is recommended to ask architecture experts about the use of threads. Use C++11 threads' library.\n\n\nSingleton design pattern\nThis pattern should not be used unless absolutely needed. When used, special care should be taken to consider concurrent access issues; the unique instance should be automatically created in the first call of “getInstance”; and the constructor should be declared as private.\nSometimes, a statically created singleton is the prefered choice (more than the dynamically created one).\n\n\nCasts\nCasts should not be used unless absolutely needed. C-style casts must be prohibited and replaced with C++ casts.\nExample\tUInt16 var1;\nUInt32 var2;\nUInt64 myResult;\nmyResult = var1 * var2;\nmyResult = (UInt64) var1 * var2;\nmyResult = UInt64(var1) * UInt64(var2);\n\n\n\n\n\n\n\n\n\n\nDon&#8217;t put two methods calls on the same line. Don&#8217;t put break keyword in switch/case statement at the end of a processing line.\n\n\n\n\n\n\nLayout conventions\n\n\n\n\n\n\n\nTabs\nTabs must not be used. Spaces must be used for indentation. Editors should be set to fill with spaces, not tabs. Tab settings tend to be different for editors, printers and web pages.\nNote: This is obviously an arbitrary choice, but mixing tabs and spaces causes much difficulty in reviewing code…\n\n\nIndentation\nIndentation offset must be set to 4 spaces and is performed according to the following rules:\n- code surrounded by braces must be indented by one level.\n\n\nBlank lines\nA blank line should be used to separate logically distinct sections of code.\n\n\nCurly brackets\nCompound statements (if, else, else if, while, for, switch, do) must ALWAYS make use of curly brackets, even where the \"associated\" body only consists of a single line. Structures must use curly brackets around the clause.\n\n\nCurly brackets\nCurly brackets should appear at the beginning of the next line or at the end of the line.\nExample\tif (a == b) {\n&#8230;&#8203;.c = 0;\n}\nelse {\n}\nif (a == b)\n{\n&#8230;&#8203;.c = 0;\n}\nelse\n{\n}\n\n\nParentheses\nAlthough C++ has precedence rules that should ensure a given expression is evaluated in the same order regardless of the compiler, additional parentheses should be used where the order of evaluation is not obvious.\n\n\nMultiple parentheses\nParentheses on multiple lines must be aligned on the previous parentheses with the same level. Operators must be at the end of the lines.\nExample\tif (a == b) &amp;&amp; &#8230;&#8203;..(c == d ||\n&#8230;&#8203;.(e == f))\nif (a == b) &amp;&amp; (c == d ||\n&#8230;&#8203;.(e == f))\n\n\nFunctions\nEach function should perform a single well-defined operation.\nFunctions should not be too long. Up to 2 pages of printout or about 100 lines of source code is reasonable. These figures include comments and blank lines.\n\n\nSource files\nSources files must be small. 1000 lines of source code is reasonable (including comments and blank lines).\nThese files are easier to read and faster to compile (Intel compiler can compile several source files in parallel).\n\n\nHeader files\nHeader files must be small. 100 lines for headers are reasonable.\n\n\nSwitch\nCase/default from a switch statement are written on the same column as the switch keyword. break; and other lines are indented.\n The break keyword must ALWAYS be on its own line. Mixing the break keyword with processing code makes the code confused : it can be interpreted as \"fall-off\" code when break is at the end of long lines.\nExample\tswitch (getStyle(config))\n{\ncase STYLE_GOOD:\n&#8230;&#8203;.// Ah, it&#8217;s so good!\n&#8230;&#8203;.break;\ncase STYLE_BAD:\n&#8230;&#8203;.// Oh no, it&#8217;s bad!\n&#8230;&#8203;.break;\ndefault:\n&#8230;&#8203;.// Hmmm!\n&#8230;&#8203;.break;\n}\n\n\nInstructions\nPut one instruction per line.\n\n\nfor\nAlways put curly brackets in for clause. for instructions must be on their own lines (not on the for line)\n\n\n\n\n\nTracing and debugging\n\n\n\n\n\n\n\nBoost::log\nBoost::log is the recommended framework to log, as it provides great functionality out of the box without the need for extra/complex configuration\n\n\n\n\n\nError handling\n\n\n\n\n\n\n\nDefault error codes and types\nThe default error codes and error types should be declared in a common b&lt;&gt;com header file.\n\n\nType of value returned for error codes\nThe type of value returned for error codes should be Error.\nExample\tError parseString(char *str);\n\n\nMemory allocation\nA method that attempts to allocate memory must provide an allocation failure mechanism, typically by returning an error code. Note that other methods that call such a method must also provide a failure mechanism, and so on&#8230;&#8203; Memory allocation should not be performed in constructors as constructors don’t return error code.\n\n\nFile management\nThe success of a file opening must be checked and if not successful, the error must be handled appropriately.\nFiles must be closed when no longer used or when an error to exit occurs.\nWhen closing the file, the return value must be checked.\n\n\nFunction call\nThe success of a function call must be checked and if not successful, the error must be handled appropriately. The error codes returned by functions must be tested and treated.\n\n\nInit/deinit functions\nAfter calling constructors and before destructors, it is sometimes necessary to call init and deinit functions to permit error handling on structures that might fail (as these errors cannot be handled in constructors and destructors.\n\n\n\n\n\nMiscellaneous conventions\n\n\n\n\n\n\n\nCompiler warnings\nSource code must not have any warnings when compiled on any targeted platform with any targeted compiler (with a reasonably high warning level – at least level 3).\n\n\nC – C++ interfacing\nAll C public interfaces (*.h) which may be compiled with a C++ compiler must wrap the contents of the file with the pair of macros BEGIN_EXTERN_C and END_EXTERN_C.\nExample\tBEGIN_EXTERN_C\nEND_EXTERN_C\n\n\nC++ interfacing\nAll C class headers (*.h) which may be compiled with a C compiler must include a C API and ensure the non visibility of C code by putting it within an “#ifdef __cplusplus … #endif” statement.\n\n\nDynamic library export\nThe definition of each class or function that is exported in a dynamic library must be preceded by the XX_EXTERN keyword, XX being the prefix for the module to which the class belongs.\n\n\nPortability\nSee http://www.mozilla.org/hacking/portable-cpp.html for more miscellaneous recommendations on portability on various platforms. If a rule differs from b&lt;&gt;com coding rules, follow the b&lt;&gt;com coding rule.\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\nCopyright\nEach b&lt;&gt;com source and header file must use the template copyright header comment.\n(See Annex A1: .h)\nSource from other origins (Open Source for example) may have their own license. In this case, the license must be respected. The headers of third party files must be left intact (then it should not be replaced by b&lt;&gt;com copyright).\n\n\nPrimary documentation\nof a class\nThe primary documentation of a class must appear in the header file.\n\n\nClass description\nEach class must have a description before the class declaration.\n(see Annex A1: class description)\n\n\nMethods\nEach method (public, protected and private) must have a short description before the method declaration.\n(see Annex A1: setup method description)\nA method that is already sufficiently documented in the superclass may omit the description or have a single-line comment '// see superclass'\n(see Annex A1: clone method description)\n\n\nMember variable\nEach member variable must have a description either before the member variable declaration or on the same line.\n(see Annex A1: member description)\n\n\nComments\nComments are written in English. Do not use accented characters in source files.\nAll comments should be “Doxygen” compatible (see Tools). All tags must start with ‘@’ and not ‘\\’.\nEach block of code should be commented. Algorithms must be commented.\nBugs from Bugzilla must not be referenced in the code.\n\n\n/*…/\nThis type of comment block must be used for comments that apply either to a class, a function, a structure, an enum, a member… which is present below the comment block.\n(see Annex A1: class description)\n\n\n//\nThis comment line should be used inside the code to comment lines in C++ sources. They should be used even for block of comments.\n(see Annex A2)\n\n\n/…/\nThis comment block should not be used for C++ except for the template copyright block on top of the file and for method and variable documentation.\n\n\n\n\n\nC/C++ Performance rules\n\nBecause we need performance for all code types to achieve close-to-realtime target, these rules replace corresponding rules in previous chapters in order to ensure better software performance.\nFor C++, to improve performance, classes must act as evolved structures/handlers. They must point at a set of non reentrant methods (avoiding static code, allowing parallelism, allowing instantiation).\n\n\n\n\n\n\n\n\n'goto' keyword\nThe 'goto' keyword must not be used.\n\n\n‘continue’ and ‘break’ keywords\nThe ‘continue’ statement must not be used; the ‘break’ statement must not be used outside of switch statements.\n\n\nC++ &#8594; C convertibility\nC++ source must always be convertible into C code. If the rules associated with classes are followed, a class can be immediately converted into a structure and a bunch of methods with, as parameter, a handle on the structure that represents the former members.\n\n\nRecursive code\nRecursive code must not be used for performance reasons and lack of control over the code and because no parallelization and optimization are possible.\n\n\nNo class as class member\nA class must not contain another class as a member except through pointers.\n\n\nStructures must not contain arrays\nA structure (class or struct) must not contain arrays except through pointers.\nThe size of the structures must remain reasonable.\n\n\nDynamic memory allocation\nDynamic memory allocation must not be used.\nmalloc/free, new/delete must be used during initialization sequence (into the class creators for instance)\nDuring run time, explicit memory allocation must not be used to avoid memory fragmentation and leaks.\nIf an array is needed at some point during the execution of the program, this need must be planned and reserved at the initialization sequence.\nLocal arrays are tolerated if they are small in size (no more than 16-32 values).\n\n\nClass instantiations during run time\nAll the necessary classes, arrays, structures must be present, allocated and initialized before run time (during the initialization sequence).\n\n\nDynamic length structure\nDynamic length structures must not be used (in order to avoid unlimited memory occupation).\n\n\n\n\n\nTools\n\n\n\n\n\n\n\nUncrustify\nMost of the code formatting rules described in this document can be enforced using “uncrustify”.\n\n\nDoxygen\nDoxygen extract comments from the source code and generates documentation. It is recommended to check the comment structure with this tool.\nRefer to the online manual (http://www.stack.nl/~dimitri/doxygen/index.html) for a complete description of DOxygen rules.\n\n\n\n\n\n\n\nAppendix A.\tRules management\n\n\nI.\tResponsibility\n\nThe Development group manager is responsible of these rules.\n\n\n\nII.\tDeviation\n\nAny b&lt;&gt;com source code must follow these rules. Third party packages follow their own rules and should not be modified to follow these rules.\n\n\n\nIII.\tTraining\n\nAny C/C++ developers and integrators must be trained to these rules.\n\n\n\nIV.\tControl\n\nThe compliancy with these coding rules can be performed with Uncrustify tool with the appropriate config file (see tools).\n\n\n\n",
      "id": 5
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "ARCloud platform",
      "content": "ARCloud platform\n\nTable of Contents\n\nARCloud platform\nIntroduction\nARCloud: a generic cloud architecture for SolAR\nExternal tools used by the ARCloud platform\nRemote access to SolAR pipelines using XPCF remoting framework\n\nXPCF remoting framework global architecture\nProcess to generate a remote SolAR pipeline\nXPCF remoting Domain Specific Language\nRuntime behaviour\n\n\nContainerization of SolAR pipelines using Docker\n\nIntroduction to containers and Docker Engine\nUsing docker containers for SolAR pipelines\n\n\nManagement of ARCloud services using Kubernetes\n\nIntroduction to Kubernetes\nUsing Kubernetes to manage ARCloud services\nService chaining management with Istio\n\n\nARCloud deployment over SUPRA\n\nWhat is SUPRA?\nWireless Edge Factory\nFunctional architecture\nTechnical architecture\n\n\nA single ARCloud service sample: the mapping pipeline\n\nMapping pipeline description\nThe mapping pipeline in ARCloud architecture\n\n\nA full ARCloud service: the Map Update service\n\nThe Map Update service description\n\n\nConclusion\nGlossary\n\n\n\nARCloud platform\n\n\n\n\n\nIntroduction\n\n\nThe next step for the SolAR Framework is to distribute some spatial computing pipelines into the cloud. The idea is to offer an access to efficient and valuable pipelines to any AR application designer:\n\n\n\n\nwhatever his location\n\n\nwhatever his hardware configuration\n\n\nwhatever his development platform\n\n\nwhatever his target equipment\n\n\n\n\nAnd by continuing to benefit from the advantages of SolAR Framework (see document SolAR Architecture).\n\n\nMoreover, the \"cloud\" solution provided by SolAR Framework must take into account the following requirements:\n\n\n\n\nperformance: pipeline components often require specific CPU, GPU, memory, storage capacities that must be taken into account by physical machines hosting these components\n\n\nlatency: AR applications must be very responsive, so spatial computing pipelines must return their processed data as quickly as possible, especially when running in the cloud\n\n\nrobustness: pipelines deployed in the cloud must be able to support a large number of calls from different AR applications, sometimes with a significant volume of data (images, point clouds, etc.)\n\n\navailability: pipelines in the cloud must always be available for AR applications, regardless of the location of the end-user device, and at any time\n\n\nscalability: pipelines in the cloud should be easy to update, and new pipelines should be easy to deploy, with minimal impact on existing AR applications\n\n\nsecurity: integrity of spatial computing pipelines deployed in the cloud must be guarenteed\n\n\n\n\nWith this objective, the SolAR Framework must evolve to offer new features to its users:\n\n\n\n\ncommunication layer: to handle calls to pipelines in the cloud, synchronous or asynchronous, including complex data sending or receiving to and from these pipelines\n&#8658; based on gRPC framework and Boost library for serialization (by default)\n\n\nautomatic generation of deployable pipeline packages: to help developers to prepare their pipelines to be deployed in the cloud, benefiting from the environment of the SolAR Framework\n&#8658; based on Docker technology\n\n\ncloud deployment orchestration: to make pipelines deployment in the cloud as easy as possible, and to offer tools to manage the lifecycle of these pipelines\n&#8658; based on Kubernetes (K8s) platform\n\n\n\n\nThis evolution of the SolAR Framework is called the ARCloud platform.\n\n\n\n\n\n\n\n\nIn the remainder of this document, the term \"service\" will be used to designate a SolAR pipeline deployed on the ARCloud platform in a docker container, including the remote communication layer.\n\n\n\n\n\n\n\nThe purpose of this documentation is to describe the ARCloud platform, which offers a complete solution for the deployment, maintenance and upgrade of SolAR services in the Cloud.\nIn addition, this document presents, with concrete examples, how to use the ARCloud platform\nto design AR client applications.\n\n\n\n\n\n\nARCloud: a generic cloud architecture for SolAR\n\n\nThe premise of deploying SolAR pipelines in the cloud is based on the need to be able to offer any AR application the ability to rely on powerful spatial computing services not locally, but somewhere in the cloud, using an internet connection.\n\n\nThe next diagram shows a global and high level vision of connections between AR devices and spatial computing services hosted on cloud servers:\n\n\n\n\n\nFigure 1. SolAR Framework global cloud vision\n\n\nEach service has its own characteristics:\n\n\n\n\nit can be called synchronously or asynchronously, or in streaming mode\n\n\nits processing may require specific technical criteria applied to the CPU, GPU, memory&#8230;&#8203;\n\n\nit may need to store a local context linked to a specific device\n\n\nit may also need to store permanent local data\n\n\nit can be chained to another service (i.e. service chaining must be managed by the platform)\n\n\netc.\n\n\n\n\nAll these aspects will be presented later in this document.\n\n\n\n\nExternal tools used by the ARCloud platform\n\n\nAs presented previously, some open source tools are used to manage the new features of SolAR Framework, necessary for the implementation of this cloud architecture:\n\n\n\n\nBoost: As described in the SolAR_Framework_architecture\">SolAR Architecture document, this C++ library provides some functionalities to deconstruct a set of data structures to a sequence of bytes (serialization), and to reconstitute an equivalent structure in another program context (deserialization).\nMore information on: https://www.boost.org/doc/libs/1_74_0/libs/serialization/doc/index.html\n[underline]#Note: Boost serialization is the default solution provided by the XPCF remoting framework to ensure serialization and deserialization of data structures, but the framework has been designed so that any other solution can be used for this feature if needed.\n\n\ngRPC: This open source remote procedure call (RPC) system uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication, bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts. It generates cross-platform client and server bindings for many languages.\nMore information on: https://www.grpc.io/\nNote: This tool is embedded in the XPCF remoting framework.\n\n\nDocker: This set of Platform as a Service (PaaS) products uses OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than virtual machines.\nMore information on: https://www.docker.com/\nNote: The container generation is managed by the XPCF remoting framework.\n\n\nKubernetes: K8s is an open-source container-orchestration system. It defines a set of building blocks, which collectively provide mechanisms that deploy, maintain, and scale applications based on CPU, GPU, memory or custom metrics. K8s is loosely coupled and extensible to meet different workloads.\nMore information on: https://kubernetes.io/\n\n\n\n\nAll of these tools will work together to constitute the ARCloud platform based on the SolAR Framework, as shown on this diagram:\n\n\n\n\n\nFigure 2. Open source tools used by the ARCloud platform based on the SolAR Framework\n\n\n\n\nRemote access to SolAR pipelines using XPCF remoting framework\n\n\nThe SolAR Framework relies heavily on XPCF framework features (component configuration and injection, module loading at runtime, interface introspection&#8230;&#8203;), as described in the SolAR Architecture document.\nAs every SolAR component is based on XPCF interface and component design, it permits to consider a common approach to automate the distribution of any XPCF component across the network (including SolAR components), through its interfaces.\nThe goal is to provide the ability to distribute the SolAR Framework (either a single component, or meta-components such as pipelines) across the network with each part becoming a service, such as depicted on this schema.\n\n\nThis approach has led to improve the XPCF framework to provide remote support from any interface declaration, through a new version: XPCF remoting framework.\n\n\nXPCF remoting framework (version 2.5.x) detailed presentation and code are available on this Git Hub repository: https://github.com/b-com-software-basis/xpcf/tree/develop#xpcf-remoting-architecture\n\n\nXPCF remoting framework global architecture\n\nThis diagram shows the generic software atchitecture of XPCF remoting framework used to enable remote access to any SolAR spatial computing pipeline (based on XPCF components) deployed in the cloud:\n\n\n\n\n\nFigure 3. XPCF remoting architecture overview\n\n\nThe client application can be any AR application running on a device (computer, mobile phone, tablet, AR smartglasses&#8230;&#8203;) and relying on certain SolAR spatial computing pipelines available in the cloud.\n\n\nThe server application is a XPCF application, running a gRPC server on a provided url.\n\n\nSolAR Framework elements\n\n\nSolAR component interfaces correspond to the interfaces proposed by the SolAR pipelines to any client application (i.e. entry points to remote ARCloud services). They are based on SolAR Framework pipeline interfaces (see API reference: https://solarframework.github.io/create/api/).\n\n\nSolAR component implementations correspond to components that run the algorithms of the spatial computing pipelines. They are not visible to client applications.\n\n\nImportant:\n\n\nThe remoting functionality works for any interface that:\n\n\n\n\nuses data structures (see document SolAR_Framework_architecture\">SolAR Architecture) that declare one of the serialization methods exposed through XPCF specifications (one of the methods provided in xpcf/remoting/ISerializable.h)\n[underline]#Note: in version 2.5.0 only Boost serialization is handled, upcoming minor releases of XPCF will provide support for the other methods\n\n\nuses basic C++ types only\n\n\n\n\nPointers are not supported as they can handle arrays &#8230;&#8203; so a pointer will need a size, but no presumption to detect such cases can be made. Anyway, interfaces represent contracts between a user and a service, so interfaces should be self-explanatory. In this context, relying on pointers does not explicit the expected behavior, and interfaces should therefore rely on structured types, standard types (string, vectors&#8230;&#8203;) or user-defined types (structures, classes).\n\n\nXPCF Framework elements\n\n\nThe GrpcServerManager uses XPCF to find every component implementing the IGrpcService interface and registers them. It is used to start the server application.\n\n\nThe IGrpcService interface defines the methods needed to manage any XPCF server component, and is used to register these components at run time (i.e. each XPCF server component implements this interface).\n\n\nElements generated by the XPCF remoting framework\n\n\nThe client configuration file contains information needed to manage gRPC communications on client side, such as channel urls (IP address and port of the server hosting the concrete code, for each interface).\n\n\nA XPCF proxy component is created for each SolAR interface and implements this interface.\nThis component transforms the interface input/output methods to grpc requests/responses, managing the serialization and deserialization of the data. It calls the grpc service and rpc methods corresponding to the methods of the interface. This component is configured with a channel url (given by the configuration file) and a grpc credential.\n\n\nThe server configuration file contains the server channel url for each interface. It also contains the parameters of each component used on the server side.\n\n\nA XPCF server component  is created for each SolAR interface, and implements this interface. It inherits from xpcf::IGrpcService, which allows the GrpcServerManager to register it. Moreover, each rpc method of the xpcf::IGrpcService class is implemented so that grpc requests/responses are translated to the SolAR component input/output methods with the correct data types (serialization/deserialization). The concrete SolAR component is injected to this component.\n\n\ngRPC proto files (text files with .proto extension) are generated by the XPCF remoting framework for each SolAR interface. These files define the structure of the data to serialize. They will be used by the protocol buffer compiler protoc to generate gRPC Stubs and gRPC Services.\n\n\n\nProcess to generate a remote SolAR pipeline\n\n\n\nGenerate the compilation database from the SolAR project (for example, using Qt Creator: Build&#8594;Generate Compilation Database).\n\n\nFrom this compilation database, generate the remoting code for the SolAR Framework interfaces, using the xpcf_grpc_gen tool included in the XPCF remoting framework. \nThis tool parses all the SolAR interfaces and creates a Qt Creator development project to compile all the generated code in a XPCF module. It also creates a client and a server XPCF configuration file (see next diagram).\nThe xpcf_grpc_gen tool uses a forked version of cppast (with customization around templates parsing) and libclang.\n\n\nBuild the generated XPCF remoting modules, using your IDE.\n\n\nAdapt client and server configuration files with needed services.\n\n\nLaunch the xpcf_grpc_server application with the server configuration file and the modules.\n\n\n\n\n\n\n\nFigure 4. XPCF remoting code generation\n\n\nxpcf_grpc_gen syntax:\n\n\n\nxpcf_grpc_gen -n &lt;project name&gt; -v &lt;project version&gt; -r &lt;repository type&gt; -u &lt;host repository url&gt; --std &lt;C++ standard&gt; --database_dir &lt;compilation database path&gt; --remove_comments_in_macro -o &lt;destination folder&gt; -g &lt;message format&gt;\n\n\n\nExample:\n\n\n\nxpcf_grpc_gen -n SolARFramework -v 0.11.0 -r build@github -u https://github.com/SolarFramework/SolARFramework --database_dir /home/user/Dev/SolAR/core/build-SolARFramework-Desktop_Qt_5_15_2_GCC_64bit-Release/ --std c++1z --remove_comments_in_macro -g protobuf -o ~/grpcGenSolar\n\n\n\nThe project generated by xpcf_grpc_gen for the entire SolAR Framework is available on GitHub: https://github.com/SolarFramework/SolARFrameworkGRPCRemote/tree/0.11.0\n\n\n\nXPCF remoting Domain Specific Language\n\nThe XPCF remoting framework provides a Domain Specific Language (DSL) to ease the code generation.\n\n\nThis DSL is:\n\n\n\n\nbased on C++ user defined attributes (those attributes can be used on interfaces or on their methods)\n\n\nmade to assist/extend the code generation\n\n\nusable in SolAR interface declarations\n\n\n\n\nCurrently, the DSL helps to:\n\n\n\n\nremove ambiguities\n\n\nmaintain proxy and server components UUID stability\n\n\nspecify interfaces to ignore\n\n\n\n\nIn future releases, the DSL will allow to specify different remoting modes for methods (asynchronous, streaming…).\n\n\nTable 1. XPCF remoting DSL description\n\n\n\n\n\n\n\n\nAttribute\nArgument type\nApply to\nSemantic\n\n\n\n\n[[xpcf::ignore]]\nnone\nclass or method level\nspecify that the corresponding class/method must be ignored while generating remoting code\n\n\n[[xpcf::clientUUID(\"uuid_string\")]]\nstring\nclass level\nspecify the XPCF remoting client UUID\n\n\n[[xpcf::serverUUID(\"uuid_string\")]]\nstring\nclass level\nspecify the XPCF remoting server UUID\n\n\n[[grpc::server_streaming]], [[grpc::client_streaming]] or [[grpc::streaming]]\nnone\nmethod level\n\n\n\n[[grpc::request(\"requestMessageName\")]]\nstring\nmethod level\noptionally set gRPC request message name\n\n\n[[grpc::response(\"responseMessageName\")]]\nstring\nmethod level\noptionally set gRPC response message name\n\n\n[[grpc::rpcName(\"rpcname\")]]\nstring\nmethod level\noptionally set gRPC method RPC name\n\n\n\n\nFor example, the following code shows how to set the client and server UUID for a XPCF remote interface:\n\n\n\nclass [[xpcf::clientUUID(&quot;fe931815-479c-4659-96c3-09074747796d&quot;)]]\n      [[xpcf::serverUUID(&quot;2ee889a4-bb14-46a0-b6e9-a391f61e4ad0&quot;)]]\n    IMessage: virtual public org::bcom::xpcf::IComponentIntrospect {\n      ...\n};\n\n\n\n\nRuntime behaviour\n\nThis diagram shows the tasks performed when starting the XPCF client and server applications:\n\n\n\n\n\nFigure 5. XPCF remoting behaviour: initialization\n\n\nOn client side:\n\n\n\n\nthe client configuration file is used to configure the channel url of each XPCF proxy component\n\n\nXPCF proxy components are injected into corresponding SolAR component interfaces\n\n\n\n\nOn server side:\n\n\n\n\nthe server configuration file is used to:\n\n\n\nconfigure each SolAR component with its dedicated parameters\n\n\nconfigure the channel url of each XPCF server component\n\n\n\n\n\nSolAR components are injected into corresponding XPCF server components interfaces\n\n\nserver components are injected as IGrpcService services\n\n\nthe GrpcServerManager registers every IGrpcService to gRPC\n\n\nthe GrpcServerManager runs the gRPC server on given address:port (either with a configuration parameter or through the environment variable XPCF_GRPC_SERVER_URL)\n&#8658; without configuration, the server listens to \"0.0.0.0:50051\"\n\n\n\n\nThen, the next diagram shows the tasks performed by the XPCF client and server applications at runtime:\n\n\n\n\n\nFigure 6. XPCF remoting behaviour: runtime\n\n\n\n\nthe client application calls a service through its SolAR component interface\n\n\nthe interface is bound to the XPCF proxy component: the service is called on the proxy and forwarded to gRPC\n\n\nthe gRPC request is caught by the corresponding XPCF server component\n\n\nthe call is forwarded from the XPCF server component to the concrete SolAR component implementation (through its interface)\n\n\n\n\nIf the function called from the SolAR component interface returns a value, this value will then be sent back to the client, following the reverse path.\nAdditionally, if some parameters of this function are output parameters, their new values will also be sent back to the client.\nThe call to such a service is synchronous, which means that the client will be blocked until it retrieves the response.\n\n\n\n\n\nContainerization of SolAR pipelines using Docker\n\n\nIntroduction to containers and Docker Engine\n\nBefore we can deploy a SolAR spatial computing pipeline in the cloud, we need to make that pipeline run in an independent software unit that contains both its code and all of its dependencies, and also offers a way to communicate with it. This will be possible using a docker container.\n\n\nDocker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Available for both Linux and Windows-based applications, containerized software will always run the same, regardless of the infrastructure. Container images become containers at runtime, when they run on Docker Engine.\n\n\n\n\n\nFigure 7. Applications running in docker containers\n\n\nDocker Engine acts as a client-server application that hosts images, containers, networks and storage volumes. The engine also provides a client-side Command-Line Interface (CLI) that enables users to interact with the daemon through the Docker Engine API.\n\n\n\n\n\nFigure 8. Docker Engine overview\n\n\nTo allow external access to a docker container (i.e. to a spatial computing service interface), it is possible to expose its port by mapping it to an external port in the host. Docker containers have an internal network and each container is associated with an IP address that can be accessed from the Docker host machine. Being internal IP, this IP cannot be used to access the containers from external network. But the Docker host machine’s main IP is accessible from outside. So, the solution is to bind the internal port of the Docker container to a port on the host machine.\n\n\n\n\n\nFigure 9. Docker port mapping\n\n\nDocker is a free software, licensed under the Apache v2 licence, and distributed as an open source project since 2013.\n\n\nMore information on: https://www.docker.com/\n\n\n\nUsing docker containers for SolAR pipelines\n\nAs explained in paragraph XPCF remoting framework global architecture, the XPCF framework offers a fully fonctional software architecture to manage SolAR pipelines as server applications:\n\n\n\n\nXPCF provides a server application named \"xpcf_grpc_server\" to host server side components (the spatial computing pipeline)\n\n\nthe xpcf_grpc_server application automatically registers the pipeline interface to gRPC, to allow remote calls from clients\n\n\nthe xpcf_grpc_server application automatically runs a gRPC server using predefined url and credentials (\"0.0.0.0:50051\" with “insecure” channel credentials by default)\n\n\n\n\nSo, this software bundle can then be easily deployed in a docker container, as shown in the following diagram:\n\n\n\n\n\nFigure 10. SolAR spatial computing pipeline running in a docker using XPCF\n\n\nIn order to deploy any remote pipeline, three elements must be provided to the xpcf_grpc_server application:\n\n\n\n\nthe XPCF server configuration file defining:\n\n\n\nthe list of XPCF remoting server components (implementing IGrpcService interface)\n\n\nthe declarations and configurations of concrete modules and components (SolAR spatial computing pipeline)\n\n\n\n\n\nthe XPCF remoting server module, which provides the server side stubs, implements the gRPC interfaces and will receive, through injection, the concrete implementation of SolAR components\n\n\nthe concrete modules and components of the SolAR pipeline\n\n\n\n\nThe generation of the application is managed by the automated bundling of modules offered by XPCF, based on remaken mechanisms (see SolAR Architecture document). The resulting bundle can then be easily deployed to a docker container.\n\n\nremaken syntax for XPCF bundle generation:\n\n\n\nremaken bundleXpcf -d &lt;destination directory&gt; &lt;XPCF xml declaration file&gt;\n\n\n\nThen, the application build directory must be copied to the new bundle directory, and the app is ready to run.\n\n\n\n\n\nManagement of ARCloud services using Kubernetes\n\n\nIntroduction to Kubernetes\n\nKubernetes (commonly called K8S) is an open source orchestrator for deploying containerized applications. This platform provides the software necessary to successfully build and deploy reliable, scalable distributed systems. It works with a variety of containerization tools, and is often used with Docker (see Introduction to containers and Docker Engine).\n\n\nClusters\n\n\nThe main abstraction on which Kubernetes is based is the cluster, which is the group of machines running Kubernetes and the containers it manages.\n\n\nNodes\n\n\nEach cluster contains Kubernetes nodes. They can be physical or virtual machines, and considered as \"worker machines\". Nodes, on the other hand, run pods: the most basic Kubernetes objects that can be created or managed.\n\n\nPods\n\n\nThe basic unit of scheduling in K8s is called a pod, which is an abstract view of containerized components. A pod consists of one or more containers that are guaranteed to be co-located on the same host machine and can share its resources.\nEach pod in K8S has a unique IP address (inside the cluster), which allows applications to use machine ports without risk of conflict. A pod can define a volume, such as a directory on a local drive or on the network, and expose it to containers in that pod. Pods can be defined through the Kubernetes API. Their management can also be delegated to a controller.\nPods are attached to the node that deploys them until they expire or are deleted. If the pod is faulty, a new pod having the same properties will be broadcast to an other available node.\n\n\nThe following figure shows an overview of a cluster:\n\n\n\n\n\nFigure 11. Kubernetes: overview of a cluster\n\n\nThe next figure zooms inside a pod to show how containers are managed, and how they can access local volumes and communicate with external applications using port exposition:\n\n\n\n\n\nFigure 12. Kubernetes: containers inside pods\n\n\nLabels and selectors\n\n\nKubernetes allows customers (users and internal components) to attach key-value pairs called labels to any API object in the system, such as pods and nodes. By correspondence, label selectors are queries made on labels linked to objects.\nLabels and selectors are the first grouping mechanism in K8s, and are used to determine which components to perform an operation on.\n\n\nControllers\n\n\nA controller is an arbitration loop that drives the current state of a cluster to its desired state. It does this by managing a set of pods. One of the types of controller is called a \"replication controller\", it handles replication and scaling by launching a specific number of copies of a pod on a cluster. It also handles the creation of replacement pods if the underlying node is faulty. Two of the controllers that are part of the Kubernetes core system are: the DaemonSet Controller to launch a single pod on each machine (or a subset of machines), as well as the Job Controller to launch pods that have a specific purpose (e.g. scripts). The set of pods that a controller manages is determined by selectors labels that are part of the controller definition.\n\n\nServices\n\n\nA Kubernetes service is a group of pods working together, for example one layer in a multi-layered application. All the pods that make up a service are defined by a label selector. Kubernetes provides a discovery and routing service by assigning an IP address and a domain name to a service, and balances the traffic load from that address on all pods corresponding to the selector. By default, a service is exposed inside a cluster, but a service can also be exposed outside of a cluster.\n\n\nIngress\n\n\nAn Ingress is an API object that manages external access to the services in a cluster. In that way, Ingress exposes HTTP or HTTPS routes from outside the cluster to services within the cluster. Moreover, Ingress may provide load balancing and manage traffic routing based on rules defined on its resource.\n\n\nThe following figure shows how Ingress routes incoming requests to services, and therefore to pods:\n\n\n\n\n\nFigure 13. Kubernetes: Ingress as a router to pods\n\n\nMore information on Kubernetes: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\n\n\n\nUsing Kubernetes to manage ARCloud services\n\nOnce SolAR pipelines have been generated as \"remote\" bundles, integrating gRPC remote access management and data serialization (see Remote access to SolAR pipelines using XPCF remoting framework), and encapsulated in Docker containers (see Containerization of SolAR pipelines using Docker), these pipelines are ready to be deployed as ARCloud services using the Kubernetes platform.\n\n\nTo do that, each SolAR pipeline bundle will be deployed on a dedicated pod, and its interface will then be exposed to other services or client applications through a Kubernetes service (a service can bundle several SolAR pipelines to provide more powerful spatial computing services to AR client applications). External requests to ARCloud services will be managed by Ingress objects.\n\n\nThis implementation of Kubernetes for ARCloud can be schematized as follows:\n\n\n\n\n\nFigure 14. Kubernetes: ARCloud implementation\n\n\n\nService chaining management with Istio\n\nA particular need for ARCloud is to be able to chain together multiple ARCloud services to provide more powerful spatial computing processing to AR client applications, as shown in the following figure:\n\n\n\n\n\nFigure 15. ARCloud service chaining\n\n\nAt this time, to deploy ARCloud services at the Kubernetes level, these rules have been defined:\n\n\n\n\none Pod runs one service (i.e. one container): many similar pods are possible (replica) and placement of services on computing machines is based on declarative conditions\n\n\na Service exposes the gRPC endpoint of the first component of the service (interface of the SolAR pipeline)\n\n\nan Ingress exposes the gRPC endpoint of the first service of the chaining (external interface of the multi pipelines spatial computing processing)\n\n\n\n\nSo, respecting these rules, the expected behavior for chained services can be represented like this:\n\n\n\n\n\nFigure 16. ARCloud Kubernetes chaining\n\n\nBut this solution is not sufficient to fully meet the need to chain services in the cloud. Indeed, several AR devices can simultaneously use the same service, so each chained service must be able to keep the context linked to the calling AR device, in order to guarantee the consistency of the data processed throughout the chaining.\n\n\nAs service instantiations can handle a specific context (related to a given AR device), chaining must be defined at the service instantiation level (pods) and not only at the service level. For this purpose, a Unique Identifier (UID) will be defined for each AR device, and used to chain service instances together, keeping the context of the device. Note that all these services are asynchronous.\n\n\nTo implement this solution, the ARCloud platform uses Istio, an open source service mesh offering tools to connect, secure, control and observe services. Istio is able to manage microservices as a service mesh, defining links between these services and managing interactions between them. Istio is also able to handle service chaining at pod level to ensure that service instantiations dedicated to a given AR device can be chained together and that the instantiation of services can run in an asynchronous way.\n\n\nMore information about Istio on the official web site: https://istio.io/latest/docs/concepts/what-is-istio/\n\n\nUsing Istio to handle service chaining while keeping the context of the calling AR device, the ARCloud architecture can be represented like this:\n\n\n\n\n\nFigure 17. Service chaining based on Istio\n\n\nThe first resource deployed is an Istio gateway that describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections.\n\n\nAn Istio virtual service can then be bound to the gateway to control the forwarding of traffic arriving at a particular host/service or gateway port. Each virtual service resource will forward incoming HTTP traffic, i.e. gRPC requests, to a destination service defined by the corresponding Istio destination rule resource.\n\n\nThe other Istio virtual services define the chaining of the Kubernetes services inside the cluster.\n\n\nIstio destination rule resources define policies that apply to traffic intended for Kubernetes services after routing has occurred. A destination rule is defined for each service, to load balance incoming gRPC requests to attached pods. A Consistent Hash-based, declared in the load balancing rule, is used to provide a soft session affinity based on the AR device UID (sent in the HTTP header). All gRPC requests sent by a particular device will be handled by the same pod at AR service level.\n\n\n\n\n\nARCloud deployment over SUPRA\n\n\nWhat is SUPRA?\n\nFor a cloud platform like ARCloud, taking into account current trends for this type of platform, some requirements are particularly relevant:\n\n\n\n\noptimized edge computing infrastructure to minimize bandwidth between AR devices and ARCloud services, and reduce latency\n\n\nKubernetes to manage container orchestration\n\n\nbare metal servers to optimize performance and reduce costs\n\n\nGPU in addition to CPU to increase performance\n\n\n\n\nTo meet these requirements, SUPRA is a complete cloud stack architecture defined by b&lt;&gt;com for Telco, AI, AR, VR and e-Health applications on bare metal infrastructure. In particular, SUPRA incorporates the following concepts, essential for ARCloud:\n\n\n\n\nContainer as a Service (CaaS) for cloud native applications\n\n\nMonitoring as a Service (MaaS)\n\n\nGPU as a Service to access to GPU resources to accelerate specific computations\n\n\n\n\nbut also:\n\n\n\n\nField Programmable Gate Arrays (FPGA) as a Service\n\n\nAI as a Service\n\n\n\n\n\n\n\nFigure 18. ARCloud platform over SUPRA infrastructure\n\n\n\nWireless Edge Factory\n\nWireless Edge Factory (WEF) is a cloud native wireless private network solution offering end-to-end tailored communication services that are deployed and run in a fully secure manner. The solution allows to build optimized, cooperative and coordinated networks and Edge Clouds for verticals.\n\n\nDesigned by b&lt;&gt;com, WEF provides:\n\n\n\n\na performant Cloud Native 5G end to end network solution, optimized to leverage on Edge infrastructure\n\n\norchestration of business services (Networking + Bus Apps)\n\n\nadaptive user plane:\n\n\n\nhardware/software depending on context (traffic, use cases, …)\n\n\ncloud native: localization, dynamic placement, RAN aggregation\n\n\n\n\n\nmethods and algorithms for dynamic and adaptive placement of Cloud Native Network Functions in multiple Edge Cloud\n\n\n\n\nThe WEF platform offers a set of Cloud Network Functions (CNF) deployed on Kubernetes Virtual Infrastructure Manager (VIM).\n\n\nThe ultimate goal of this solution is to propose a full 5G SA connectivity.\n\n\nWEF is used in ARCLoud context to manage connections between 5G compatible AR devices and cloud computers that host SolAR spatial computing services.\n\n\n\n\n\nFigure 19. WEF global architecture\n\n\nMore information on: https://b-com.com/en/bcom-wireless-edge-factory\n\n\n\nFunctional architecture\n\nThe ARCloud services must be deployed on several machines, according to the technical criteria necessary for their optimum processing (CPU, GPU, memory&#8230;&#8203;). To easily operate these deployments, SUPRA provides common functions, such as:\n\n\n\n\nContainer orchestration across multiple hosts\n\n\nService discovery and load balancing\n\n\nSecret and configuration management\n\n\nStorage orchestration\n\n\nControl of external access to services\n\n\nObservability\n\n\n\n\nMoreover, AR devices should be able to access ARCloud services over a 5G network, to benefit from a fast and reliable connection to the cloud. That is the role of the Wireless Edge Factory (WEF) solution.\n\n\nThus, by applying the functional architecture principles recommended by SUPRA, integrating 5G access with WEF, the deployment of ARCloud services can be represented as follows:\n\n\n\n\n\nFigure 20. ARCloud over SUPRA: functional architecture\n\n\n\nTechnical architecture\n\nThe ARCloud platform consists of several services interconnected with each other, providing high-level and powerful spatial computing processing, running on the cloud platform. This aims to speed up the computation and to not depend on the resources of AR devices. Instead of computing complicated calculations on the device, the AR application simply transfers data (such as a video stream or a set of images) to the ARCloud platform, which then sends back the resulting data (for example, a render video stream to display to the end-user).\n\n\nTechnically, to meet the requirements defined for the ARCloud platform, the whole solution must provide tools to:\n\n\n\n\nchain AR services (pods) at network level, without impacting pod configuration\n\n\nmanage session affinity (to keep an AR device context when requesting a spatial computing service)\n\n\n\n\nThe following figure shows the technical elements required for the operation of the ARCloud platform:\n\n\n\n\n\nFigure 21. ARCloud over SUPRA: technical architecture\n\n\n\n\n\nA single ARCloud service sample: the mapping pipeline\n\n\nMapping pipeline description\n\nThis sample is based on a spatial computing pipeline previously developed using the SolAR Framework (see document SolAR Architecture).\n\n\nThe objective of this \"mapping\" pipeline is to build a 3D sparse map surrounding the AR device for visualization and relocalization, based on images and their poses provided by the tracking system of the AR device.\n\n\nA sparse map component in SolAR Framework consists of the following storage components:\n\n\n\n\nIdentification\n\n\nCoordinate system\n\n\nPoint cloud manager\n\n\nKeyframe manager\n\n\nCovisibility graph\n\n\nKeyframe retriever\n\n\n\n\nThe figure below shows the class diagram of the sparse map, including component interfaces and implementations. Boost library serialization is used to save and load map data to external storage files. Moreover, this serialization allows to easily exchange map data between services.\n\n\n\n\n\nFigure 22. Class diagram of the sparse map developed using SolAR Framework\n\n\nThe processing of this pipeline is divided into three main steps:\n\n\n\n\nThe bootstrap: This step aims to define the first two keyframes and to triangulate the initial point cloud from the matching keypoints between these keyframes. Moreover, before that, this step computes the 3D transformation between the fiducial marker and the AR device. Then, this transformation is applied to correct the poses given by this device into the reference coordinate system of the fiducial marker.\n\n\nThe mapping: Firstly, this step updates the visibilities of the features in each input frame with the 3D point cloud. Then, it tries to detect new keyframes based on these visibilities. Once a new keyframe is found, it is triangulated with its neighboring keyframes to create new 3D map points. Finally, a local bundle adjustment is performed to optimize the local map points and the poses of keyframes.\n\n\nThe loop closure: Although the use of the tracking system built into the AR device provides high accuracy of the poses, the pose errors still accumulate over time. Therefore, the loop closure process allows to optimize the map and the keyframe poses, as well as to avoid creating a redundant map when AR devices return to the pre-built area. The loop closure process includes three components: loop detection, loop correction and loop optimization.\n\n\n\n\nSo, to initialize the mapping pipeline processing, a device must first give two data:\n\n\n\n\nthe caracteristics of the camera it uses (resolution, focal)\n\n\nthe description of the fiducial marker to track (matrix and dimensions)\n\n\n\n\nThen, the pipeline is able to process images and poses. To do this, some input data are needed:\n\n\n\n\nthe images captured by the device (images are sent to the pipeline one by one)\n\n\nthe relative position and orientation of the capture device, for each image\n\n\n\n\nAnd finally, after the pipeline processing, the output data are:\n\n\n\n\nthe current map of the place calculated by the pipeline from the first image to the current one (in fact, a point cloud)\n\n\nthe recalculated positions and orientations of the capture device, inside this point cloud, from the first image to the current one (in fact, only for some keyframes determined by the pipeline).\n\n\n\n\nTo facilitate the use of this pipeline by any client application embedded in a device, it offers a simple interface based on the SolAR::api::pipeline::IMappingPipeline class (see https://solarframework.github.io/create/api/ for interface definition and data structures).\nThis interface is defined as follows:\n\n\n\n/// @brief Set the camera parameters\n/// @param[in] cameraParams: the camera parameters (its resolution and its focal)\n/// @return FrameworkReturnCode::_SUCCESS if the camera parameters are correctly set, else FrameworkReturnCode::_ERROR_\nvirtual FrameworkReturnCode setCameraParameters(const datastructure::CameraParameters &amp; cameraParams) = 0;\n\n\n\n\n/// @brief Request to the mapping pipeline to process a new image/pose\n/// Retrieve the new image (and pose) to process, in the current pipeline context\n/// (camera configuration, fiducial marker, point cloud, key frames, key points)\n/// @param[in] image: the input image to process\n/// @param[in] pose: the input pose to process\n/// @return FrameworkReturnCode::_SUCCESS if the data are ready to be processed, else FrameworkReturnCode::_ERROR_\nvirtual FrameworkReturnCode mappingProcessRequest(const SRef&lt;datastructure::Image&gt; image,\n                                                  const datastructure::Transform3Df &amp; pose) = 0;\n\n\n\n\n/// @brief Provide the current data from the mapping pipeline context for visualization\n/// (resulting from all mapping processing since the start of the pipeline)\n/// @param[out] outputPointClouds: pipeline current point clouds\n/// @param[out] keyframePoses: pipeline current keyframe poses\n/// @return FrameworkReturnCode::_SUCCESS if data are available, else FrameworkReturnCode::_ERROR_\nvirtual FrameworkReturnCode getDataForVisualization(std::vector&lt;SRef&lt;datastructure::CloudPoint&gt;&gt; &amp; outputPointClouds,\n                                                    std::vector&lt;datastructure::Transform3Df&gt; &amp; keyframePoses) const = 0;\n\n\n\nTo complete the description of the mapping pipeline, the following diagram shows the different steps it implements, from initialization to the constitution of the point cloud:\n\n\n\n\n\nFigure 23. Mapping pipeline description\n\n\nThe mapping pipeline sample is available on GitHub (multithreading version): https://github.com/SolarFramework/Sample-Mapping/tree/0.11.0/Mapping/SolARPipeline_Mapping_Multi\n\n\n\nThe mapping pipeline in ARCloud architecture\n\nThe benefit of deploying the mapping pipeline in the cloud, based on the ARCloud architecture, is to provide AR device applications with an easy way to benefit from this powerful SolAR Framework spatial computing algorithm, regardless of the type, the location, the hardware / software configuration of the device, while benefiting from the computing capacities of remote computers dedicated to these particular resource-intensive processing, with latency times allowing almost real-time user feedback.\n\n\nUsing XPCF remoting framework, the mapping pipeline will be embeded into a server application handling external requests with a gRPC Service, based on the SolAR Framework interface implemented by this pipeline. On the AR client application side, a gRPC Stub, also based on the same SolAR Framework interface, will manage requests to the remote mapping pipeline. This software architecture is illustrated in the following diagram:\n\n\n\n\n\nFigure 24. ARCloud mapping service description\n\n\nBased on this architecture, the data flows between AR client applications and ARCloud remote components are described in the following sequence diagram:\n\n\n\n\n\nFigure 25. ARCloud mapping service sequence diagram\n\n\nThe multithreaded mapping pipeline remote server project is available on GitHub: https://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi\n\n\nAnd two test clients, one sending images to the server and the other getting the point cloud and poses from the server, are also available on GitHub:\n\n\nhttps://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi/tests/SolARServiceTest_Mapping_Multi_Producer\n\n\nhttps://github.com/SolarFramework/Service-Mapping/tree/0.11.0/SolARService_Mapping_Multi/tests/SolARServiceTest_Mapping_Multi_Producer\n\n\n\n\n\nA full ARCloud service: the Map Update service\n\n\nThe Map Update service description\n\nThe Map Update service is actually a set of SolAR Framework based microservices deployed on the ARCloud platform to provide a global map management solution for any AR applications. Its principle is to collect images captured by AR devices to calculate the map (point cloud) of the real environment in which the device moves, and then to build by enrichment a global map of all places where AR applications have been used. Then, this service will be able to quickly relocate a device in this global map. Moreover, it will allow the image capture device to be precisely repositioned and oriented in the point cloud that surrounds it.\n\n\nThus, this figure represents the different microservices that contribute to the Map Update service, as well as the data flows between them:\n\n\n\n\n\nFigure 26. ARCloud map update service description\n\n\n\n\nThe Relocalization Service gets the whole global map from the Global Map service to initialize its context\n\n\nThe AR Device starts sending pairs of images and poses to the Tracking Service\n\n\nThe Tracking Service requests the Relocalization Service to find the initial pose of the device\n\n\nIf the relocalization is successful, the Tracking Service updates the pose provided by the AR Device to define it relative to the coordinate system of the global map using the pose provided by the relocalization service. Then, it sends the corrected poses to the AR Device application, and the pairs of images and corrected poses to the Mapping Service\n\n\nThe Mapping Service attempts to create a local map in the global map coordinate system:\n\n\n\nif successful, it sends this map to the Map Update Service then cleans the local map\n\n\nin case of failure, it creates a floating map, sends it to the Map Overlap Detection Service and then cleans this map\n\n\n\n\n\nThe Map Update Service gets the global map from the Global Map service, and the local maps from the Mapping Service or the Map Overlap Detection Service to:\n\n\n\nmerge the local maps into the global map (fusion)\n\n\ndetect and update map changes (update)\n\n\nremove redundant cloud points and keyframes (pruning)\n\n\nprocess loop closure and bundle adjustment (optimization)\n\n\n\n\n\nThe Map Overlap Detection Service gets the global map from the Global Map service and the floating maps from the Mapping Service to:\n\n\n\ndetect map overlaps\n\n\nprocess map tranformations\n\n\nand finally transform the floating maps into local maps and send them to the Map Update Service\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\nThe ARCloud architecture, based on the SolAR Framework, provides a complete and powerful solution for designers of AR applications through an efficient set of always up-to-date spatial computing pipelines, deployed in the cloud, and available anytime, anywhere. Furthermore, these remote pipelines give users of AR applications a better experience, by pooling all the individual experiences to build the premise of the global digital twin.\n\n\nFor creators of AR vision components and pipeline assemblers, the ARCloud solution offers a complete environment from complexe spatial computing algorithms writting to full vision pipeline deployment:\n- a rich framework, SolAR, offering ready-to-use components and all the tools to design new components based on predefined functionalities, with documentation and support\n- process and tools to:\n    - organise components in modules, and assemble them to create high-level pipelines\n\t- make these pipelines accessible remotly, managing the serialization of complexe data structures\n    - generate remote pipeline bundles to encapsulate them in autonomous docker containers\n\t- deploy and manage spatial computing services, using Kubernetes, on a complete cloud stack architecture, SUPRA, ready for 5G access\n\n\nBut the major asset of the ARCloud solution lies in its community which will make this platform ever richer and more efficient.\n\n\n\n\nGlossary\n\n\n\nb&lt;&gt;com\n\nTechnology Research Institute focused on innovation in areas like Artificial Intelligence, immersive video and audio, content protection, 5G networks, Internet of Things, cognitive technologies (see https://b-com.com/en)\n\nAI\n\nArtificial Intelligence\n\nAPI\n\nApplication Programming Interface\n\nAR\n\nAugmented Reality\n\nBA\n\nBundle Adjustment\n\nCaaS\n\nContainer as a Service\n\nCLI\n\nCommand-Line Interface\n\nCNF\n\nCloud Network Functions\n\nDLL\n\nDynamic Link Library\n\nDSL\n\nDomain Specific Language\n\nGPU\n\nGraphics Processing Unit\n\ngRPC\n\ngeneral-purpose Remote Procedure Calls\n\nGUI\n\nGraphical User Interface\n\nIDE\n\nIntegrated Development Environment (Qt Creator, Visual Studio&#8230;&#8203;)\n\nIT\n\nInformation Technology\n\nK8s\n\nKubernetes\n\nMaaS\n\nMonitoring as a Service\n\nPaaS\n\nPlatform as a Service\n\nSDK\n\nSoftware Development Kit\n\nSLAM\n\nSimultaneous Localization And Mapping\n\nSolAR\n\nSolution for Augmented Reality (b&lt;&gt;com)\n\nSWIG\n\nSimplified Wrapper and Interface Generator\n\nUUID\n\nUniversally Unique IDentifier\n\nVIM\n\nVirtual Infrastructure Manager\n\nVR\n\nVirtual Reality\n\nWEF\n\nWireless Edge Factory (b&lt;&gt;com)\n\nXML\n\nExtensible Markup Language\n\nXPCF\n\nCross-Platform Component Framework (b&lt;&gt;com)\n\n\n\n\n",
      "id": 6
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "create a component",
      "content": "Create a component\n\nTable of Contents\n\nCreate a component\nWhat is a component ?\nCreate a component with the wizard\n\nDownload QTCreator wizards for XPCF\nCreate a SolAR component in QTCreator\nThe component header file\nThe component cpp file\nDeclare your component in your module\nAdd your component to your registry file\n\n\n\n\n\nCreate a component\n\n\n\n\n\nWhat is a component ?\n\n\nA SolAR component is an element embedding processing (and soon storage) capabilities. Components are designed to be connected together in order to create a full pipeline. For this reason, a component ideally contains one (potentially several) processing function defining the input (the data to process) and the ouptut (the processing result). For interoperability issues, the component has to implement a SolAR component interface defined by the SolAR Framework API available  here.\n\n\nAs each component implementation can require dedicated configuration parameters, the SolAR framework provides an easy-to-use mechanism to initialize them at load-time with a external xml configuration file.\n\n\nFinally, when a SolAR component is implemented, it has to be embedded in a SolAR module for its publication to the SolAR pipeline assemblers.\n\n\n\n\nCreate a component with the wizard\n\n\nDownload QTCreator wizards for XPCF\n\nCreating a new component from scratch can be a little bit tricky. To help you, a QTCreator wizard is available and will make the task much easier.\nStart by installing this wizard by launching the install.bat on Windows or install.sh on Linux (in your ${XPCF_MODULE_ROOT}/xpcf/[version]/wizards/qtcreator).\n\n\n\nCreate a SolAR component in QTCreator\n\nOpen QTCreator and create the project of your module for which you want to add a new component.\n\n\nRight click on your Sources folder, and click on Add New&#8230;&#8203; in the contextual menu.\n\n\n\n\n\nFigure 1. Add a new component in QT\n\n\nSelect XPCF and XPCF Component Class and click on Choose button.\n\n\n\n\n\nFigure 2. Create an SolAR Component in QT\n\n\nThen, set the name of your component, inspired by the name of the SolAR interface you will implement. Do not forget to specifiy the namespace of your component based on the common namespace used for your module. In addition, you will have to define the SolAR interface that your component will inherit. Again, do not forget the namespace of this interface. If you need more information about the interfaces available in SolAR, check its API documentation. Finally, you can define if your component is configurable. If yes, you will be able to easily configure it in a dedicated file attached to your pipeline.\n\n\n\n\n\nFigure 3. Set component name and inherited interface in QT\n\n\nNormally, your component class will be added to the project of your module, just select a version control if needed and click on the Finish\nbutton.\n\n\nTwo files have been created, one header file and one cpp file.\n\n\nNow, let&#8217;s take a closer look at these files to implement your component.\n\n\n\nThe component header file\n\nMyDescriptorsExtractor.h\n\n#ifndef MYDESCRIPTORSEXTRACTOR_H\n#define MYDESCRIPTORSEXTRACTOR_H\n#include &lt;xpcf/component/ConfigurableBase.h&gt;\n\n(1)\n#include &quot;IDescriptorsExtractor.h&quot;\n#include &quot;SolARMyModuleAPI.h&quot;\n\nnamespace SolAR {\nnamespace MODULES {\nnamespace MyModule {\n\n(2)\nclass MyModule_API_DLLEXPORT MyDescriptorsExtractor : public org::bcom::xpcf::ConfigurableBase, public virtual SolAR::api::features::IDescriptorsExtractor\n{\npublic:\n    MyDescriptorsExtractor();\n    ~MyDescriptorsExtractor() override;\n    void unloadComponent () override;\n\n(3)\n    inline std::string getTypeString() override { return std::string(&quot;DescriptorsExtractorType::MYCOMPONENT&quot;) ;};\n\n    /// @brief Extracts a set of descriptors from a given image around a set of keypoints based on AKAZE algorithm\n    /// &quot;Fast explicit diffusion for acceleratedfeatures in nonlinear scale space&quot;\n    /// [in] image: source image.\n    /// [in] keypoints: set of keypoints.\n    /// [out] decsriptors: se of computed descriptors.\n    void extract (const SRef&lt;Image&gt; image,\n                  const std::vector&lt; Keypoint &gt; &amp;keypoints,\n                  SRef&lt;DescriptorBuffer&gt; &amp; descriptors) override;\n\n};\n\n\n} // namespace MyModule\n} // namespace MODULES\n} // namespace SolAR\n\n(4)\ntemplate &lt;&gt; struct org::bcom::xpcf::ComponentTraits&lt;SolAR::MODULES::MyModule::MyDescriptorsExtractor&gt;\n{\n\n    static constexpr const char * UUID = &quot;{4bc8c7d0-9e57-4816-8e92-e26103cc43f7}&quot;;\n    static constexpr const char * NAME = &quot;MyDescriptorsExtractor&quot;;\n    static constexpr const char * DESCRIPTION = &quot;MyDescriptorsExtractor implements SolAR::api::features::IDescriptorsExtractor interface&quot;;\n};\n\n#endif // MYDESCRIPTORSEXTRACTOR_H\n\n\n\n\n\n1\nAdd the include files for the SolAR abstract interface that your component inherits, as well as the header file defining the export declaration MACRO of your module.\n\n\n2\nAdd your export declaration MACRO defined in MyModuleAPI.h file to export your component in the dynamic library of your module (Here MyModule_API_DLLEXPORT).\n\n\n3\nHere, you will have to define the interface of your component inherited from your SolAR interface. For instance, the getStringType and the extract methods declared in the IDescriptorExtractor abstract interface.\n\n\n4\nThe traits are automatically defined with a automatically generated UUID, the name of the component and a description that you are free to change.\n\n\n\n\nAs in this sample you are implementing a descriptor extractor component inherited from the SolAR interface IDescriptorsExtractor, you will have to add the following interface to your component:\n\n\n\nstd::string getTypeString() override;\n\n/// @brief Extracts a set of descriptors from a given image around a set of keypoints based on my wonderfull algorithm\n/// [in] image: source image.\n/// [in] keypoints: set of keypoints.\n/// [out] decsriptors: se of computed descriptors.\nvoid extract (const SRef&lt;Image&gt; image, const std::vector&lt; Keypoint &gt; &amp;keypoints, SRef&lt;DescriptorBuffer&gt; &amp; descriptors) override;\n\n\n\n\n\n\n\n\n\nAs the SolAR interfaces are defined with abstract class with pure virtual methods, you will have to declare in your component all the pure virtual methods declared in the interface it inherits.\n\n\n\n\n\n\n\n\n\n\n\nOf course, you can add any method and variable required by your component, but in private, as the only access to a component should be done through the inherited SolAR interface!\n\n\n\n\n\n\nThe component cpp file\n\nMyDescriptorsExtractor.cpp\n\n#include &quot;MyDescriptorsExtractor.h&quot;\n\n\nnamespace xpcf = org::bcom::xpcf;\n\ntemplate&lt;&gt; SolAR::MODULES::MyModule::MyDescriptorsExtractor * xpcf::ComponentFactory::createInstance&lt;fullComponentType&gt;();\n\n\nnamespace SolAR {\nnamespace MODULES {\nnamespace MyModule {\n\nMyDescriptorsExtractor::MyDescriptorsExtractor():xpcf::ConfigurableBase(xpcf::toMap&lt;MyDescriptorsExtractor&gt;())\n{\n\n    declareInterface&lt;SolAR::api::features::IDescriptorsExtractor&gt;(this);\n    (1)\n    //  Inject declarations come here : declare any component that must be injected to your component through its interface\n    // declareInjectable&lt;IFilter&gt;(m_filter);\n    //\n    // Inject declaration can have a name :\n    // declareInjectable&lt;IFilter&gt;(m_blurFilter, &quot;blurFilter&quot;);\n    //\n    // Inject declaration can be optional i.e. not finding a binding component for the interface is not an error :\n    // declareInjectable&lt;IImageFilter&gt;(m_imageFilter, false);\n\n    // wrap any component member variable to expose as properties with declareProperty&lt;T&gt;() with T matching the variable type\n    // For example : declareProperty&lt;float&gt;(&quot;blurFactor&quot;,m_blurFactor);\n    // declareProperty(&quot;name&quot;,m_memberVariable) also works with template type deduction when m_memberVariable is a supported type of IProperty\n}\n\nMyDescriptorsExtractor::~MyDescriptorsExtractor()\n{\n\n}\n\n(2)\nvoid MyDescriptorsExtractor::unloadComponent ()\n{\n    // provide component cleanup strategy\n\n    // default strategy is to delete self, uncomment following line in this case :\n    // delete this;\n    return;\n}\n\n(3)\nxpcf::XPCFErrorCode MyDescriptorsExtractor::onConfigured()\n\n{\n    // Add custom onConfigured code\n    return xpcf::XPCFErrorCode::_SUCCESS;\n}\n\n(4)\nvoid MyDescriptorsExtractor::extract(const SRef&lt;Image&gt; image, const std::vector&lt;Keypoint&gt; &amp; keypoints, SRef&lt;DescriptorBuffer&gt; &amp; descriptors)\n{\n    // Add the code to extract your descriptor\n\n}\n\n} // namespace MyModule\n} // namespace MODULES\n} // namespace SolAR\n\n\n\n\n\n1\nIn the constructor of your component, you can simply inject another component to your current component. Thus, you will be able to define which component you want to inject in your configuration file. You can also define here the class variable members you want to configure from your configuration file. Both component injection and variable configuration funciton can be set in just a line of code.\n\n\n2\nSpecify here what your component has to do before it will be deleted.\n\n\n3\nSpecify here some processing to do when the class member variable values have been set according to the configuration file.\n\n\n4\nAdd the implementation of the method abstract declared by the inherited SolAR interface. Here, the extract method of the IDescriptorExtractor interface.\n\n\n\n\nNow, you can code the implementation of your methods defined in your header file.\n\n\n\nDeclare your component in your module\n\nThat is great, you have created your component, you have implemented its functions, but now you have to declare it in the module.\n\n\nTo do that, just open the main file of your module, and add the following lines of code:\n\n\nMyModule_main.cpp\n\n#include &lt;xpcf/module/ModuleFactory.h&gt;\n#include &lt;iostream&gt;\n\n#include &quot;MyDescriptorExtractor.h&quot; (1)\n\nnamespace xpcf=org::bcom::xpcf;\n\n/**\n *  @ingroup xpcfmodule\n */\n/**\n  * Declare module.\n  */\nXPCF_DECLARE_MODULE(&quot;{41a884a8-645b-47bc-9f41-66d057b2ec5d}&quot;,&quot;SolAR::MODULES::MyModule&quot;,&quot;MyModule module description&quot;);\n\n/**\n * This method is the module entry point.\n * XPCF uses this method to create components available in the module.\n *\n * Each component exposed must be declared inside a xpcf::tryCreateComponent&lt;ComponentType&gt;() call.\n */\nextern &quot;C&quot; XPCF_MODULEHOOKS_API xpcf::XPCFErrorCode XPCF_getComponent(const xpcf::uuids::uuid&amp; componentUUID,SRef&lt;xpcf::IComponentIntrospect&gt;&amp; interfaceRef)\n{\n    xpcf::XPCFErrorCode errCode = xpcf::XPCFErrorCode::_FAIL;\n    errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::MyModule::MyDescriptorExtractor&gt;(componentUUID,interfaceRef); (2)\n //   if (errCode != xpcf::XPCFErrorCode::_SUCCESS) {\n //       errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::MyModule::OtherComponents&gt;(componentUUID,interfaceRef);\n //   }\n    return errCode;\n}\n\n/**\n  * The declarations below populate list of the components available in the module (it represents the module index).\n  * XPCF uses this index to introspect the components available in a module, providing the ability to generate the configuration file skeleton from the code.\n  */\nXPCF_BEGIN_COMPONENTS_DECLARATION\n(3)\nXPCF_ADD_COMPONENT(SolAR::MODULES::MyModule::MyDEscriptorsExtractor)\nXPCF_END_COMPONENTS_DECLARATION\n\n\n\n\n\n1\nAdd the header file of the component you want to add to your module\n\n\n2\nAdd the next 3 following lines of code to your module main file for each component you want to add to your module (update the second line with the name of your component).\n\n\n3\nAdd your component in the component declaration of your module.\n\n\n\n\n\nAdd your component to your registry file\n\nFinally, edit the xpcf_MyModule_registry.xml that should be at the root folder of your module project. Add a description for each component you want to embed in your module as follows:\n\n\nxpcf_MyModule_registry.xml\n\n&lt;xpcf-registry&gt;\n&lt;module uuid=&quot;5b066de7-2a9f-4dff-a730-ce82adafe2f6&quot; name=&quot;MyModule&quot; description=&quot;MyModule_Description&quot; path=&quot;$XPCF_MODULE_ROOT/SolARBuild/MyModule/0.11.0/lib/x86_64/shared&quot;&gt;\n\n&lt;component uuid=&quot;4bc8c7d0-9e57-4816-8e92-e26103cc43f7&quot; name=&quot;MyDesrciptorsExtractor&quot; description=&quot;MyDesrciptorsExtractor Description&quot;&gt; (1)\n        &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt; (2)\n        &lt;interface uuid=&quot;c0e49ff1-0696-4fe6-85a8-9b2c1e155d2e&quot; name=&quot;IDescriptorsExtractor&quot; description=&quot;SolAR::api::features::IDescriptorsExtractor&quot;/&gt; (3)\n&lt;/component&gt;\n\n&lt;/module&gt;\n&lt;/xpcf-registry&gt;\n\n\n\n\n\n1\nCopy and paste the component UUID, name and description defined in the traits defined in the component header file.\n\n\n2\nKeep always this declaration related to the inheritance to IComponentIntrospect.\n\n\n3\nAdd a description to all SolAR interfaces inherited by your component. To get access to UUID of SolAR interfaces, please take a look at the SolAR framework API documentation.\n\n\n\n\nThis is done, your component has been added to your module. You can now build your module, and use it in your pipelines.\n\n\n\n",
      "id": 7
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "contribution workflow",
      "content": "Contribution workflow\n\nTable of Contents\n\nContribution workflow\nContribute to SolAR Framework\nContributing by creating your own components\nContact\n\n\n\nContribution workflow\n\n\n\n\n\n\n\n\n\nContribute to SolAR Framework\n\n\nContributers may be interested in enhancing Solar Framework. For instance, they may want to:\n\n\n\n\ndefine new interfaces or new data structures\n\n\npropose bug fixes\n\n\netc.\n\n\n\n\nThe proposed workflow is based on github forks and pull requests:\n\n\n\n\nif not already done, create a github account: github\n\n\nopen the url of the repository related to the project you want to contribute to. The list of repositories and their urls is here: github SolAR repositories\n\n\nclick on the \"fork\" button in order to copy the repository to your own github account\n\n\nkeep your fork up to date: read the page help github in order\nto sync your local copy with the original repository\n\n\n\n\nYou may then ask the upstream repository to accept your changes: this is performed by creating a pull request.\nFollow the instructions at pull request documentation to see how to create a pull request\nbetween your fork and the original repository.\n\n\nWhen your pull request is created, request for a pull request review, by following these instructions: https://help.github.com/articles/requesting-a-pull-request-review/\n\n\n\n\nContributing by creating your own components\n\n\nYou may be interested by developing your own components based on SolAR interfaces.\nIn this case, create your own github repository, and create any SolAR components and packages you need by following these instructions.\n\n\nOnce your component is created, don&#8217;t hesitate to contact us!\n\n\n\n\nContact\n\n\nFor any request, please contact us at framework.solar@b-com.com.\n\n\n",
      "id": 8
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "C++",
      "content": "\nTable of Contents\n\nUse a pipeline in C++\nInstall SolAR before creating your app\nCreate an application project\n\nQT Creator\nVisual Studio\n\n\nSelect your dependencies\nConfigure your pipeline\nmain.cpp\n\n\n\nUse a pipeline in C++\n\n\n\n\n\nInstall SolAR before creating your app\n\n\nRefer to Install section.\n\n\n\n\n\n\n\n\nInstall the XPCF Wizards.\n\n\n\n\n\n\n\nCreate an application project\n\n\nQT Creator\n\n\n\nOpen QTCreator and create a new project (in File menu).\n\n\n\n\n\n\n\n\n\n\n\nThen, set the name of your application (e.g. MySolARApp) that will use a SolAR pipeline, and its location.\n\n\n\n\n\n\n\n\n\n\n\nThe next step let you configure project details relative to build options. Select Use QTVS if you intend to use Visual Studio Qt extension.\n\n\n\n\n\n\n\n\n\n\n\nGive the location of the installed version of XPCF to use for this application\n\n\n\n\n\n\n\n\n\n\n\nNext, define your build system with qmake. For the next step, choose your development kits. We recommend to use MSVC 2017 64bit on Windows or Clang on Linux.\n\n\n\n\n\n\n\n\n\n\n\nFinally, click on Finish on the summary panel to generate the files and complete the project setup.\n\n\n\n\n\n\n\n\n\nYour project is now created and the result is a .pro file like following:\n\n\nMySolARApp.pro\n\n## remove Qt dependencies\nQT -= core gui\nTARGET = MySolARApp  (1)\nVERSION=x.x.x (2)\n\nCONFIG += c++1z\nCONFIG -= qt\nCONFIG += console\n\nDEFINES += MYVERSION=$${VERSION}\n\nQMAKE_PROJECT_DEPTH = 0\n\ninclude(findremakenrules.pri)\n\nCONFIG(debug,debug|release) {\n    DEFINES += _DEBUG=1\n    DEFINES += DEBUG=1\n}\n\nCONFIG(release,debug|release) {\n    DEFINES += _NDEBUG=1\n    DEFINES += NDEBUG=1\n}\n\nDEPENDENCIESCONFIG = sharedlib install_recurse\n\nwin32:CONFIG -= static\nwin32:CONFIG += shared\n\n## Configuration for Visual Studio to install binaries and dependencies. Work also for QT Creator by replacing QMAKE_INSTALL\nPROJECTCONFIG = QTVS\n\n#NOTE : CONFIG as staticlib or sharedlib, DEPENDENCIESCONFIG as staticlib or sharedlib, QMAKE_TARGET.arch and PROJECTDEPLOYDIR MUST BE DEFINED BEFORE templatelibconfig.pri inclusion\ninclude ($$shell_quote($$shell_path($${QMAKE_REMAKEN_RULES_ROOT}/templateappconfig.pri)))  # Shell_quote &amp; shell_path required for visual on windows\n  (3)\n\nDEFINES += BOOST_ALL_DYN_LINK\nDEFINES += BOOST_AUTO_LINK_NOMANGLE\nDEFINES += BOOST_LOG_DYN_LINK\n\nSOURCES += \\\n    main.cpp\n\nunix {\n    LIBS += -ldl\n    QMAKE_CXXFLAGS += -DBOOST_LOG_DYN_LINK\n}\n\nmacx {\n    QMAKE_MAC_SDK= macosx\n    QMAKE_CXXFLAGS += -fasm-blocks -x objective-c++\n}\n\nwin32 {\n    QMAKE_LFLAGS += /MACHINE:X64\n    DEFINES += WIN64 UNICODE _UNICODE\n    QMAKE_COMPILER_DEFINES += _WIN64\n    QMAKE_CXXFLAGS += -wd4250 -wd4251 -wd4244 -wd4275\n\n    # Windows Kit (msvc2013 64)\n    LIBS += -L$$(WINDOWSSDKDIR)lib/winv6.3/um/x64 -lshell32 -lgdi32 -lComdlg32\n    INCLUDEPATH += $$(WINDOWSSDKDIR)lib/winv6.3/um/x64\n\n}\n\nandroid {\n    ANDROID_ABIS=\"arm64-v8a\"\n}\n\nDISTFILES += \\\n    MySolARApp_conf.xml \\\n    packagedependencies.txt\n\n#NOTE : Must be placed at the end of the .pro\ninclude ($$shell_quote($$shell_path($${QMAKE_REMAKEN_RULES_ROOT}/remaken_install_target.pri)))) # Shell_quote &amp; shell_path required for visual on windows\n\n\n\nNow, just update the .pro file:\n\n\n\n\n1\nset the TARGET with the name of your application,\n\n\n2\nset the version number of your application,\n\n\n3\ncheck if the builddefs folder used to define the building pipeline is well referenced\n\n\n\n\nFinally, click on Projects in the left menu of QTcreator, click on Run, set your working directory to the root directory of your project, and check _Add build library search path to PATH if not already done.\n\n\n\nVisual Studio\n\nYou can also simply create your application with Visual Studio by using the .pro file (see above for more details how to configure the .pro file).\n\n\nMicrosoft Visual Studio provides a Qt Visual Studio Tools. This enables developers to import QT project files (.pro) into Visual Studio.\n\n\nInstall QT Visual Studio Tools:\n\n\n\n\nIn Visual Studio, select Tools &gt; Extensions and Updates &gt; Online to install and update QT Visual Studio Tools.\n\n\n\n\nImport the .pro. file into Visual Studio:\n\n\n\n\nSelect Qt VS Tools &gt; Open Qt Project File (.pro) and choose your .pro file.\n\n\n\n\nRight now, your project is configured.\n\n\n\n\n\nSelect your dependencies\n\n\nCopy in your project folder the packagedependencies.txt file of the pipeline you want to use, and add a dependency to the pipeline:\n\n\npackagedependencies.txt\n\nSolARFramework|0.10.0|SolARFramework|SolARBuild@github|https://github.com/SolarFramework/SolarFramework/releases/download\n(1)\nOtherDependency|version x.x.x|OtherDependency|LocalFolder@WhereDistant|urlToIt\n\n\n\n\n\n1\nAdd the dependency to the pipeline you want to use, with its version number.\n\n\n\n\nMore documentation here :\nRemaken is a dependencies manager. https://github.com/b-com-software-basis/remaken\nIt manages your third parties and it creates a QMAKE_REMAKEN_RULES_ROOT env. variable.\n\n\n\n\nConfigure your pipeline\n\n\nCopy the configuration file provided with your pipeline in your project folder:\n\n\nMySolARApp_conf.xml\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot; ?&gt;\n&lt;xpcf-registry autoAlias=&quot;true&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;\n\n&lt;module uuid=&quot;94b21be9-703b-4a00-86a9-0db1bf70ef89&quot; name=&quot;MyPipelineModule&quot; path=&quot;$XPCF_MODULE_ROOT/SolARBuild/MyPipeline/x.x.x/lib/x86_64/shared&quot; description=&quot;MyPipelineModule&quot;&gt;\n  &lt;component uuid=&quot;855c83b7-f4ec-48ab-8e89-56018ea9e169&quot; name=&quot;MyPipeline&quot; description=&quot;MyPipeline&quot;&gt;\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt;\n    &lt;interface uuid=&quot;b5a6225e-6a91-4050-b298-886f4c17d9d2&quot; name=&quot;IPoseEstimationPipeline&quot; description=&quot;The interface to define a video see-throuh pipeline&quot; /&gt;\n  &lt;/component&gt;\n&lt;/module&gt;\n&lt;module uuid=&quot;15e1990b-86b2-445c-8194-0cbe80ede970&quot; name=&quot;SolARModuleOpenCV&quot; description=&quot;SolARModuleOpenCV&quot; path=&quot;$XPCF_MODULE_ROOT/SolARBuild/SolARModuleOpenCV/0.10.0/lib/x86_64/shared&quot;&gt;\n  &lt;component uuid=&quot;5B7396F4-A804-4F3C-A0EB-FB1D56042BB4&quot; name=&quot;SolARCameraOpencv&quot; description=&quot;SolARCameraOpencv&quot;&gt;\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt;\n    &lt;interface uuid=&quot;5DDC7DF0-8377-437F-9C81-3643F7676A5B&quot; name=&quot;ICamera&quot; description=&quot;ICamera&quot;/&gt;\n  &lt;/component&gt;\n&lt;/module&gt;\n&lt;module uuid=&quot;28b89d39-41bd-451d-b19e-d25a3d7c5797&quot; name=&quot;SolARModuleTools&quot;  description=&quot;SolARModuleTools&quot;  path=&quot;$XPCF_MODULE_ROOT/SolARBuild/SolARModuleTools/0.10.0/lib/x86_64/shared&quot;&gt;\n  &lt;component uuid=&quot;85db2f25-4f1c-4e06-9011-e020284bfc4f&quot; name=&quot;SolARBasicSink&quot; description=&quot;A Sink component for a synchronized pose and image pair&quot;&gt;\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot; /&gt;\n    &lt;interface uuid=&quot;c0d9fee4-d7d7-4866-a6cd-3bacac23316a&quot; name=&quot;ISinkPoseImage&quot; description=&quot;An interface allowing to store a synchronized pose and image from the pipeline to make it available to a third party application&quot; /&gt;\n  &lt;/component&gt;\n  &lt;component uuid=&quot;1e43cda9-7850-4a8a-a32b-f3f31ea94902&quot; name=&quot;SolARBasicSource&quot; description=&quot;A source component for feeding the pipeline with external images&quot;&gt;\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot; /&gt;\n    &lt;interface uuid=&quot;06e2fc5d-39da-4486-b2a6-1d8bd788fa13&quot; name=&quot;ISourceImage&quot; description=&quot;An interface allowing to store an image from extern source to make it available to the pipeline&quot; /&gt;\n  &lt;/component&gt;\n&lt;/module&gt;\n\n&lt;properties&gt;\n  &lt;configuration component=&quot;SolARCameraOpencv&quot;&gt;\n    &lt;property name=&quot;calibrationFile&quot; type=&quot;string&quot; value=&quot;camera_calibration.yml&quot;/&gt;\n    &lt;property name=&quot;deviceID&quot; type=&quot;uint&quot; value=&quot;0&quot;/&gt;\n  &lt;/configuration&gt;\n&lt;/properties&gt;\n\n&lt;/xpcf-registry&gt;\n\n\n\n\n\nmain.cpp\n\n\nCopy the following code in your main.cpp file for testing your SolAR pipeline:\n\n\nmain.cpp\n\n/**\n * information concerning the copyright and license of your SolAR application\n */\n\n#include &lt;boost/log/core.hpp&gt;\n#include &quot;core/Log.h&quot;\n#include &quot;xpcf/xpcf.h&quot;\n\n\n// ADD COMPONENTS HEADERS HERE, e.g #include &quot;SolarComponent.h&quot;\n#include &quot;api/pipeline/IPoseEstimationPipeline.h&quot;\n#include &quot;api/display/IImageViewer.h&quot;\n#include &quot;api/display/I3DOverlay.h&quot;\n#include &quot;datastructure/CameraDefinitions.h&quot;\n\nnamespace xpcf  = org::bcom::xpcf;\n\nusing namespace SolAR;\nusing namespace SolAR::datastructure;\nusing namespace SolAR::api;\n\nint main(){\n\n#if NDEBUG\n    boost::log::core::get()-&gt;set_logging_enabled(false);\n#endif\n\n    try {\n        LOG_ADD_LOG_TO_CONSOLE();\n        SRef&lt;xpcf::IComponentManager&gt; componentMgr = xpcf::getComponentManagerInstance();\n        componentMgr-&gt;load(&quot;MySolARApp_conf.xml&quot;);\n        auto pipeline = componentMgr-&gt;resolve&lt;pipeline::IPoseEstimationPipeline&gt;();\n\n        (1)\n        if (pipeline-&gt;init() == FrameworkReturnCode::_SUCCESS)\n        {\n            auto imageViewerResult = componentMgr-&gt;resolve&lt;display::IImageViewer&gt;();\n            auto overlay3DComponent = componentMgr-&gt;resolve&lt;display::I3DOverlay&gt;();\n\n            // Set camera parameters\n            CameraParameters camParam = pipeline-&gt;getCameraParameters();\n            overlay3DComponent-&gt;setCameraParameters(camParam.intrinsic, camParam.distortion);\n\n            unsigned char* r_imageData=new unsigned char[camParam.resolution.width * camParam.resolution.height * 3];\n            SRef&lt;Image&gt; camImage=xpcf::utils::make_shared&lt;Image&gt;(r_imageData,camParam.resolution.width,camParam.resolution.height, Image::LAYOUT_BGR, Image::INTERLEAVED, Image::TYPE_8U);\n\n            Transform3Df s_pose;\n\n            if (pipeline-&gt;start(camImage-&gt;data()) == FrameworkReturnCode::_SUCCESS)\n            {\n                while (true)\n                {\n                    Transform3Df pose;\n\n                    sink::SinkReturnCode returnCode = pipeline-&gt;update(pose);\n\n                    if (returnCode == sink::SinkReturnCode::_NOTHING)\n                        continue;\n\n                    if ((returnCode == sink::SinkReturnCode::_NEW_POSE) || (returnCode == sink::SinkReturnCode::_NEW_POSE_AND_IMAGE))\n                    {\n                        for(int i=0;i&lt;3;i++)\n                            for(int j=0;j&lt;3;j++)\n                                s_pose(i,j)=pose(i,j);\n                        for(int i=0;i&lt;3;i++)\n                                s_pose(i,3)=pose(i,3);\n                        for(int j=0;j&lt;3;j++)\n                            s_pose(3,j)=0;\n                        s_pose(3,3)=1;\n                        overlay3DComponent-&gt;draw(s_pose, camImage);\n                    }\n\n                    if (imageViewerResult-&gt;display(camImage) == SolAR::FrameworkReturnCode::_STOP){\n                        pipeline-&gt;stop();\n                        break;\n                    }\n                }\n            }\n            delete[] r_imageData;\n        }\n    }\n    catch (xpcf::Exception e)\n    {\n        LOG_ERROR (&quot;The following exception has been caught : {}&quot;, e.what());\n        return -1;\n    }\n}\n\n\n\n\n\n1\nwhen you will load your pipeline, fill in the path to its configuration file and its UUID (the UUID of the component representing the pipeline available in the configuration file).\n\n\n\n\n\n\n\n\n\n\nDon&#8217;t forget to re-run qmake before building your application for QT Creator and re-import .pro file for Visual Studio.\n\n\n\n\n\nBuild and run your application.\n\n\n",
      "id": 9
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "SolAR Framework architecture",
      "content": "SolAR Framework architecture\n\nTable of Contents\n\nSolAR Framework architecture\nIntroduction\nPresentation of SolAR Framework\nSolAR encapsulation concepts\n\nSolAR Components\nSolAR Modules\nSolAR Pipelines\n\n\nSolAR Framework structure\n\nSolAR Framework Core\nThird-party dependencies\n\n\nManagement of dependencies for SolAR projects\n\nInstallation of packages\nBuild rules management\n\n\nUnity Plugin\nGlossary\n\n\n\nSolAR Framework architecture\n\n\n\n\n\nIntroduction\n\n\nAugmented Reality (AR) applications developers and end-users face a dilemma.\n\n\nOn one hand, major IT actors have released toolkits to develop AR applications. Nevertheless, they do not always meet the specific needs required by dedicated use cases or contextual environments (localization accuracy, lighting conditions, indoor/outdoor environments, tracking area range, dynamic scenes, etc.).\n\n\nNo solution fits all, and generally, these toolkits do not provide the level of tuning required to optimally adapt the vision based localization solution to the use case. Moreover, these closed solutions do not always ensure the confidentiality of data, and can store information concerning your environment (3D maps, key frames) or the augmentations (3D meshes, procedure scenarios) that could contain crucial intellectual property and private information.\n\n\nOn the other hand, open source vision libraries and SLAM implementations can generally be modified and configured to optimally meet AR applications requirements. However, many SLAM implementations generally developed by academic actors do not provide the license or the level of maturity required for the development of commercial solutions. Likewise, open source vision libraries offer a huge number of low-level functions but require a huge expertise and important development resources to obtain a usable camera pose estimation pipeline ready for commercial use.\n\n\nTo that end, SolAR offers an alternative to current commercial AR SDKs or existing open-source solutions, providing the benefits of both worlds – openness, ease of use, efficiency, adaptiveness. It aims at creating an ecosystem bringing researchers, developers, and end-users together to help the adoption of augmented reality.\n\n\n\n\nThe purpose of this documentation is to describe the architecture of the SolAR framework: the modules that compose this framework with their interfaces and data structures, the external libraries used, the links between all these elements and the way these dependences are managed. Then, the notion of AR pipeline will be presented through concrete examples. Finally, we show how to plug SolAR pipelines in Unity to create AR applications.\n\n\n\n\n\n\nPresentation of SolAR Framework\n\n\nSolAR is an open-source framework under Apache v2 licence dedicated to Augmented Reality.\n\n\nSolAR is modular and evolutive. It allows, in various contexts of use, the construction of customized computer vision pipelines targeting Augmented Reality applications.\n\n\nThis framework offers a C++ SDK to easily and quickly develop and use custom solutions for camera pose estimation or 3D mapping. It provides developers with a full chain from low-level vision components development to camera pose estimation pipelines and AR service development.\n\n\nSolAR contains:\n\n\n\n\ninterfaces promoting interoperability\n\n\ncomputer vision components constructed from third-party bricks\n\n\nplugins for third-party applications\n\n\n\n\nAccording to user needs, SolAR framework offers several profiles:\n\n\n\n\nSolAR pipeline user: use of prebuilt standalone or Unity integrated AR pipelines to design AR applications\n\n\nSolAR pipeline assembler: assembly of components to build new customized AR pipelines\n\n\nSolAR component developer: creation of customized bricks responding to specific needs\n\n\nSolAR framework contributor: contribution to SolAR expansion\n\n\n\n\nThe SolAR Framework addresses the full chain of AR applications development related to computer vision:\n\n\n\n\n\nFigure 1. SolAR Framework global process for AR application development\n\n\n\n\nComponent creation: SolAR defines a unique API for basic components required for computer vision pipelines (features detection, descriptors extraction, matching, Perspective N Points, homography, image filters, bundle adjustment, etc.). The SolAR community can implement new components compliant with the SolAR API, or wrap components available in third party libraries (e.g. OpenCV, Ceres, G2O, or Point Cloud Library).\n\n\nComponent publication: A set of components are packaged into modules to ease their publication. SolAR modules, whether royalty free or under a commercial license, are stored on an artefact repository to be available to the SolAR community of pipeline assemblers.\n\n\nVision pipeline assembling: Modules of components published by the SolAR community can be downloaded from the modules repositories. Then, the SolAR framework provides developers with a pipeline mechanism allowing them to assemble SolAR components to define their own vision pipelines such as a camera pose estimation solution.\n\n\nVision pipeline publication: When a vision pipeline has been assembled and configured by default, it can be published in a repository of artefacts dedicated to SolAR vision pipelines to make it accessible to the SolAR pipeline users.\n\n\nAR service development: Thanks to a SolAR plugin for Unity, XR service developers can download SolAR pipelines stored on the dedicated artefact repository and integrate them in few clicks to their applications. They can simply develop AR applications as with any AR SDK and deploy them. Since SolAR is based on a unified interface, AR application developers will be able to easily make their applications evolve with the new solutions developed by the SolAR community.\n\n\n\n\nThe SolAR Framework is made up of several parts:\n\n\n\n\nSolAR Core offers all features to create AR components (as a component developer) and to manage vision pipelines (as a pipeline assembler)\n\n\nSolAR Components are concrete implementations of vision processing algorithms: each component can define its own parameters that will be used to fine tune the vision pipeline\nAlso, a SolAR component of higher level can embed a low-level component based on a injection mechanism\n\n\nSolAR Modules are shared libraries embedding a set of SolAR components to ease their management and publication\n\n\nSolAR Pipelines are chains of connected components designed to process and analyze image data, step by step, and can be used to create AR applications (as a pipeline user)\n\n\nUnity Plugin is dedicated to pipelines built to be used in Unity\n\n\n\n\nThe SolAR Framework uses several third-party dependencies, coming from b&lt;&gt;com or other parties, such as XPCF, Boost, spdlog, Eigen, which will be detailed in this document.\n\n\n\n\n\nFigure 2. SolAR Framework global view\n\n\nThe SolAR initiative was launched by the b&lt;&gt;com Institute of Research and Technology, and is open to any contributors or users who share the SolAR goals.\n\n\nSolAR web site: http://www.solarframework.org\n\n\n\n\nSolAR encapsulation concepts\n\n\nThe SolAR Framework offers 3 levels of encapsulation to model:\n\n\n\n\nunitary vision processing: components\n\n\ncomponents organization: modules\n\n\nvision processing chain: pipelines\n\n\n\n\nThe relationships between these 3 concepts can be represented like this:\n\n\n\n\n\nFigure 3. Components, Modules and Pipelines\n\n\nA pipeline corresponds to a sequence of functions provided by several components, which are embedded in different modules. Each module contains many components, and an assembler has to choose among all the proposed modules the components he needs to build his own pipeline.\n\n\nTo create and manage components, modules or pipelines, \"best-practices\" are presented on SolAR web site: http://www.solarframework.org/community/best_practices/\n\n\nSolAR Components\n\nSolAR components are elements embedding processing or storage capabilities. Components are designed to be connected together in order to create a full pipeline. For this reason, a component ideally defines one (as an exception several) processing function generally defining  input(s) (the data to process) and ouptut(s) (the processed data). For interoperability purposes, a component must implement a SolAR component interface defined by the SolAR Framework API (see Framework API).\n\n\nEach module implementation and interface is identified by a Universally Unique IDentifier (UUID) to nearly ensure the uniqueness of a component implementation when the system instantiates it.\n\n\nAs each component implementation can require dedicated configuration parameters, the SolAR framework provides an easy-to-use mechanism, based on XPCF tool, to initialize them at load-time with an external XML configuration file.\n\n\nFinally, when a SolAR component is implemented, it has to be embedded in a SolAR module for its publication to the SolAR pipeline assemblers.\n\n\nAll this steps are discribed on the SolAR web site: http://www.solarframework.org/create/component/#create_component\n\n\n\nSolAR Modules\n\nA SolAR module consists of a shared library embedding a set of SolAR components as well as the corresponding factories to instantiate them. The management of modules is based on the XPCF tool, which provides the following features:\n\n\n\n\nintrospection to figure out which components are available in a module\n\n\nseparate implementation from interface to create the concrete implementation of a SolAR component and to bind it to an abstract SolAR and XPCF component interfaces\n\n\ncomponent creation\n\n\n\n\nEach module implementation is identified with a Universally Unique IDentifier (UUID) to nearly ensure the uniqueness of the module when the system loads it.\nAs mentionned previoulsy, the third party tool XPCF can introspect the shared library of a module to obtain information about the embedded components and the interfaces they implement. As a result, the introspection of a shared library requires to load it, which could become tricky when the shared library has been built on a different platform (useful for authoring tools supporting cross-platform compilation such as Unity).\n\n\nFor this reason, XPCF proposes to associate to each module a registry file in xml format that describes the module with:\n\n\n\n\nthe module UUID\n\n\nthe components embedded in the module with their UUID\n\n\nthe abstract component interfaces implemented by the components\n\n\n\n\nThus, any system will be able to introspect a module without the need to load the corresponding shared library.\n\n\nThe SolAR module management is presented on the web site: http://www.solarframework.org/create/module/\n\n\nSome modules are already provided with the SolAR Framework, and will be presented below.\n\n\nSolARModuleCeres\n\nThis module is based on a third-party library called Ceres Solver.\nCeres Solver is an open source C++ library for modeling and solving large, complicated optimization problems. It can be used to solve Non-linear Least Squares problems with bounds constraints and general unconstrained optimization problems.\n\n\nCeres Solver is licensed under the BSD license.\n\n\nCeres Solver web site: http://ceres-solver.org\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleCeres\n\n\n\nSolARModuleFBOW\n\nThis module is based on a third-party library called FBOW.\nFBOW (Fast Bag of Words) is an extremmely optimized version of the DBow2/DBow3, which are open source C++ libraries for indexing and converting images into a bag-of-word representation. The library is highly optimized to speed up the Bag of Words creation using AVX,SSE and MMX instructions. It requires OpenCV library.\n\n\nFBOW is licensed under the MIT license.\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleFBOW\n\n\n\nSolARModuleG2O\n\nThis module aggregates components dedicated to bundle adjustement optimization based on g2o library.\ng2o is an open-source C++ framework for optimizing graph-based nonlinear error functions. g2o has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA.\n\n\ng2o is licenced under LGPL v3.\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleG2O\n\n\n\nSolARModuleOpenCV and SolARModuleNonFreeOpenCV\n\nThese two SolAR modules are based on OpenCV (Open Source Computer Vision Library), which is a library of programming functions mainly aimed at real-time computer vision.\n\n\nOpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n\n\n\nCore functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n\n\nImage Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n\n\nVideo Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n\n\nCamera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n\n\n2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n\n\nObject Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n\n\nHigh-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n\n\nVideo I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n\n\n&#8230;&#8203; some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.\n\n\n\n\nThe library is cross-platform and free for use under the open-source BSD license.\n\n\nHowever, note that there are algorithms in OpenCV that have been patented and hence require a separate licence agreement if used for commercial purposes: that is the reason why a SolARModuleNonFreeOpenCV module has been designed, to group these \"non free\" part of the library.\n\n\nOpenCV web site: https://opencv.org/\n\n\nThe SolARModuleOpenCV module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleOpenCV\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModuleNonFreeOpenCV module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleNonFreeOpenCV\n\n\n\nSolARModuleOpenGL\n\nThis SolAR module is based on OpenGL (Open Graphics Library) which is a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering.\n\n\nThe OpenGL specification describes an abstract API for drawing 2D and 3D graphics. Although it is possible for the API to be implemented entirely in software, it is designed to be implemented mostly or entirely in hardware.\n\n\nOpenGL is is a Free Software License B.\n\n\nOpenGL web site: https://www.opengl.org/\n\n\nSolARModuleOpenGL also uses the freeglut library, which is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library.\n\n\nGLUT (and hence freeglut) takes care of all the system-specific chores required for creating windows, initializing OpenGL contexts, and handling input events, to allow for trully portable OpenGL programs.\n\n\nfreeglut is released under the MIT license.\n\n\nfreeglut web site: http://freeglut.sourceforge.net/\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModuleOpenGL module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleOpenGL\n\n\n\nSolARModuleOpenGV\n\nThis module is based on OpenGV (Open Geometric Vision), a C++ library designed to solve geometric computer vision problems. It contains efficient implementations of absolute-pose, relative-pose, triangulation, and point-cloud alignment methods for the calibrated case. All problems can be solved for central or non-central cameras, and embedded into a random sample consensus or nonlinear optimization scheme. The library is relying on the adapter pattern, and thus may easily be included into other projects. It furthermore contains a Matlab interface and a full benchmark kit for testing and comparing algorithms against each other.\n\n\nOpenGV is released under the FreeBSD license.\n\n\nOpenGV web site: http://laurentkneip.github.io/opengv/\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModuleOpenGV module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleOpenGV\n\n\n\nSolARModulePCL\n\nThis module is based on PCL (Point Cloud Library), a C++ library designed to point cloud processing. The PCL framework contains numerous state-of-the art algorithms including filtering, feature estimation, surface reconstruction, registration, model fitting and segmentation. These algorithms can be used, for example, to filter outliers from noisy data, stitch 3D point clouds together, segment relevant parts of a scene, extract keypoints and compute descriptors to recognize objects in the world based on their geometric appearance, and create surfaces from point clouds and visualize them. It requires Flann (Fast Library for Approximate Nearest Neighbors).\n\n\nPCL is released under the terms of the BSD license and is open source software. It is free for commercial and research use.\n\n\nPCL web site: https://pointclouds.org\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModulePCL module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModulePCL\n\n\n\nSolARModuleRealSense\n\nThe SolAR Module RealSense is designed to use Intel® RealSense™ depth sensors throught its RealSense SDK. Intel® RealSense™ technologies offer a variety of vision‑based solutions designed to understand the world in 3D.\n\n\nRealSense SDK is open-source and licensed under the Apache v2 licence.\n\n\nRealSense SDK web site: https://dev.intelrealsense.com/docs\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModuleRealSense module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleRealSense\n\n\n\nSolARModuleTools\n\nThis module implements some generic features used in vision processing, such as 2D or 3D transformations, matches between keypoints, homography checking, keyframe searching, 3D points filtering, etc.\n\n\nSolARModuleTools is open-source, designed by b&lt;&gt;com, under Apache v2 licence.\n\n\nIts API is documented on the following link: http://www.solarframework.org/assemble/moduleapi/\n\n\nThe SolARModuleTools module is available on SolAR Git Hub: https://github.com/SolarFramework/SolARModuleTools\n\n\n\n\nSolAR Pipelines\n\nDefinition\n\nThe SolAR framework has been designed to simplify the creation of vision pipelines adressing Augmented Reality applications.\n\n\n\n\n\nFigure 4. Pipeline creation flow using SolAR Framework\n\n\nA SolAR vision pipeline is a chain of connected vision processing components generally executed in parallel. For instance, a SolAR vision pipeline may take as input images captured by one or more cameras as well as data from an inertial sensor. These data will be processed by a set of SolAR components to estimate the pose of an AR device.\n\n\nAs a vision pipeline assembler, a SolAR user can easily:\n\n\n\n\ndownload existing SolAR vision components created and published by the SolAR component creators\n\n\nassemble the vision components to create a vision pipeline\n\n\nupdate the vision pipeline architecture\n\n\nswap components by other ones\n\n\nconfigure components\n\n\ntest the vision pipeline\n\n\npublish it for AR pipeline users who will develop AR services\n\n\n\n\nTo do that, the SolAR framework handles everything that will allow a modular assembling of a vision pipeline:\n\n\n\n\nSolAR Data Structures define the information that flow in a pipeline and are exchanged between components\n\n\nSolAR Component Interfaces define for now more than 50 abstract interfaces for the different categories of vision processing components required to implement vision pipelines\n\n\nSolAR Components are concrete implementations of vision processing components compliant with SolAR components interfaces\n\n\nSolAR Modules are shared libraries embedding a set of SolAR components to ease their management and publication\n\n\nComponent management based on XPCF\n\n\n\n\nHere is a documentation on standalone C++ pipeline creation: http://www.solarframework.org/assemble/standalone_c_plus_plus/\n\n\n\nA simple pipeline example: Natural Image Marker\n\nThe SolAR Natural Image Marker sample shows a SolAR pipeline for augmented reality based on a natural image. This pipeline loads a reference image marker, then tries to detect it on real time camera images and to estimate the pose of the camera in relation to the coordinate system of the image marker. If the marker is detected, the pipeline draws a 3D cube on its position, renders the 3D cube from a virtual camera which pose corresponds to the one estimated by the pipeline, and displays the rendered 3D cube over the current camera image.\n\n\nHere is a screenshot example of the Natural Image Marker pipeline:\n\n\n\n\n\nFigure 5. Natural Image Marker pipeline example\n\n\nTo perform this vision pipeline, the Natural Image Marker project uses several SolAR modules and some of their components:\n\n\n\n\nSolARModuleOpenCV:\n\n\n\nSolARCameraOpencv\n\n\nSolARMarker2DNaturalImageOpencv\n\n\nSolARKeypointDetectorOpencv\n\n\nSolARKeypointDetectorRegionOpencv\n\n\nSolARDescriptorsExtractorAKAZE2Opencv\n\n\nSolARDescriptorMatcherKNNOpencv\n\n\nSolARGeometricMatchesFilterOpencv\n\n\nSolARPoseEstimationPlanarPointsOpencv\n\n\nSolAROpticalFlowPyrLKOpencv\n\n\nSolARProjectOpencv\n\n\nSolARUnprojectPlanarPointsOpencv\n\n\nSolAR2DOverlayOpencv\n\n\nSolARSideBySideOverlayOpencv\n\n\nSolAR3DOverlayOpencv\n\n\nSolARImageViewerOpencv\n\n\n\n\n\nSolARModuleTools\n\n\n\nSolARBasicMatchesFilter\n\n\nSolARKeypointsReIndexer\n\n\nSolARImage2WorldMapper4Marker2D\n\n\n\n\n\n\n\nTo manage SolAR modules and components needed to process this pipeline (definitions, interfaces, configurations, bindings, etc.), XPCF uses a dedicated XML file ('conf_NaturalImageMarker.xml' for this project).\nHere is an extract of this file:\n\n\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot; ?&gt;\n&lt;xpcf-registry autoAlias=&quot;true&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;\n         &lt;module uuid=&quot;15e1990b-86b2-445c-8194-0cbe80ede970&quot; name=&quot;SolARModuleOpenCV&quot; path=&quot;$REMAKEN_PKG_ROOT/packages/SolARBuild/win-cl-14.1/SolARModuleOpenCV/0.7.0/lib/x86_64/shared&quot; description=&quot;moduleOpenCV&quot;&gt;\n         &lt;component uuid=&quot;5B7396F4-A804-4F3C-A0EB-FB1D56042BB4&quot; name=&quot;SolARCameraOpencv&quot; description=&quot;SolARCameraOpencv&quot;&gt;\n                &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt; +\n                &lt;interface uuid=&quot;5DDC7DF0-8377-437F-9C81-3643F7676A5B&quot; name=&quot;ICamera&quot; description=&quot;ICamera&quot;/&gt;\n        &lt;/component&gt;\n                &lt;component uuid=&quot;efcdb590-c570-11e7-abc4-cec278b6b50a&quot; name=&quot;SolARMarker2DNaturalImageOpencv&quot; description=&quot;SolARMarker2DNaturalImageOpencv&quot;&gt;\n                &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt; +\n                &lt;interface uuid=&quot;3c9cee8a-e9ca-4c16-851a-669a94c2a68d&quot; name=&quot;IMarker&quot; description=&quot;IMarker&quot;/&gt; +\n                &lt;interface uuid=&quot;e9cdcf6e-c54c-11e7-abc4-cec278b6b50a&quot; name=&quot;IMarker2Dquared&quot; description=&quot;IMarker2Dquared&quot;/&gt; +\n                &lt;interface uuid=&quot;8fed06f8-c54d-11e7-abc4-cec278b6b50a&quot; name=&quot;IMarker2DNaturalImage&quot; description=&quot;IMarker2DNaturalImage&quot;/&gt;\n        &lt;/component&gt;\n...\n  &lt;/module&gt;\n        &lt;factory&gt;\n      &lt;bindings&gt;\n        &lt;bind interface=&quot;ICamera&quot; to=&quot;SolARCameraOpencv&quot; /&gt;\n      &lt;/bindings&gt;\n  &lt;/factory&gt;\n        &lt;properties&gt;\n                &lt;configure component=&quot;SolARCameraOpencv&quot;&gt;\n                        &lt;property name=&quot;calibrationFile&quot; type=&quot;string&quot; value=&quot;camera_calibration.yml&quot;/&gt;\n                        &lt;property name=&quot;deviceID&quot; type=&quot;UnsignedInteger&quot; value=&quot;0&quot;/&gt;\n                &lt;/configure&gt;\n                &lt;configure component=&quot;SolARMarker2DNaturalImageOpencv&quot;&gt;\n            &lt;property name=&quot;filePath&quot; type=&quot;string&quot; value=&quot;grafMarker.yml&quot;/&gt;\n        &lt;/configure&gt;\n                &lt;configure component=&quot;SolARKeypointDetectorOpencv&quot;&gt;\n                        &lt;property name=&quot;type&quot; type=&quot;string&quot; value=&quot;AKAZE2&quot;/&gt;\n                        &lt;property name=&quot;imageRatio&quot; type=&quot;Float&quot; value=&quot;0.5&quot;/&gt;\n                        &lt;property name=&quot;nbDescriptors&quot; type=&quot;Integer&quot; value=&quot;1000&quot;/&gt;\n        &lt;/configure&gt;\n...\n\n\n\nThis XML file is used to:\n\n\n\n\ndeclare the modules used by the pipeline and, for each module, the components required with their interfaces\n\n\nbind components to specific interfaces\n\n\ndefine properties of each component\n\n\n\n\nThe following diagram shows the chaining between the components for the pose estimation based on planar points:\n\n\n\n\n\nFigure 6. Pipeline for pose estimation based on planar points\n\n\nAnd here is the detailed sequence diagrams that shows, step by step, data exchange between the components, and the main features processed by each component:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7. Natural Image Marker pipeline sequence diagrams\n\n\nThese pipeline sample is available on GitHub: https://github.com/SolarFramework/Sample-NaturalImageMarker/tree/master/SolARSample_NaturalImageMarker_Mono\n\n\n\nA multithreaded implementation of Natural Image Marker pipeline\n\nFor AR applications, otimizing latency time is essential to deliver a better end-user experience. So, each pipeline involved in an AR application should optimize its processings to provide the resulting data in minimum time. To this end, the SolAR Framework makes possible to implement pipelines by paralleling the processing of components, when possible.\n\n\nThe multithreaded Natural Image Marker pipeline exemple provides the same feature as the pipeline detailed in the previous paragraph: it loads a reference image marker, then tries to detect it on real time camera images and to estimate the pose of the camera in relation to the coordinate system of the image marker.\n\n\nTo perform this vision pipeline, this project uses the same SolAR modules and components than those listed previously. So, the XML file used by XPCF to manage modules and components is exactly the same (\"conf_NaturalImageMarker_Multi.xml\").\n\n\nThe main difference between the multithreaded pipeline and the \"simple\" one (see paragraph A simple pipeline example: Natural Image Marker) lies in the use of components:\n\n\n\n\nIn the \"simple\" pipeline example, the components are called sequentially, the data produced by one component being used as input data for the next one (as you can see on this diagram).\n\n\nIn the multithreaded pipeline, the vision process is divided into several tasks that run in parallel and exchange the data they produce through shared buffers.\n\n\n\n\nThe tasks identified to process the Natural Image Marker pipeline are:\n\n\n\n\ncamera image capture: gets an image from the camera\n\n\nmarker detection: tries to detect the reference image (the natural image marker) in a camera image and to determine the camera pose\n\n\ndisplay: draws a 3D cube on marker position, renders the 3D cube from a virtual camera which pose corresponds to the one estimated by the pipeline, and displays the rendered 3D cube over the current camera image\n\n\n\n\nThese three tasks are started simultaneously and run in parallel for the duration of the pipeline process, by executing their processing in a loop. They use asynchronous shared buffers to get the data they need for their processing (from other tasks) and to make the data they produce available for other tasks.\n\n\nThe following diagram shows the chaining between the components inside each task, and the data exchanged asynchronously between tasks through buffers:\n\n\n\n\n\nFigure 8. Multithreaded implementation of Natural Image Marker pipeline\n\n\nTo implement the notions of task and shared buffer, SolAR pipelines use parallel helpers provided by XPCF:\n\n\n\n\nDelegateTask: task that executes a function in a loop, in its own thread\n\n\n\nexample of DelegateTask declaration:\n\n\nxpcf::DelegateTask taskCamImageCapture(camImageCapture);\n\n\n\n\nexample of loop function declaration:\n\n\nstd::function&lt;void(void)&gt; camImageCapture = [&amp;stop,camera,&amp;m_dropBufferCamImageForDetection, &amp;m_dropBufferCamImageForTracking](){ ...\n\n\n\n\nexample of starting a DelegateTask instance:\n\n\ntaskCamImageCapture.start();\n\n\n\n\nexample of stopping a DelegateTask instance:\n\n\ntaskCamImageCapture.stop();\n\n\n\n\n\n\n\nDropBuffer: asynchronous shared buffer which can contain a single object or a pair, a tuple of objects\n\n\n\nexamples of DropBuffer declaration:\n\n\nxpcf::DropBuffer&lt;SRef&lt;Image&gt;&gt; m_dropBufferCamImageForDetection;\nxpcf::DropBuffer&lt;std::tuple&lt;SRef&lt;Image&gt;, Transform3Df, bool&gt;&gt; m_dropBufferPoseForDisplay;\n\n\n\n\nexamples of data storage in a DropBuffer:\n\n\nm_dropBufferCamImageForDetection.push(camImage);\nm_dropBufferPoseForDisplay.push(std::make_tuple(camImage, Transform3Df::Identity(), false));\n\n\n\n\nexamples of data recovery from a DropBuffer:\n\n\nm_dropBufferCamImageForDetection.tryPop(camImage);\nm_dropBufferPoseForDisplay.tryPop(resultFromDetection);\n\n\n\n\n\n\n\n\n\nThese pipeline sample is available on GitHub: https://github.com/SolarFramework/Sample-NaturalImageMarker/tree/master/SolARSample_NaturalImageMarker_Multi\n\n\n\n\n\n\nSolAR Framework structure\n\n\nSolAR Framework Core\n\nData Structure\n\nThe SolAR Data Structure define the information that flows in a pipeline and are exchanged between components. These information are used to connect components.\n\n\nData Structure classes are designed to model all kinds of data needed in a vision pipeline, such as:\n\n\n\n\ncamera parameters\n\n\ngeneric descriptors, set of descriptors, match between 2 descriptors\n\n\n2D geometric structures: point, edge, rectangle, size\n\n\n3D geometric strutures: point, map of 3D points, cloud of 3D points\n\n\nframe, key frame\n\n\n2D image\n\n\netc.\n\n\n\n\nAll of the Data Structure classes are described on the SolAR web site: http://www.solarframework.org/create/api/\n\n\nThe Data Structure code is available on the SolAR GitHub repository: https://github.com/SolarFramework/SolARFramework/tree/master/interfaces/datastructure\n\n\n\nFramework API\n\nThe SolAR Framework API has been designed to avoid interoperability issues between components. For this purpose, any component has to implement this API, which can be considered as the SolAR Component Interface.\n\n\nThis API is split into several parts:\n\n\n\n\n'display': to draw 2D information, 3D contents, to display point clouds, images, etc.\n\n\n'features':  offers different functions related to visual features (e.g. points or lines) such as detection of keypoints from an image or a region of an image, extraction of descriptors from a set of keypoints, matches between sets of descriptors, extraction of contours from an image, a squared binary pattern or an image of squared binary pattern, and so on&#8230;&#8203;\n\n\n'fusion': to fuse different kinds of data such as inertial and visual data\n\n\n'geom': a complete set of geometrical functions such as 2D or 3D transformations of set of points, projection of 3D points on 2D image plane, undistorsion applying to set of points, etc.\n\n\n'image': functions dedicated to images (conversion, filtering, loading, warping and cropping&#8230;&#8203;)\n\n\n'input': to access devices (camera calibration and images capture, data of IMU) or files (to load different kinds of markers)\n\n\n'loop': to detect a loop closure from a given keyframe, to optimize a system of 3D points and keyframes from a loop closing detection\n\n\n'pipeline': to define a global pose estimation pipeline\n\n\n'pointCloud': provides several filters to prune a point cloud\n\n\n'reloc': to retrieve a set of pre-recorded keyframes close to a given frame, or to get a camera pose giving a frame\n\n\n'sink': to store a synchronized pose and image from the pipeline to make it available to a third party application, and to update a texture buffer with a new image\n\n\n'slam': functions dedicated to SLAM processing (SLAM initialization using image stream of a camera, mapping, tracking)\n\n\n'solver': functions for modeling and solving large, complicated optimization problems, whether related to a 3D map (e.g. 3D map filtering, 3D mapping and triangulation), to camera pose (e.g. pose estimation from 2D-2D, 2D-3D or 3D-3D correspondences or from a marker), or to both (e.g. bundle adjustment)\n\n\n'source': to set a new image coming from a third party\n\n\n'storage': to store the covisibility graph between keyframes, a set of keyframes or a point cloud (to share persistent data between processing components)\n\n\n'tracking': to estimate the optical flow between two images\n\n\n\n\nThis API is detailed on the SolAR web site: http://www.solarframework.org/create/api/\n\n\nThe API code is available on the SolAR GitHub repository: https://github.com/SolarFramework/SolARFramework/tree/master/interfaces/api\n\n\n\nSolAR Pipeline Manager\n\nThis part of the SolAR Framework is currently used to run SolAR pipelines in the Unity environment. But more generally, its goal is to manage pipelines in different environments, Unity being one of them. It does not offer an access to the whole SolAR API, but just to the API allowing to load and initialize an existing pipeline implementing the IPipeline interface, to start and stop it, to optionally feed it with images, and to get access to the pose of the camera estimated by the pipeline.\n\n\nAs Unity only supports the C# language, the Pipeline Manager must first use the SolAR Wrapper (see the next paragraph) to wrap the C++ code of a pipeline in C#. Then, this manager will take the right interfaces to make this pipeline compatible with the Unity environment.\n\n\nThe Pipeline Manager code is available on the SolAR GitHub repository: https://github.com/SolarFramework/SolARPipelineManager\n\n\n\nSolAR Wrapper\n\nThis wrapper is made of several SWIG scripts which allow to use the whole SolAR API in languages other than C++, such as C# for Unity environment. But we can imagine other languages like Java, Python&#8230;&#8203;\n\n\nThe Simplified Wrapper and Interface Generator (SWIG) is an open-source software tool used to connect computer programs or libraries written in C or C++ with scripting languages such as Lua, Perl, PHP, Python, R, Ruby, Tcl, and other languages like C#, Java, JavaScript, Go, D, OCaml, Octave, Scilab and Scheme. Output can also be in the form of XML.\n\n\nThe export of SolAR pipelines to Unity is described here: http://www.solarframework.org/assemble/unity_pipeline/#export_your_pipeline_for_unity\n\n\nThe SolAR Wrapper project is available on the SolAR Git Hub repository: https://github.com/SolarFramework/SwigWrapper\n\n\n\n\nThird-party dependencies\n\nTo process data transformations, geometric operations, component, module and pipeline management, and so on, the SolAR Framework needs some external tools or libraries described below.\n\n\nXPCF\n\nThe Cross-Platform Component Framework (XPCF) is a lightweight framework designed to provide dependency injection, with clarity, simplicity and light code.\n\n\nThis framework offers several major features:\n\n\n\n\nan \"in-code\" dependency injection factory to create components and bind them to a specific interface\n\n\na component abstract factory to load components at runtime from modules (named ModuleManager)\n\n\nincrease modularity with better separation of concerns through interface discovery (using component introspection)\n\n\na centralized description of modules, components and interfaces (with a dedicated configuration file)\n\n\nmodule introspection to figure out the components contained and the interfaces they implement\n\n\nsecure memory management using smart pointers for components (prevents from memory leaks, dangling null pointer)\n\n\ncomponent and service configuration at creation through abstract variant parameters\n\n\nsupport for both components and services\n\n\nsupport for Domain Specific Language through the \"IFeatures\" interface (for instance, a component can declare \"opencv\" and \"cuda\" features as it is an opencv/cuda accelerated implementation of a keypointdetector)\n\n\n\n\nXPCF defines some important concepts that will be used in the SolAR Framework, such as:\n\n\n\n\nModule: a shared library hosting a set of factories to create components/services instances\n\n\nComponent: a class that can have several instances over the application lifetime, each instance is unloaded (destroyed) when no more reference exists upon one of its interfaces\n\n\nService: a component singleton that only has one instance at any given time, and is never unloaded\n\n\n\n\nThe component, module and pipeline management of the SolAR Framework is based on XPCF:\n\n\n\n\nit provides an \"in-code\" dependency injection factory to create concrete component instances and bind them to abstract SolAR component interfaces\n\n\nit handles the loading of modules and components at runtime\n\n\nit provides interfaces to load pipelines, used modules and components at run-time, etc.\n\n\n\n\nAll these mechanisms are described in the chapters dedicated to SolAR Components, Modules and Pipelines.\n\n\n\n\n\nFigure 9. XPCF: components, modules and pipelines management\n\n\nXPCF is an open-source framework designed by b&lt;&gt;com, under Apache v2 licence.\n\n\nXPCF presentation and code are available on this Git Hub repository: https://github.com/b-com-software-basis/xpcf\n\n\n\nBoost\n\nBoost is a set of libraries for the C++ programming language that provides support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, serialization and unit testing.\n\n\nMost of the Boost libraries are licensed under the Boost Software License, designed to allow Boost to be used with both free and proprietary software projects. Many of Boost&#8217;s founders are on the C++ standards committee, and several Boost libraries have been accepted for incorporation into the C\\+ Technical Report 1, the C+11 standard (e.g. smart pointers, thread, regex, random, ratio, tuple) and the C++17 standard (e.g. filesystem, any, optional, variant, string_view).\n\n\nBoost web site: https://www.boost.org/\n\n\n\nspdlog\n\nspdlog is a C++ logging library. Its main features are:\n\n\n\n\nVery fast – performance is the primary goal\n\n\nHeaders only\n\n\nNo dependencies\n\n\nCross platform - Linux / Windows on 32/64 bits\n\n\nMulti/Single threaded loggers\n\n\nRotating log files\n\n\nand more&#8230;&#8203;\n\n\n\n\nspdlog is licensed under the MIT license.\n\n\nspdlog GitHub repository: https://github.com/gabime/spdlog\n\n\n\nEigen\n\nEigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\n\n\nEigen is Free Software. Starting from the 3.1.1 version, it is licensed under the MPL2, which is a simple weak copyleft license.\n\n\nEigen web site: http://eigen.tuxfamily.org\n\n\n\n\n\n\nManagement of dependencies for SolAR projects\n\n\nInstallation of packages\n\nThe SolAR framework is using the meta dependencies management tool Remaken supporting Conan, vcpkg and its native C++ packaging structure based on pkg-config description files.\n\n\nRemaken relies on other tools and package managers, as shown on this diagram:\n\n\n\n\n\nFigure 10. SolAR Framework dependency management tools\n\n\nRemaken\n\nRemaken is a meta dependencies management tool which supports different package formats.\n\n\nCross platforms packaging systems:\n\n\n\n\nConan\n\n\nvcpkg\n\n\nRemaken (pkg-config based)\n\n\n\n\nDedicated system tools:\n\n\n\n\napt-get (debian and derivatives)\n\n\npacman (archlinux)\n\n\nyum (redhat, fedora, centos)\n\n\nzypper (openSUSE)\n\n\npkg (freebsd)\n\n\npkgutil (solaris)\n\n\nHomebrew (mac OS X)\n\n\nchocolatey (windows)\n\n\n\n\nRemaken uses a packagedependencies.txt file defining all dependencies of a project. Thus, by maintaining a simple dependency file describing the third parties used by a project, it will be easier to download the dependencies, link and build this project, and install and deploy the solution with all its dependencies in a very simple way.\n\n\nFor each project, a packagedependencies.txt file can be created in the root project folder. Each line of this file must follow the pattern:\n\n\n\nframework#channel | version | [condition]#library name | identifier@repository_type | repository_url | link_mode | options\n\n\n\nFor example:\n\n\n\nopencv | 3.4.3 | opencv | thirdParties@github | https://github.com/SolarFramework/binaries/releases/download\nxpcf | 2.1.0 | xpcf | bcomBuild@github | https://github.com/SolarFramework/binaries/releases/download | static\nspdlog | 0.14.0 | spdlog | bincrafters@conan | conan-center | na\nfreeglut#testing | 3.0.0 | freeglut | user@conan | https://github.com/SolarFramework/binaries/releases/download\n\n\n\ngithub, artifactory, nexus and path dependencies are installed using remaken packaging format through an url or filesystem repository.\n\n\nSystem dependencies are installed using operating system dependent package manager (apt for linux debian and derivatives, Homebrew for Mac OS X, chocolatey for windows&#8230;&#8203;).\n\n\nConan dependencies are installed using packaging format with Conan package manager.\n\n\nvcpkg dependencies are installed using vcpkg packaging format with vcpkg package manager.\n\n\nRemaken is an open-source framework designed by b&lt;&gt;com, under Apache v2 licence.\n\n\nThe Remaken tool and its documentation are available on Git Hub repository: https://github.com/b-com-software-basis/remaken\n\n\n\npkg-config\n\npkg-config is a tool that defines and supports a unified interface for querying installed libraries for the purpose of compiling software that depends on them. It allows programmers and installation scripts to work without explicit knowledge of detailed library path information.\n\n\nThe primary use of pkg-config is to provide the necessary details for compiling and linking a program to a library. This metadata is stored in pkg-config files. These files have the suffix '.pc' and reside in specific locations known to the pkg-config tool.\n\n\nThe '.pc' file format contains predefined metadata keywords and freeform variables. For example, here is the SolAR Framework configuration file:\n\n\n\nlibname=SolARFramework\nprefix=/usr/local\nexec_prefix=${prefix}\nlibdir=${exec_prefix}/lib\nincludedir=${prefix}/interfaces\nName: SolARFramework\nDescription:\nVersion: 0.7.0\nRequires:\nLibs: -L${libdir} -l${libname}\nLibs.private: ${libdir}/${pfx}${libname}.${lext}\nCflags: -I${includedir}\n\n\n\nHere is a short description of the keyword fields:\n\n\n\n\nName: A human-readable name for the library or package. This does not affect usage of the pkg-config tool, which uses the name of the .pc file.\n\n\nDescription: A brief description of the package.\n\n\nURL: An URL where people can get more information about and download the package.\n\n\nVersion: A string specifically defining the version of the package.\n\n\nRequires: A list of packages required by this package. The versions of these packages may be specified using the comparison operators '=', '&lt;', '&gt;', '&#8656;' or '&gt;='.\n\n\nRequires.private: A list of private packages required by this package but not exposed to applications. The version specific rules from the 'Requires' field also apply here.\n\n\nConflicts: An optional field describing packages that this one conflicts with. The version specific rules from the 'Requires' field also apply here. This field also takes multiple instances of the same package. E.g., 'Conflicts: bar &lt; 1.2.3, bar &gt;= 1.3.0'.\n\n\nCflags: The compiler flags specific to this package and any required libraries that don&#8217;t support pkg-config. If the required libraries support pkg-config, they should be added to 'Requires' or 'Requires.private'.\n\n\nLibs: The link flags specific to this package and any required libraries that don&#8217;t support pkg-config. The same rule as 'Cflag&#8217;s applies here.\n\n\nLibs.private: The link flags for private libraries required by this package but not exposed to applications. The same rule as 'Cflags' applies here.\n\n\n\n\npkg-config is licenced under GNU GPL.\n\n\npkg-config web site: https://www.freedesktop.org/wiki/Software/pkg-config/\n\n\n\nConan\n\nConan is a portable package manager, intended for C and C++ developers, but it is able to manage builds from source, dependencies, and precompiled binaries for any language.\n\n\nConan is a decentralized package manager with a client-server architecture. This means that clients can fetch packages from, as well as upload packages to, different servers (“remotes”), similar to the “git” push-pull model to/from git remotes.\n\n\n\n\n\nFigure 11. Generic Conan mechanism\n\n\nOne of the most powerful features of Conan is that it can create and manage pre-compiled binaries for any possible platform and configuration. Using pre-compiled binaries and avoiding repeatedly building from source, save a lot of time to developers and Continuous Integration servers, while also improving the reproducibility and traceability of artifacts.\n\n\nInstallation of packages from servers is also very efficient. Only the necessary binaries for the current platform and configuration are downloaded, not all of them. If the compatible binary is not available, the package will be built from sources in the client. This package, built for a specific configuration, can then be uploaded on the server to make it available to other developers.\n\n\nConan works with any build system too. There are built-in integrations with most popular ones, like CMake, Visual Studio (MSBuild), Autotools and Makefiles, SCons, etc. But it is not a requirement to use any of them. It is not even necessary that all packages use the same build system, every package can use their own build system, and depend on other packages using different build systems. It is also possible to integrate with any build system, including proprietary ones.\n\n\nSolAR has its own Conan remote server to store recipes corresponding to binaries related to SolAR third parties and modules. Thus, binaries used by SolAR on the recommended platforms can be directly downloaded without the need to build them on the client. The “conan-solar” remote server is accessible at the following URL: https://artifact.b-com.com/api/conan/solar-conan-local\n\n\nConan web site: https://conan.io/\n\n\nConan is Free and Open Source, with a permissive MIT license.\n\n\n\nVcpkg\n\nvcpkg is a cross-platform open source package manager by 'Microsoft', for 'Windows', 'Linux' and 'MacOS'. It provides access to C and C++ libraries to its supported platforms.\n\n\nVcpkg is licensed under MIT license.\n\n\nVcpkg is available here: https://github.com/microsoft/vcpkg\n\n\n\nOther package managers\n\nA package manager is a software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer&#8217;s operating system in a consistent manner.\n\n\nA package manager deals with packages, distributions of software and data in archive files. Packages contain metadata, such as the software&#8217;s name, description of its purpose, version number, vendor, checksum (preferably a cryptographic hash function), and a list of dependencies necessary for the software to run properly. Upon installation, metadata is stored in a local package database. Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. They work closely with software repositories, binary repository managers, and app stores.\n\n\nPackage managers are designed to eliminate the need for manual installs and updates.\n\n\nPackage managers currently used by the SolAR Frame work are:\n- Chocolatey (for 'Windows'): https://chocolatey.org/\n- Homebrew (for 'MacOS'): https://brew.sh/\n- APT (for 'Linux' distributions such as 'Debian' or 'Ubuntu'): https://en.wikipedia.org/wiki/APT_(software)\n- yum(for 'Linux' distributions such as 'Fedora', 'CentOS', 'Red Hat'): http://yum.baseurl.org/\n\n\n\n\nBuild rules management\n\nTo manage libraries dependencies during the build process, SolAR projects use builddefs-qmake: a set of cross-platform rules based on qmake used to provide homogeneous C++ builds, ensuring build rules for shared libraries and executables are the same along the builds. It supports either dynamic or static libraries builds.\n\n\nThese rules are defined in a set of qmake project include files (\".pri\" files), and are used for different purposes. These rules can be installed by remaken by running \"remaken init\" command (with option \"–tag\" if a specific version of the rules is expected).\n\n\nFirst, the rules can set the include and link flags based on the packagedependencies.txt file for static (DEPENDENCIESCONFIG = static) and dynamic (DEPENDENCIESCONFIG = shared)  dependencies. It also supports recursive search of dependencies (DEPENDENCIESCONFIG = recursive). This feature avoids the need for the developer to maintain the flags in the project file.\n\n\nSecondly, it can handle the installation in a dedicated folder (INSTALLSUBDIR) of the resulting binaries (DEPENDENCIESCONFIG = install), and can also install all its dependencies in a recursive way (DEPENDENCIESCONFIG = install_recurse). The dependencies ignored during installation can be defined in a dedicated file (packageignoreinstall.txt).\n\n\nFinally, if the dependencies are installed thanks to Conan, the rules will automatically download them to your machine, and even build them if the binaries are not available on the Conan remote server for the requested configuration.\n\n\nAlso, a specific flag called QTVS allows the loading of the qmake projects in Visual Studio with the plugin QT Visual Studio Tools.\n\n\nTo sum up, here is the global sequence diagram of SolAR dependency management:\n\n\n\n\n\nFigure 12. SolAR Framework dependency management\n\n\n\n\n\nUnity Plugin\n\n\nTo assemble C++ pipelines for Unity, SolAR Framework provides a dedicated plugin. The SolAR plugin for Unity is a simple interface to load and run in Unity environment any SolAR pipelines and use them to design AR services with Unity. Integration of SolAR pipelines in Unity Engine gives opportunities to developers of AR services to get access to fully configurable SolAR based solutions to design AR applications (for Windows, Linux and Android).\n\n\nWhile SolAR is dedicated to computer vision pipelines, for now, the pipeline interface is only dedicated to pose estimation pipelines. New pipeline interfaces will be soon added for other vision tasks such as 3D reconstruction, object recognition, etc.\n\n\nSolAR uses SolAR Pipeline Manager and SolAR Wrapper to manage the Unity plugin.\n\n\nIn that way, pipelines should first be created as C++ SolAR pipelines, and then be exported for Unity.\n\n\nA C++ SolAR pipeline is a shared library embedding an implementation of the IPipeline interface defined by the SolAR Framework. Thus, this pipeline can be loaded at run-time and ran by any third party application such as Unity.\n\n\nAny SolAR pipeline must implement IPipeline class, which means that six methods must be implemented:\n\n\n\n\ninit: Initializes the pipeline by providing a reference to the component manager loaded by the PipelineManager. You will have to instantiate all the components used by your pipeline and initialize them.\n\n\ngetCameraParameters: Provides the camera parameters.\n\n\nstart: Starts the pipeline with a texture buffer that will be updated when a new frame will be processed and ready for display.\n\n\nstop: Stops the pipeline.\n\n\nupdate: A methode that provides the new pose of the camera.\n\n\nloadSourceImage: If there is no camera in your pipeline, you can feed it with an external image.\n\n\n\n\nOnce the C++ SolAR pipeline is built, you can import the DLL in Unity, select it in the SolARPipelineLoader object and run Unity Editor. Pipeline parameters are easily editable from Unity Editor.\n\n\n\n\n\nFigure 13. SolAR Unity Editor\n\n\nThe steps to use pipelines for Unity are described on SolAR web site: http://www.solarframework.org/use/unity/\n\n\nThe steps to assemble pipelines for Unity are described on SolAR web site: http://www.solarframework.org/assemble/unity_pipeline/\n\n\n\n\nGlossary\n\n\n\nb&lt;&gt;com\n\nTechnology Research Institute focused on innovation in areas like Artificial Intelligence, immersive video and audio, content protection, 5G networks, Internet of Things, cognitive technologies\n\nAPI\n\nApplication Programming Interface\n\nAR\n\nAugmented Reality\n\nBA\n\nBundle Adjustment\n\nDLL\n\nDynamic Link Library\n\nGPU\n\nGraphics Processing Unit\n\nGUI\n\nGraphical User Interface\n\nIT\n\nInformation Technology\n\nPaaS\n\nPlatform as a Service\n\nSDK\n\nSoftware Development Kit\n\nSLAM\n\nSimultaneous Localization And Mapping\n\nSolAR\n\nSolution for Augmented Reality\n\nSWIG\n\nSimplified Wrapper and Interface Generator\n\nUUID\n\nUniversally Unique IDentifier\n\nXML\n\nExtensible Markup Language\n\nXPCF\n\nCross-Platform Component Framework\n\n\n\n\n",
      "id": 10
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "get modules",
      "content": "\nTable of Contents\n\nGet Modules\nGet Modules\n\nInstall\n\n\n\n\n\nGet Modules\n\n\n\n\n\nGet Modules\n\n\nInstall\n\nAll module binaries and source code can be downloaded from GitHub (for more information on the installation of the framework, take a look on this install page):\n\n\n\n\n\n\n\n\n\n\nGithub Repository\nbinaries\n\n\nFramework\nSolARframework\nhttps://github.com/SolarFramework/SolARFramework/releases\n\n\nModules\nSolARModuleCeres\nhttps://github.com/SolarFramework/SolARModuleCeres/releases\n\n\nSolARModuleFBOW\nhttps://github.com/SolarFramework/SolARModuleFBOW/releases\n\n\nSolARModuleG2O\nhttps://github.com/SolarFramework/SolARModuleG2O/releases\n\n\nSolARModuleNonFreeOpenCV\nhttps://github.com/SolarFramework/SolARModuleNonFreeOpenCV/releases\n\n\nSolARModuleOpenCV\nhttps://github.com/SolarFramework/SolARModuleOpenCV/releases\n\n\nSolARModuleOpenGL\nhttps://github.com/SolarFramework/SolARModuleOpenGL/releases\n\n\nSolARModuleOpenGV\nhttps://github.com/SolarFramework/SolARModuleOpenGV/releases\n\n\nSolARModulePCL\nhttps://github.com/SolarFramework/SolARModulePCL/releases\n\n\nSolARModuleRealSense\nhttps://github.com/SolarFramework/SolARModuleRealSense/releases\n\n\nSolARModuleTools\nhttps://github.com/SolarFramework/SolARModuleTools/releases\n\n\nSolARModulePopSift\nhttps://github.com/SolarFramework/SolARModulePopSift/releases\n\n\n\n\n\n",
      "id": 11
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "get Samples",
      "content": "\nTable of Contents\n\nGet samples\nGet samples\n\nInstall\n\n\n\n\n\nGet samples\n\n\n\n\n\nGet samples\n\n\nInstall\n\nAll sample binaries and source code can be downloaded from GitHub using the following links.\n\n\n\n\n\n\n\n\nGithub Repository\nbinaries\n\n\nNatural Image Marker\nhttps://github.com/SolarFramework/NaturalImageMarker/releases\n\n\nFiducial Marker\nhttps://github.com/SolarFramework/FiducialMarker/releases\n\n\nSLAM\nhttps://github.com/SolarFramework/Sample-Slam/releases\n\n\nTriangulation\nhttps://github.com/SolarFramework/Sample-Triangulation/releases\n\n\nMapping\nhttps://github.com/SolarFramework/Sample-Mapping/releases\n\n\nRelocalization\nhttps://github.com/SolarFramework/Sample-Relocalization/releases\n\n\nMap Update\nhttps://github.com/SolarFramework/Sample-MapUpdate/releases\n\n\n\n\n\n",
      "id": 12
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Install",
      "content": "\nTable of Contents\n\nInstall SolAR\nSupported platform\n\n\n\nInstall SolAR\n\n\n\n\n\nSupported platform\n\n\n\n\n\n\n\nWindows\n\n\n\n\n\nLinux\n\n\n\n\n\nAndroid\n\n\n\n\n",
      "id": 13
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Community",
      "content": "\nTable of Contents\n\nCommunity\nCommunity\nContact\n\n\n\nCommunity\n\n\n\n\n\nCommunity\n\n\nThere are several ways to contribute to SolAR framework.\n\n\nYou can :\n\n\n\n\ncontribute to Core framework, by defining API, architecture, framework tools &#8230;&#8203; this typically needs software engineering and architecture skills.\n\n\ncontribute to Components, by creating a new SolAR component, that can be used in a pose estimation solution&#8230;&#8203; this typically needs computer vision skills (and software skills).\n\n\n\n\nPlease contribute!\nRefer to our contribution workflow section\n\n\n\n\nContact\n\n\nFor any request, please contact us at framework.solar@b-com.com.\n\n\n",
      "id": 14
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Create",
      "content": "\nTable of Contents\n\nCreate\nWhy create SolAR components ?\nWhat is a SolAR component ?\nwhat is a SolAR module ?\n\n\n\nCreate\n\n\n\n\n\nWhy create SolAR components ?\n\n\nThe SolAR framework has been designed to easily assemble a set of SolAR vision components to build vision pipelines adressing AR services. To ensure the interoperability, a concrete vision component must implement one of the interfaces among more than 50 currently defined in the SolAR framework. Thus, a SolAR pipeline assembler can easily choose and connect SolAR vision components together to assemble a vision pipeline, and he can easily swap one component by another one to improve or adapt the pipeline to a dedicated device or task. This modularity aims to create an ecosystem where researchers and vision experts will be able to easily share, promote or make their research results accessible to a large community to help the adoption of augmented reality.\n\n\n\n\nWhat is a SolAR component ?\n\n\nEach component processes input data and streams the processing results to feed the input of other components. Thus, each processing can be performed in parallel by buffering data between components to optimize the global vision pipeline. Components exchange data with strong types defined by the data structures available in the framework to ensure strong interfaces and avoid the connection of components with incompatible data.\n\n\nAlso, each concrete implementation of a SolAR component can define its own parameters to provide configuration genericity. This configuration is very simple with SolAR and allows to configure the pipeline at load time thanks to a simple xml file, and even at runtime through the SolAR API.\n\n\n\n\nwhat is a SolAR module ?\n\n\nAs it is difficult to manage tens, hundreds or even thousands of components independently (i.e. one shared library per component), SolAR components must be grouped into SolAR modules (i.e. a unique shared library embedded a set of concrete implementation of SolAR components). Thus, it will be much easier to manage the publication of modules which can be organized according to the creator&#8217;s choice (a module dedicated to a third party such as OpenCV, to a research institution, to a company, to a dedicated pipeline assembly, etc.).\n\n\n",
      "id": 15
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Use",
      "content": "Use\n\nTable of Contents\n\nUse\nOverview\nHow to contribute\n\n\n\nUse\n\n\n\n\n\nOverview\n\n\nSolAR is an open-source framework designed to solve computer vision problems.\n\n\nThis part of the website is to help developers be successful with our framework and create AR applications.\n\n\n\n\n\nFor the moment :\n\n\n\nUnity3D engine support\n\n\nC++ native support\n\n\nAndroid platform support.\n\n\n\n\n\nIn the future :\n\n\n\niOS platform support\n\n\n\n\n\n\n\n\n\nHow to contribute\n\n\nPlease refer to our contribution workflow section\n\n\n",
      "id": 16
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Documentation",
      "content": "\nTable of Contents\n\nDocumentation\nSolAR Framework Architecture\nARCloud Platform\nARCloud Services\n\n\n\nDocumentation\n\n\n\n\n\nSolAR Framework Architecture\n\n\nYou will find here all the architecture documentation for the SolAR Framework.\n\n\n\n\nARCloud Platform\n\n\nYou will find here an evolution of the SolAR Framework which offers a complete solution for the deployment, maintenance and upgrade of SolAR services in the Cloud.\n\n\n\n\nARCloud Services\n\n\nYou will find here the presentation of the ARCloud Services: a set of computer vision pipelines dedicated to Augmented Reality (AR) applications and ready to be deployed on a Cloud architecture.\n\n\n",
      "id": 17
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Assemble",
      "content": "\nTable of Contents\n\nAssemble\nWhat is a SolAR vision pipeline ?\nWhy use SolAR to assemble vision pipelines for AR ?\nHow it works ?\n\n\n\nAssemble\n\n\n\n\n\nWhat is a SolAR vision pipeline ?\n\n\nThe SolAR framework has been designed to simplify the design of vision pipelines adressing augmented reality applications. A SolAR vision pipeline is a chain of connected vision processing components generally executed in parallel. For instance, a SolAR vision pipeline may take as input images captured by one or more cameras as well as data from an inertial sensor. These data will be processed in series by a set of SolAR components to estimate the pose of an AR device.\n\n\n\n\nWhy use SolAR to assemble vision pipelines for AR ?\n\n\nThe SolAR framework has been designed from the beginning to clearly separate tasks and roles and offer modularity. Indeed, a vision pipeline assembler will be able to focus on the pipeline design without worrying about the implementation of components, the intergation of existing computer vision libraries or the integration of the pipeline in an AR service.\n\n\nA vision pipeline assembler can easily:\n\n\n\n\ndownload existing SolAR vision components created and published by the SolAR component creators,\n\n\nassemble the vision components to create a vision pipeline,\n\n\nupdate the vision pipeline architecture,\n\n\nswap components by other ones,\n\n\nconfigure components,\n\n\ntest the vision pipeline,\n\n\npublish it for AR pipeline users who will develop AR services.\n\n\n\n\n\n\nHow it works ?\n\n\nThe SolAR framework handles everything that will allow a modular assembling of a vision pipeline:\n\n\n\n\nSolAR Data Structures define the information that flows in a pipeline and are exchanged between components.\n\n\nSolAR Component Interfaces define for now more than 50 standardized abstract interfaces for the different categories of vision processing components required to implement vision pipelines (e.g. keypoint detector, descriptor extractor, features matching, PnP and optimization). This unified interface is required to ensure interoperability between components allowing to easily swap one with another to improve the final vision pipeline.\n\n\nSolAR Components are concrete implementations of vision processing components compliant with SolAR components interfaces. Several components can implement the same SolAR component interface. In general, these components are created by wrapping computer vision libraries that could be either open-source (OpenCV, PCL, ROS, etc.) or proprietary. Each component can define its own parameters that will be used to fine tune the vision pipeline.\n\n\nSolAR Modules are shared libraries embedding a set of SolAR components to ease their management and publication.\n\n\nComponent Manager based on XPCF, a lightweight cross platform component framework similar to the well known COM model. it manages the introspection of SolAR modules and components, it provides an \"in-code\" dependency injection factory to create concrete component instance and bind them to a abstract SolAR component interface, it handles the loading of modules and components at runtime, it provides interfaces for load of pipeline and used modules and components at run-time, etc. XPCF is a third party developped by b&lt;&gt;com under Apache licencse 2.0 (more information about XPCF is available on GitHub).\n\n\n\n\n",
      "id": 18
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Home",
      "content": "\n  \n    \n      \n    \n  \n\n\n\n  \n    What is SolAR?\n    SolAR is an open-source framework under Apache v2 license dedicated to Augmented Reality. \n    \n    SolAR is modular and evolutive. It allows, in various contexts of use, the construction of customized computer vision pipelines targeting Augmented Reality applications (e.g. camera pose estimation or 3D mapping).\n    \n    SolAR contains:\n      \n        \n           interfaces promoting interoperability\n           computer vision components constructed from third-party bricks\n           plugins for third-party applications\n        \n      \n    \n  \n  \n    What's new in SolAR framework?  (Release notes)\n    v0.11.0 (2022/11/15) \n      \n        \n           Support of QR Code for relocalization \n           Support of PNG and JPEG decoding \n           New Front End service (for cloud deployment) \n           New Relocalization Markers pipeline and service \n           Optimization of Map Update, Mapping and Relocalization pipelines \n           CUDA versions of Map Update, Mapping and Relocalization services \n           Bug fixes, optimizations \n        \n      \n    v0.10.0 (2021/12/30) \n      \n        \n           Stereo camera components (e.g. calibration, rectification, depth estimation, reprojection to 3D) \n           Map update pipeline\n           SLAM and mapping optimizations\n           gRPC module: stubs for client and server sides, for all SolAR interfaces, to create remote services\n           Map Update, Relocalization and Mapping services (for cloud deployment) and client test applications\n           Add module PopSift for CUDA optimized SIFT feature detection and extraction.\n        \n      \n  \n\n\n\n  SolAR framework Overview      \n    \n      An overview of our SolAR framework, an open-source solution dedicated to build Augmented Reality applications that offers cloud services.\n    \n    \n      \n  \n\n\n\n    SolAR ARCloud Services\n  \n    \n      First step - map initialization\n      This video shows the map creation process, using the 'SolaR ARCloud MapInit Service'. \n      Wearing a Hololens 2 headset, an operator is able to create the first part of the future global map. A fiducial marker is used only once, during this first initialization of the map, to define the 'World reference point' of the map.\n    \n  \n  \n    \n  \n\n\n  \n    \n      Second step - map extension \n      In this video, an operator launches the 'SolaR ARCloud MapExtend Service' from an already mapped area and can explore a new area to extend the existing map. \n      Once the MapExtend Service has located the user in a previously mapped area, a corresponding portion of the global map  is sent to the device.  Then, the service is able to extend the extracted map according to the user's movement. The merging of this new extended map with the global map is done later in the background.\n    \n  \n  \n    \n  \n\n\n\n  \n    \n      Relocalization service \n      This video presents the positioning service 'SolAR ARCloud Reloc Service' which is a service to quickly locate the user on the global map and track his movements.\n    \n  \n  \n    \n  \n\n\n\n  \n    Who needs SolAR?\n    According to your needs, SolAR framework offers several user profiles:\n  \n\n\n  \n    \n      \n        \n            \n            \n                \n          \n        \n      \n      \n        SolAR pipeline user\n      \n      \n          As an AR service designer, I can use a prebuild standalone or Unity integrated AR pipeline...\n      \n    \n  \n  \n    \n      \n        \n          \n              \n              \n          \n        \n      \n      \n        SolAR pipeline assembler\n      \n      \n          As an AR pipeline developer, I can assemble components to build my own customized AR pipeline...\n      \n    \n  \n  \n  \n    \n      \n        \n          \n          \n        \n      \n    \n    \n      SolAR component developer\n    \n    \n        As an SolAR component developer, I can create customized bricks responding to specific needs...\n    \n    \n  \n\n\n  \n    How to contribute to SolAR?\n    Join the community, find out how you can contribute to the framework according to your profile and skills and participate to the augmented reality adoption. \n    \n    \n      \n    \n      \n        \n          \n              \n                    \n          \n        \n      \n      \n        SolAR framework contributor\n      \n      \n          As an SolAR framework developer, I can contribute to SolAR expansion...\n      \n    \n  \n\n\n",
      "id": 19
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "How to install",
      "content": "\nTable of Contents\n\nInstall\nHow to install\nNext Steps\n\n\n\nInstall\n\n\n\n\n\nHow to install\n\n\nPrior to start, be sure that SolAR and your working environment are correctly configured as explained in the following :\n\n\nUnresolved directive in installassemble_back.adoc - include::../create/_install.adoc[]\n\n\n\n\nNext Steps\n\n\nIn the context of SolAR, a pipeline is a set of connected actions that takes on one hand a camera frame.\nActions are performed via C++ methods with dedicated input/output parameters. Generally output parameters of one action serve as input parameters for subsequent actions, hence the term pipeline.\n\n\nThe SolAR framework provides a comprehensive list of usable methods that are located in the API directory.\n\n\nAlso SolAR provides a set of modules that implement the said methods.\n\n\nTo build a pipeline, it is necessary to identify what are the required actions and which modules/components are available to perform them.\n\n\nSection \"Get Modules\" explains this in detail.\n\n\nOnce it is done, due to the use of XPCF, the three main items to take care of are :\n\n\n\n\na configuration file\n\n\na dependencies description file\n\n\na build configuration file (.pro or cmake)\n\n\n\n\n",
      "id": 20
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "install",
      "content": "\nTable of Contents\n\nInstall\nInstall\n\nInstall SolAR version 0.6.0 - Windows 7/10\nInstall SolAR version 0.6.0 - Ubuntu 18.04\n\n\n\n\n\nInstall\n\n\n\n\n\nInstall\n\n\nInstall SolAR version 0.6.0 - Windows 7/10\n\nA SolAR Framework Windows installer is provided in order to quickly install:\n\n\n\n\nthe SolARFramework library (debug and release modes) and interfaces (C++ header files)\n\n\nSolAR modules libraries (debug and release modes) and interfaces (C++ header files)\n\n\nthe required third party libraries (debug and release modes) and third party interfaces (C++ header files)\n\n\nthe QT creator pre-requisites\n\n\nSolAR samples (C++ sample code)\n\n\n\n\nThe installer can be downloaded here:\n\n\nhttps://github.com/SolarFramework/binaries/releases/download/SolARFramework-installer%2Fwin/SolarFramework-installer-0.6.0.exe\n\n\nUsing the installer is straightforward:\n\n\n\n\ndownload then launch the installer\n\n\nread then accept the license agreement, then press \"Next\"\n\n\n\n\n\n\n\n\n\n\nPlease close QT Creator, if running, on your computer, before executing the installer.\n\n\n\n\n\n\n\n\n\n\n\n\nselect the destination installation folder (default is: C:\\SolARFramework).\n\n\n\n\n\n\n\n\n\n\n\nselect the components you want to install.\n\n\n\n\n\n\n\n\n\nRegarding this last step, you have the choice between:\n\n\n\n\nSolAR libraries: this will install SolAR Framework and SolAR modules libraries and interfaces only.\n\n\nQT creator dependencies: this will install a windows version of pkg-config program, used by SolAR build scripts to generate Makefiles under QT creator\n\n\nThird party libraries: this will install the following third party libraries and interfaces: fbow, freeglut, opencv, boost, eigen, spdlog and xpcf\n\n\nSources: this will install a sources folder containing SolAR Framework, Modules and Samples sources.\n\n\nSample code: this will create a Samples folder under your SolARFramework installation folder. This folder contains sample C++ projects that you can use\nto learn the basics of SolAR.\n\n\nBuild scripts: this will create a build-scripts folder containing bash scripts to manage git repositories and to compiles sources\n\n\nExecutable Samples: this will create a Samples folder containing 4 executable samples (fiducial marker, natural image marker, triangulation based on 2 images and a SLAM sample)\n\n\n\n\nPlease use the default values: it will install everything you need to use SolAR, especially if you want to first test a sample code.\n\n\n\nInstall SolAR version 0.6.0 - Ubuntu 18.04\n\n\n\ndownload both files SolARFramework-0.6.0.tar.xz and SolARFrameworkInstall-0.6.0.sh at the following address:\n\n\nopen a terminal and go to the directory where you have saved SolARFramework-0.6.0.tar.xz and SolARFrameworkInstall-0.6.0.sh\n\n\nrun the following commands\n\n\nfoo@bar:~$ chmod 755 SolARFrameworkInstall-0.6.0.sh\nfoo@bar:~$ ./SolARFrameworkInstall-0.6.0.sh all\n\n\n\n\nnote that you can run the following command to get instructions on how to use the installer:\n\n\nfoo@bar:~$ ./SolARFrameworkInstall-0.6.0.sh\n\n\n\n\nas a result, a SolARFramework directory should now be present under your HOME directory. This directory should contain:\n\n\nfoo@bar:~$ ls SolARFramework\nbuild-scripts  samples  sources SolARLibraries\n\n\n\n\nin your terminal you can now run samples (executable) contained under the samples/ folder\n\n\nyou can also browse SolAR sources under the sources folder\n\n\nyou will be able to update your source tree, running the following command:\n\n\nfoo@bar:~$ ./build-scripts/updateGit.sh develop\n\n\n\n\nfinally you will be able to compile the source code:\n\n\nfoo@bar:~$ ./build-scripts/cmake-build.sh all\n\n\n\n\nnote that in order to run samples your LD_LIBRARY_PATH needs to contain current directory (\"./\"). This can be performed by running the following command:\n\n\nfoo@bar:~$ export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH\n\n\n\n\n\n\n\n",
      "id": 21
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "on linux",
      "content": "\nTable of Contents\n\nInstall On Linux\nSupported distribution\nInstall development environment\n\nInstall QT Creator IDE\nInstall dependencies\n\n\nBuild SolAR\n\nClone SolAR\nBuild with scripts\nBuild on Qt Creator\n\n\nQ&amp;A\n\nRemaken does not work\n\n\n\n\n\nInstall On Linux\n\n\n\n\n\nSupported distribution\n\n\nCurrently, only Ubuntu 18.04 and Ubuntu 20.04 distribution has been tested, and are highly recommended. We plan to support Ubuntu 22.04 very soon.\n\n\n\n\nInstall development environment\n\n\nThe SolAR framework uses a dedicated pipeline to link and compile code as well as internal tools for third party downloads which should make your job much easier.\nAs you will see later, the SolAR framework is based on QMake originally created by the Qt Company, but compliant with most IDE.\n\n\nInstall QT Creator IDE\n\nAs SolAR is using QMake file to setup the projects, we highly recommend to install Qt Creator. Moreover, you will have access to wizards which will help you create new SolAR modules, components and pipelines.\n\n\nStart to install required packages:\n\n\n\nsudo apt install build-essential libqt5x11extras5\n\n\n\nUbuntu 18.04\n\nInstall default Qt5 package:\n\n\n\nsudo apt install qt5-default\n\n\n\nDownload the latest version of QTCreator compatible with Ubuntu 18.04, namely the version 5.0.3:\nQT Creator 5.0.3 installer download\n\n\nWhen downloaded, just install QT Creator 5.0.3:\n\n\n\nchmod +x qt-creator-opensource-linux-x86_64-5.0.3.run\n./qt-creator-opensource-linux-x86_64-5.0.3.run\n\n\n\n\nUbuntu 20.04\n\nDownload the latest version of QTCreator on the following link (scroll down to choose \"Downloads for open-source users\" for open-source development):\n\n\nQt installer download\n\n\nCreate a Qt account if not already done, select where you want to install Qt. You don&#8217;t need to install Qt SDK, just the Qt Creator. So just check the following components (if not available during installation, search for QT maintenance tool or QT uninstall after installation to launch the QT tools to upgrade it):\n\n\n\n\nQt x.x.x (last version proposed)\n\n\n\nWith Desktop gcc 64-bit\n\n\nAndroid (if you want to build SolAR for Android)\n\n\n\n\n\nDeveloper and Designer Tools\n\n\n\n\nand start the installation.\n\n\n\n\nInstall dependencies\n\nTo download dependencies, the SolAR framework uses the meta dependencies management tool Remaken supporting Conan, VCPKG and its native C++ packaging structure based on pkg-config description files.\n\n\nInstall Remaken\n\n\n\nDownload the XPCF Linux Installer zip file.\n\n\nUncompress this zip file.\n\n\nIf Homebrew is not installed on your machine, you can run the following script ./installBrew.sh. Homebrew is used to install Remaken.\n\n\nrun source ~/.profile to update brew eval modification to your ${HOME}/.profile. Try brew --version commands in terminal to check installation\n\n\nThen, run `./installRemaken.sh `. This will install the latest version of remaken on your machine.\n\n\nYou can specify where remaken will install its packages by setting the desired path to REMAKEN_PKG_ROOT (defaults to ~/.remaken/packages/)\n\n\nAdd the XPCF_MODULE_ROOT environment variable with the following command\ntest -r ~/.profile &amp;&amp; echo \"export XPCF_MODULE_ROOT=~/.remaken/packages/linux-gcc/\" &gt;&gt;~/.profile\n(if REMAKEN_PKG_ROOT is not set and default path is used)\n\n\nrun again source ~/.profile to update your ${HOME}/.profile.\n\n\nBy default, Remaken installer will also install Conan, cmake and pkg_config which will be used afterwards. Try remaken --help, conan -h, and cmake -h commands in a terminal to check installation. Conan will check these remotes when searching for your dependencies.\n\n\nSolAR has its own conan remote to store pre-built dependencies for the common configurations used by the SolAR Framework. You will need to add these conan remotes with the following commands in a command prompt:\n\n\n\n\n\nconan remote add conan-solar https://artifact.b-com.com/api/conan/solar-conan-local --insert 0\nconan remote add conan-bcom https://artifact.b-com.com/api/conan/bcom-conan-local --insert 1\n\n\n\nYou should obtain a list of conan remotes that looks like this:\n\n\n\n$ conan remote list\nconan-solar: https://artifact.b-com.com/api/conan/solar-conan-local [Verify SSL: True]\nconan-bcom: https://artifact.b-com.com/api/conan/bcom-conan-local [Verify SSL: True]\nconancenter: https://center.conan.io [Verify SSL: True]\n\n\n\nIf other remotes are installed, such as conan-center or bincrafters (both deprecated), you can remove them by running\nconan remote remove name_of_the_remote\nor just disable them by running\nconan remote disable name_of_the_remote.\n\n\nThen, run:\n\n\n\nremaken init -e -f\n\n\n\nThis command will install the latest version of qmake building rules in your ${USER_HOME}/.remaken/rules folder.\nPermanently add the PKG_CONFIG_PATH environment variable pointing to the folders where .pc files can be found:\n\n\n\ntest -r ~/.profile &amp;&amp; echo \"export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig:/usr/share/pkgconfig\" &gt;&gt;~/.profile\n\n\n\nand reload your .profile:\n\n\n\nsource ~/.profile\n\n\n\nFinally, you need to configure your remaken profile according to your development environment. To do so, run the following command in a command prompt:\n\n\n\nremaken profile init --cpp-std 17 -b gcc -o linux -a x86_64\n\n\n\n\nSet conan profile\n\nAs we are using conan to download dependencies, you need to configure your default conan profile. To do so, create a new default profile with the following command:\n\n\n\nconan profile new default --detect\n\n\n\nDo not take care about the warning !\nThen, update your default conan profile with the following commands:\n\n\n\nconan profile update settings.compiler.libcxx=libstdc++11 default\nconan profile update settings.compiler.cppstd=17 default\nconan profile update settings.compiler.version=7 default\nconan profile update settings.ceres-solver:build_type=Release default\n\n\n\nCheck that your default conan profile is well set by running:\n\n\n\nconan profile show default\n\n\n\nIt should look like the following one:\n\n\n\n[settings]\nos=Linux\nos_build=Linux\narch=x86_64\narch_build=x86_64\ncompiler=gcc\ncompiler.version=7\ncompiler.libcxx=libstdc++11\ncompiler.cppstd=17\nbuild_type=Release\n[options]\n[build_requires]\n[env]\n\n\n\nFinally, set an environment variable to ask conan to automatically install system packages required by some recipes:\n\n\n\ntest -r ~/.profile &amp;&amp; echo \"export CONAN_SYSREQUIRES_MODE=\"enabled\"\" &gt;&gt;~/.profile\n\n\n\nNow, you are ready to download your dependencies with remaken.\n\n\n\nInstall CUDA\n\nIf you want to build or run SolAR pipelines with CUDA optimizations, you will need to install CUDA.\n\n\nFollow the Nvidia instructions available on the following links:\n\n\n\n\nLinux: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n\n\nWSL: https://docs.nvidia.com/cuda/wsl-user-guide/index.html#cuda-support-for-wsl2\n\n\n\n\nYou will also need to install cuDNN to run deepl learning inference by following the Nvidia instructions: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\n\n\nAdd the required environment variables:\n\n\n\ntest -r ~/.profile &amp;&amp; echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib\" &gt;&gt;~/.profile\ntest -r ~/.profile &amp;&amp; echo \"export CUDA_PATH=/usr/local/cuda/\" &gt;&gt;~/.profile\n\n\n\n\nUse Remaken\n\nRemaken uses a file called packagedependencies.txt to describe which depedencies to install, in which version, where to install them, where to download them, with which package manager and with which configuration.\n\n\nA global packagedepedencies.txt defining the common dependencies with specific packagedependencies-&lt;os&gt;.txt files defining dependencies which are specific for each os are available in the parent GIT repository SolAR, and can be downloaded on the following link:\n\n\n\n\npackagedependencies.txt\n\n\npackagedependencies-linux.txt\n\n\npackagedependencies-android.txt (optionnal, required for android cross building)\n\n\n\n\nCopy these files where you want on your computer, open a command prompt in the folder where you have copied the packagedependencies.txt and packagedependencies-&lt;os&gt;.txt files, and run remaken with the following command:\n\n\n\nremaken install packagedependencies.txt\n\n\n\nThis command will install all SolAR dependencies in release mode in your ${HOME}/.remaken/packages folder. You can go and have a cup of coffee while it&#8217;s downloading.\n\n\nTo download the dependencies in debug mode, run the following command:\n\n\n\nremaken install -c debug packagedependencies.txt\n\n\n\n\n\n\n\n\n\nYou do not need to launch remaken install on packagedependencies_linux. remaken install packagedependencies.txt will automatically load it depending on the platform you are executing it.\n\n\n\n\n\nThis is done, all your dependencies are downloaded and ready to use !\n\n\nSome of the modules will download and build third parties using Conan which requires CMake.\n\n\n\n\n\n\nBuild SolAR\n\n\nClone SolAR\n\nSolAR is made up of a multitude of projects (SolAR Framework, SolAR pipeline manager, modules, samples, etc.). To help you, we have created a parent repository with sub-modules regrouping all source codes of SolAR projects. You can clone it from the following url:\nhttps://github.com/SolarFramework/SolAR.git\n\n\n\n\n\n\n\n\nNo space in the path of the folder where you are cloning SolAR !\n\n\n\n\n\n\ngit clone --recurse-submodules https://github.com/SolarFramework/SolAR.git\n\n\n\nIf you forgot to use --recurse-submodules, you can initialize the submodules afterwards with:\n\n\n\ncd SolAR\ngit submodule update --init --recursive\n\n\n\nIf you want to move all submodules on HEAD of master, launch the following commands\n\n\n\ngit submodule foreach --recursive git checkout master\ngit submodule foreach --recursive git pull\n\n\n\nIf you do not want to download all the source codes of SolAR, you can have a look to the different repositories available on Github on the Community/Git page.\n\n\n\nBuild with scripts\n\nFor Linux, build scripts are available in the scripts folder.\nJust run ./build_all.sh to rebuild the SolAR framework, the modules and their tests, the pipelines and their tests, the samples, the services and their tests. If you want to rebuild them seprately, use the corresponding script. Check with ./build_all.sh --help the different options such as the cross-build for Android, the number of processors to use, the version of Qt, the path to SolAR root folder, etc.\n\n\n\n\n\n\n\n\nbuild_all.sh will not build modules PopSift, FBOW and OpenCV with CUDA optimization. To do it, launch build_allModulesCuda.sh.\n\n\n\n\n\n\nBuild on Qt Creator\n\nCounting the framework, the pipeline manager, modules, module tests, samples, pipelines, pipeline tests, there are more than 60 QT projects on GitHub. In order to ease the building of all these projects, they are grouped in the following parent QT projects available into the root folder of SolAR:\n\n\n\n\nSolARCore\n\n\nSolARAllModules\n\n\nSolARAllModulesTests\n\n\nSolARAllSamples\n\n\nSolARAllPipelines\n\n\nSolARAllPipelineTests\n\n\nSolARAllServices\n\n\nSolARAllServicesTests\n\n\n\n\nYou can open one of them or all in QT Creator.\n\n\n\n\n\n\n\n\nIf you have not yet run a qmake on each project, some error could appear concerning the fact that conanbuildinfo.pri files do not exist. This is normal, run qmake on each project will create these files, and the error message will no longer appear.\n\n\n\n\n\nCheck by clicking on the Projects tab, and then on the Manage Kits&#8230;&#8203; button that your Qt x.x.x GCC 64 bit kit is well configured.\n\n\nThe build step  will copy the built binaries into ${HOME}/.remaken/packages for the modules and pipelines, and into &lt;module&gt;/deploy for samples and tests.\n\n\n\n\n\n\n\n\nSince QT Creator 4.14.0, we highly recommand to set the project option qmake system() behavior when parsing to Ignore to highly reduce the project loading time. But do not forget to run qmake on projects before building them.\n\n\n\n\n\n\n\n\nFigure 1. Add install and install_deps argument for Debug and Release mode.\n\n\nIf you open several projects, you will have to set their build order. You can do it in QT Creator by defining the dependencies of each project in the Projects menu, select your project, click on Dependencies, and check the projects that depend directly on the selected project (checking `Synchronize configuration' will synchronize all projects in Debug or Release configuration).\n\n\n\n\n\nFigure 2. Project dependencies settings\n\n\nTable 1. Project dependencies\n\n\n\n\n\n\nProject\nProject dependencies\n\n\n\n\nSolARCore\nNo dependency\n\n\nSolARAllModules\nSolARCore\n\n\nSolARAllModulesCuda\nSolARCore\n\n\nSolARAllModulesTests\nSolARAllModules\n\n\nSolARAllPipelines\nSolARAllModules\n\n\nSolARAllPipelineTests\nSolARAllPipelines\n\n\nSolARAllSamples\nSolARAllModules\n\n\nSolARAllServices\nSolARAllPipelines\n\n\nSolARAllServicesTests\nSolARAllServices\n\n\n\n\nThen, in the Build menu, click on Rebuild All Projects for All Configurations, and go get a cup of coffee.\n\n\n\n\n\nQ&amp;A\n\n\nRemaken does not work\n\nBe sure to install, init and use Remaken from a command prompt with admin rights.\nYou can check that cmake, pkg-config and conan are well installed with the following command:\n\n\n\ncmake -version\n\n\n\n\npkg-config --version\n\n\n\n\nconan -v\n\n\n\n\n",
      "id": 22
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Made with SolAR",
      "content": "Made with SolAR\n\nTo be done\n",
      "id": 23
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "create module",
      "content": "\nTable of Contents\n\nCreate a module\nWhat is a module ?\nCreate a module with the wizard\n\nDownload QTCreator wizards for XPCF\nCreate a SolAR module in QTCreator\nQTCreator project file\nPackage Dependencies file\nMyModuleAPI.h\nMyModule_main.cpp\nbcom-MyModule.pc.in file\nThe registry file\n\n\n\n\n\nCreate a module\n\n\n\n\n\nWhat is a module ?\n\n\nA module consists of a shared library embedding a set of SolAR components as well as the corresponding factory parameters to instantiate them. The management of modules is based on the XPCF third party providing a lightweight cross platform framework.\nXPCF provides the following features to manage SolAR modules:\n\n\n\n\nIntrospection to figure out which components are available in the module\n\n\nSeparate implementation from interface to create the concrete implementation of a SolAR component and to bind it to abstract SolAR and XPCF component interfaces.\n\n\nComponent creation\n\n\n\n\nEach module implementation is identified with a Universally Unique IDentifier (UUID) to nearly ensure the uniqueness of a component implementation when the system instantiates it.\n\n\nAs mentionned previoulsy, the third party XPCF can introspect the shared library of a module to obtain information about the embedded components and the interfaces they implement. Only, the introspection of a shared library requires to load it, what could become tricky when the shared library has been built on a different platform than yours (useful for authoring tools supporting cross-platform compilation such as Unity). For this reason, XPCF proposes to associate to each module a registry file in xml format that describes the module with:\n\n\n\n\nThe module UUID,\n\n\nThe components embedded in the module with their UUID,\n\n\nThe abstract component interfaces implemented by the components.\n\n\n\n\nThus, any system will be able to introspect a module without the need to load the corresponding shared library.\n\n\n\n\nCreate a module with the wizard\n\n\nDownload QTCreator wizards for XPCF\n\nCreating a new module from scratch can be a little bit tricky. To help you, a QTCreator wizard is available and will make the task much easier.\nStart by installing this wizard by launching the install.bat on Windows or install.sh on Linux (in your ${XPCF_MODULE_ROOT}/xpcf/[version]/wizards/qtcreator).\n\n\n\nCreate a SolAR module in QTCreator\n\nOpen QTCreator and create a new project (in File menu).\n\n\nSelect XPCF projects and XPCF Module template and click on Choose button.\n\n\n\n\n\nFigure 1. Create an SolAR Module in QT\n\n\nThen, set the name of your module and its location (The creation of a dedicated folder regrouping all module projects is recommended).\n\n\n\n\n\nFigure 2. Set module project name in QT\n\n\nNext, provide the details concerning your module. You can set a different name to the package if you want to embed several modules in the same package. But, by default, keep the same one. Also, you can define the version and if you want to produce a static or a shared library. We highly recommend to use the shared library for modules. Also, for installation, all dependencies of your module can be copied in a recursive mode in your package folder. Thus, when you will deploy your module, you will be sure that all required third parties will be also deployed. For the link step, you can let the SolAR build pipeline find automatically the dependencies recursively. Finally, if you want to load your project in visual studio, check the box QTVS. Thus, you will be able to load the QT project in visual studio via the plugin QT Visual Studio Tools.\n\n\n\n\n\nFigure 3. Set module details in QT\n\n\nThen, define your build system with qmake.\n\n\nFor the next step, you have to enter the directory where the XPCF binaries are located. Normally, you will find it in your &lt;USER_HOME&gt;, in the folder .remaken\\packages\\. You have also to set the namespace of your module. In order to have an easy access to all your modules by completion when coding, we recommend to use the  namespace SolAR::MODULES::MyModule for any module.\n\n\n\n\n\nFigure 4. Set XPCF version directory and namespace\n\n\nThen, choose your development kit that is used to configure your project in QTCreator. We recommend to use MSVC 2017 64bit on Windows or Clang on Linux.\n\n\n\n\n\nFigure 5. Development kits selection\n\n\nFinally, no subproject to add here. If you want to add a version control, select it and click on the Finish button.\n\n\nNow, six files have been created: the project file MyModule.pro, a C++ file MyModule_main.cpp, a header file MyModuleAPI.h defining the macro to export your interfaces, a packagedependencies.txt file to manage the dependencies of your module, a bcom_MyModule.pc.in used by pkg_config to describe your module, and finally findremakenrules.pri to link to remaken rules.\n\n\nBut let&#8217;s take a closer look at these files.\n\n\n\nQTCreator project file\n\nMyModule.pro\n\nQT       -= core gui\nCONFIG -= app_bundle qt\n\nTARGET = MyModule\nFRAMEWORK = $${TARGET}\nVERSION=0.11.0\nDEFINES +=  $${TARGET}VERSION=\\\"$${VERSION}\\\"\n\n# Uncomment following line to add more verbose information from builddefs-qmake rules\n# CONFIG += verbose\n# Uncomment following line to prepare remaken package\n# CONFIG += package_remaken\n\nCONFIG += with_qtvs\n\nwith_qtvs {\n    PROJECTCONFIG = QTVS\n}\n\nCONFIG += c++1z\nCONFIG += shared\n\nstaticlib {\n    DEPENDENCIESCONFIG = staticlib\n    REMAKEN_PKGSUBDIR=static\n} else {\n    DEPENDENCIESCONFIG = sharedlib\n    REMAKEN_PKGSUBDIR=shared\n}\n\nCONFIG(debug,debug|release) { (1)\n    DEFINES += _DEBUG=1\n    DEFINES += DEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/debug\n}\n\nCONFIG(release,debug|release) { (1)\n    DEFINES += NDEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/release\n}\n\npackage_remaken {\n    message(\"Preparing remaken package installation in $${REMAKEN_PKGSUBDIR}\")\n    INSTALLSUBDIR=$${REMAKEN_PKGSUBDIR}\n}\n\ninclude(findremakenrules.pri) (2)\n\nDEPENDENCIESCONFIG = sharedlib\nDEPENDENCIESCONFIG += install_recurse (3)\n\n## Configuration for Visual Studio to install binaries and dependencies. Work also for QT Creator by replacing QMAKE_INSTALL\nPROJECTCONFIG = QTVS (4)\n\n#NOTE : CONFIG as staticlib or sharedlib, DEPENDENCIESCONFIG as staticlib or sharedlib and PROJECTDEPLOYDIR MUST BE DEFINED BEFORE templatelibbundle.pri inclusion\ninclude ($${QMAKE_REMAKEN_RULES_ROOT}/templatelibconfig.pri)\n\n\nDEFINES += BOOST_ALL_NO_LIB\nDEFINES += BOOST_ALL_DYN_LINK\n\nSOURCES +=     MyModule_main.cpp\n\nHEADERS +=     MyModuleAPI.h\nunix {\n}\n\nmacx {\n    DEFINES += _MACOS_TARGET_\n    QMAKE_MAC_SDK= macosx\n    QMAKE_CFLAGS += -mmacosx-version-min=10.7 #-x objective-c++\n    QMAKE_CXXFLAGS += -mmacosx-version-min=10.7 -std=c++17 -fPIC#-x objective-c++\n    QMAKE_LFLAGS += -mmacosx-version-min=10.7 -v -lstdc++\n    LIBS += -lstdc++ -lc -lpthread\n}\n\nwin32 {\n    DEFINES += _X86_VC12_TARGET_\n    DEFINES += MBCS _MBCS\n }\n\nINCLUDEPATH += $${PWD}\n\nheader_files.path = $${PROJECTDEPLOYDIR}/interfaces/\nheader_files.files = $$files($${PWD}/I*.h)\n\nINSTALLS += header_files\nDISTFILES +=     Makefile\n\nOTHER_FILES +=     packagedependencies.txt\n\n#NOTE : Must be placed at the end of the .pro\ninclude ($${QMAKE_REMAKEN_RULES_ROOT}/remaken_install_target.pri) (5)\n\n\n\n\n\n1\nThese CONFIG definitions must be added to your .pro file if you want to work with Visual Studio.\n\n\n2\nThis .pri file has been installed by the wizard. It will allow to find the remaken folder depending on the OS you are using.\n\n\n3\nThe dependencies of your module will be installed recursively. More details are available on the builddefs-qmake project on GitHub.\n\n\n4\nThe installation of your module will also work with Visual Studio. Warning, in QTCreator, this will replace the usual QMAKE_INSTALL.\n\n\n5\nPlace at the end the .pri file to install your module.\n\n\n\n\n\nPackage Dependencies file\n\nAs mentionned previously, SolAR framework provides developers with a build pipeline allowing, among other things, to easily manage dependencies (download, version management, packaging during deployment step, etc.).\n\n\nTo define the dependencies used by your pipeline, just add to your packagedependencies.txt a reference to the SolARFramework and to the third parties used by your module as shown below:\n\n\n/.packagedependencies.txt\n\n\n\nSolARFramework|0.11.0|SolARFramework|SolARBuild@github|https://github.com/SolarFramework/SolarFramework/releases/download\nMyThirdParty1|x.x.x|MyThirdPaty1Folder|ThirdParty@github|https://github.com/MyGitHubProject/MyModule/releases/download (1)\nMyThirdParty2#stable|x.x.x|MyThirdParty2Folder|conan|conan-center|shared|MyThirdParty2BuildOption (2)\n\n\n\n\n\n1\nYour third party is available in the release of your github project. To know how to package your third party, check the Third parties packaging page\n\n\n2\nYour third party can be directly downloaded using conan (for instance, from conan-center).\n\n\n\n\nYou will find bellow the syntax for each dependency (more information are available on the Remaken project on GitHub):\n\n\n\ndependency_name|dependency_version|dependency_folder_name|dependency_path|dependency_download_url\n\n\n\nXPCF and the build pipeline handle dependencies recursivity, meaning that you do not need to add the dependencies of the SolAR framework.\n\n\nYour third parties, should be available in your &lt;USER_HOME&gt;/.remaken/packages/&lt;yourCompiler&gt;/ folder. To automatically download your dependencies, just run remaken where your packagedependencies.txt is located.\n\n\nNumerous samples of packagedependencies.txt files can be found with the SolAR Modules you have surely installed on your machine.\n\n\n\nMyModuleAPI.h\n\nThe MyModuleAPI.h file has been automatically created and it defines the macro MYMODULE_EXPORT_API you will have to place in front of each component interface to export them in your shared library.\n\n\n\nMyModule_main.cpp\n\n/MyModule_main.cpp\n\n\n\n#include &lt;xpcf/module/ModuleFactory.h&gt;\n#include &lt;iostream&gt;\n\nnamespace xpcf=org::bcom::xpcf;\n\n/**\n *  @ingroup xpcfmodule\n */\n/**\n  * Declare module.\n  */\n (1)\nXPCF_DECLARE_MODULE(&quot;{ab4241b5-db78-4312-9e9b-f7896cddb961}&quot;,&quot;SolAR::MODULES::MyModule&quot;,&quot;MyModule module description&quot;);\n\n/**\n * This method is the module entry point.\n * XPCF uses this method to create components available in the module.\n *\n * Each component exposed must be declared inside a xpcf::tryCreateComponent&lt;ComponentType&gt;() call.\n */\n (2)\nextern &quot;C&quot; XPCF_MODULEHOOKS_API xpcf::XPCFErrorCode XPCF_getComponent(const xpcf::uuids::uuid&amp; componentUUID,SRef&lt;xpcf::IComponentIntrospect&gt;&amp; interfaceRef)\n{\n    xpcf::XPCFErrorCode errCode = xpcf::XPCFErrorCode::_FAIL;\n    /* Sample code to declare components instanciation\n    errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::MyModule::componentType&gt;(componentUUID,interfaceRef);\n    if (errCode != xpcf::XPCFErrorCode::_SUCCESS) {\n        errCode = xpcf::tryCreateComponent&lt;SolAR::MODULES::MyModule::otherComponentType&gt;(componentUUID,interfaceRef);\n    }\n    */\n    return errCode;\n}\n\n/**\n  * The declarations below populate list of the components available in the module (it represents the module index).\n  * XPCF uses this index to introspect the components available in a module, providing the ability to generate the configuration file skeleton from the code.\n  */\n (3)\nXPCF_BEGIN_COMPONENTS_DECLARATION\n/* sample components declarations\nXPCF_ADD_COMPONENT(SolAR::MODULES::MyModule::componentType)\nXPCF_ADD_COMPONENT(SolAR::MODULES::MyModule::otherComponentType)\n*/\nXPCF_END_COMPONENTS_DECLARATION\n\n\n\nThis .cpp file has three main goals:\n\n\n\n\n1\nIt declares the module to XPCF with a given UUID, a given namespace and a given name\n\n\n2\nIt allows the XPCF component factory, thanks to the XPCF_getComponent method, to retrieve and create each component from its UUID\n\n\n3\nit declares the components embedded in the module\n\n\n\n\nEach time you will want to embed a new component into the module, you will have to add its creation in the XPCF_getComponent method and to add it in the component declaration.\n\n\n\nbcom-MyModule.pc.in file\n\nThe SolAR build pipeline can use pkg-config that helps to insert the correct compiler options on the command line. Thus, an application can use gcc -o test test.c pkg-config --libs --cflags glib-2.0 for instance, rather than hard-coding values on where to find glib (or other libraries). When you will install your module, this file will be directly copied to your binary folder &lt;USER_HOME&gt;/.remaken/packages/&lt;yourInstallSubDir&gt;/&lt;yourCompiler&gt;/&lt;MyModule&gt;/&lt;MyModule_Version&gt;.\n\n\n\nThe registry file\n\nAs mentionned previoulsy, a shared library has to be loaded for introspection, what is quite tricky when it has been built on a platform different from yours. For this reason, we always associate to a module a registry file describing information about the module itself and its components.\n\n\nCreate an xml file, name it xpcf_MyModule_registry.xml, and copy the following code in it (again replace MyModule with the name of your module):\n\n\nxpcf_MyModule_registry.xml\n\n&lt;xpcf-registry&gt;\n&lt;module uuid=&quot;MyModule_UUID&quot; name=&quot;MyModule_Name&quot; description=&quot;MyModule_Description&quot; path=&quot;$XPCF_MODULE_ROOT/SolARBuild/MyModule/MyModule_version/lib/x86_64/shared&quot;&gt;\n(1)\n&lt;/module&gt;\n&lt;/xpcf-registry&gt;\n\n\n\n\n\n1\nCopy and paste the module UUID, name and description defined in the file MyModule.cpp. Update the path by replacing MyModule by the name of your module and MyModule_version by the version number of your module defined in your .pro file.\n\n\n\n\nThis file is somewhat empty, but will be completed when adding components. It will be copied to the XPCF registry folder HOME_USER_/.xpcf/.\n\n\n\n",
      "id": 24
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "module api",
      "content": "\nTable of Contents\n\nModule API\nModule API\n\n\n\nModule API\n\n\n\n\n\nModule API\n\n\nIn order to create your SolAR pipeline, please refer to the list of available API of modules provided by SolAR :\n\n\n\n",
      "id": 25
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "pipeline",
      "content": "\nTable of Contents\n\nAssemble C++ pipeline\nWhat is a C++ pipeline ?\nInitialize your pipeline step-by-step\n\nCreate your pipeline project\nSelect your dependencies\nCreate the configuration file\n\n\nImplement your pipeline\n\nUsing QT Creator Wizard\nPipeline API\nPipeline Template\nPipeline sample\n\n\nAdd your pipeline to your module\nExport your pipeline for Unity\n\nGeneral\nUnity Android Deployment\n\n\nTest your pipeline in C++\n\n\n\nAssemble C++ pipeline\n\n\n\n\n\nWhat is a C++ pipeline ?\n\n\nA C++ pipeline is a shared library embedding an implementation of the pipeline interface defined by the SolAR framework (IMappingPipeline, IMapUpdatePipeline, IPoseEstimation Pipeline, IRelocalizationPipeline, etc.). Thus, this pipeline can be loaded at run-time and run by any third party application such as a C++ application, a service or by Unity (Unity supports only IPoseEstimationPipeline at this time). To provide configuration capabilities to a pipeline, it is considered by SolAR as a component embedded in a module. For this reason, readers are encouraged to have a look at the create a module section.\n\n\nNew pipeline interfaces will be soon added for other vision task such as 3D dense mapping, object recognition, etc.\n\n\n\n\nInitialize your pipeline step-by-step\n\n\nCreate your pipeline project\n\nIf not already done, start by installing this wizard by launching the install.bat on Windows or install.sh on Linux (in your ${XPCF_MODULE_ROOT}/xpcf/[version]/wizards/qtcreator).\n\n\n\n\nQT Creator\n\n\n\n\nOpen QTCreator and create a new project (in File menu).\n\n\n\n\n\n\n\nThen, set the name of your pipeline (e.g. MyPipeline) and the location of the QT project.\n\n\n\n\n\n\n\nSet the name of your package (you can reuse the name of your pipeline), the package version, and select the value install recursively all dependencies and check the QTVS box if you want to load your project in Visual Studio.\n\n\n\n\n\n\n\nThen, select the XPCF package path, and set a namespace for your pipeline (we recommend to set the namespace to SolAR::PIPELINES).\n\n\n\n\n\n\n\nFor the next step, choose your development kits. We recommend to use MSVC 2017 64bit on Windows or Clang on Linux.\n\n\nYour project is now created and the result is a .pro file like following:\n\n\nMyPipeline.pro\n\nQT       -= core gui\nCONFIG -= app_bundle qt\n\nTARGET = MyPipeline\nFRAMEWORK = $${TARGET}\nVERSION=0.11.0\nDEFINES +=  $${TARGET}VERSION=\\\"$${VERSION}\\\"\n\n# Uncomment following line to add more verbose information from builddefs-qmake rules\n# CONFIG += verbose\n# Uncomment following line to prepare remaken package\n# CONFIG += package_remaken\n\nCONFIG += with_qtvs\n\nwith_qtvs {\n    PROJECTCONFIG = QTVS\n}\n\nCONFIG += c++1z\nCONFIG += shared\n\nstaticlib {\n    DEPENDENCIESCONFIG = staticlib\n    REMAKEN_PKGSUBDIR=static\n} else {\n    DEPENDENCIESCONFIG = sharedlib\n    REMAKEN_PKGSUBDIR=shared\n}\n\nCONFIG(debug,debug|release) {\n    DEFINES += _DEBUG=1\n    DEFINES += DEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/debug\n}\n\nCONFIG(release,debug|release) {\n    DEFINES += NDEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/release\n}\n\npackage_remaken {\n    message(\"Preparing remaken package installation in $${REMAKEN_PKGSUBDIR}\")\n    INSTALLSUBDIR=$${REMAKEN_PKGSUBDIR}\n}\n\ninclude(findremakenrules.pri) (1)\n\nDEPENDENCIESCONFIG = sharedlib\nDEPENDENCIESCONFIG += install_recurse (2)\n\n## Configuration for Visual Studio to install binaries and dependencies. Work also for QT Creator by replacing QMAKE_INSTALL\nPROJECTCONFIG = QTVS (3)\n\n#NOTE : CONFIG as staticlib or sharedlib, DEPENDENCIESCONFIG as staticlib or sharedlib and PROJECTDEPLOYDIR MUST BE DEFINED BEFORE templatelibbundle.pri inclusion\ninclude ($${QMAKE_REMAKEN_RULES_ROOT}/templatelibconfig.pri)\n\n\nDEFINES += BOOST_ALL_NO_LIB\nDEFINES += BOOST_ALL_DYN_LINK\n\nSOURCES +=     MyPipeline_main.cpp\n\nHEADERS +=     MyPipelineAPI.h\nunix {\n}\n\nmacx {\n    DEFINES += _MACOS_TARGET_\n    QMAKE_MAC_SDK= macosx\n    QMAKE_CFLAGS += -mmacosx-version-min=10.7 #-x objective-c++\n    QMAKE_CXXFLAGS += -mmacosx-version-min=10.7 -std=c++17 -fPIC#-x objective-c++\n    QMAKE_LFLAGS += -mmacosx-version-min=10.7 -v -lstdc++\n    LIBS += -lstdc++ -lc -lpthread\n}\n\nwin32 {\n    DEFINES += _X86_VC12_TARGET_\n    DEFINES += MBCS _MBCS\n }\n\nINCLUDEPATH += $${PWD}\n\nheader_files.path = $${PROJECTDEPLOYDIR}/interfaces/\nheader_files.files = $$files($${PWD}/I*.h)\n\nINSTALLS += header_files\nDISTFILES +=     Makefile\n\nOTHER_FILES +=     packagedependencies.txt\n\n#NOTE : Must be placed at the end of the .pro\n    include ($${QMAKE_REMAKEN_RULES_ROOT}/remaken_install_target.pri) (4)\n\n\n\n\n\n1\nThis .pri file has been installed by the wizard. It will allow to find the remaken folder depending on the OS you are using.\n\n\n2\nThe dependencies of your pipeline will be installed recursively. More details are available on the builddefs-qmake project on GitHub.\n\n\n3\nThe build and installation of your pipeline will also work with Visual Studio. Warning, in QTCreator, this will replace the usual QMAKE_INSTALL.\n\n\n4\nPlace at the end the .pri file to install your pipeline.\n\n\n\n\nVisual Studio\n\nYou can also simply create your pipeline with Visual Studio by using the .pro file (see above for more details how to configure the .pro file).\n\n\nMicrosoft Visual Studio provides a Qt Visual Studio Tools. This enables developers to import QT project files (.pro) into Visual Studio.\n\n\nInstall QT Visual Studio Tools:\n\n\n\n\nIn Visual Studio, select Tools &gt; Extensions and Updates &gt; Online to install and update QT Visual Studio Tools.\n\n\n\n\nImport the .pro. file into Visual Studio:\n\n\n\n\nSelect Qt VS Tools &gt; Open Qt Project File (.pro) and choose your .pro file.\n\n\n\n\nRight now, your project is configured.\n\n\n\n\nSelect your dependencies\n\nAs mentionned previously, SolAR framework provides developers with a building pipeline allowing among other things to easily manage dependencies (download, version management, packaging during deployment step, etc.).\n\n\nTo define the dependencies used by your pipeline, just replace in your packagedependencies.txt the reference to XPCF package by a reference to the SolARFramework as shown below:\n\n\n/.packagedependencies.txt\n\n\n\nSolARFramework|0.11.0|SolARFramework|SolARBuild@github|https://github.com/SolarFramework/SolarFramework/releases/download\n\n\n\nHere is the syntax for each dependency (more information available on the Remaken project on GitHub):\n\n\n\nframework#channel | version | [condition]#library name | identifier or local folder@repository_type | repository_url | link_mode | options\n\n\n\nAs the component manager provided by XPCF can load at runtime the modules used by your pipeline as defined into your configuration file presented next, you do not need to add them into the packagedependencies.txt file. Also, XPCF and the build pipeline handle dependency recursivity, meaning that you do not need to add the dependency to XPCF already referenced by the SolAR framework package.\n\n\nYour dependencies, also called artifacts, should be available in your ${XPCF_MODULE_ROOT}/ folder. To automatically download your dependencies, just run remaken install where your packagedependencies.txt is located.\n\n\n\n\n\n\n\n\nRefer to https://github.com/b-com-software-basis/remaken for more information.\n\n\n\n\n\nNumerous samples of packagedependencies.txt files can be found with the SolAR Samples you have surely installed on your machine.\n\n\n\nCreate the configuration file\n\nThis file will be used at run-time. It will allow XPCF to load Module components and configure them at run-time. So you can experiment different implementations and configurations of a pipeline without the need to recompile your application.\n\n\nCreating a configuration file is very easy.\nOnce you have identified the modules and the components required to assemble your pipeline, just record them in the xml configuration file.\n\n\nFollowing, an example of a configuration file for a pipeline using the camera and image viewer components embedded in the SolARModuleOpenCV module :\n\n\nMyPipelineConfiguration.xml\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot; ?&gt;\n&lt;xpcf-registry autoAlias=&quot;true&quot;&gt;\n\n&lt;module uuid=&quot;15e1990b-86b2-445c-8194-0cbe80ede970&quot; name=&quot;SolARModuleOpenCV&quot; path=&quot;$XPCF_MODULE_ROOT/SolARBuild/SolARModuleOpenCV/0.11.0/lib/x86_64/shared&quot; description=&quot;OpenCV&quot;&gt; (1)\n  &lt;component uuid=&quot;5B7396F4-A804-4F3C-A0EB-FB1D56042BB4&quot; name=&quot;SolARCameraOpencv&quot; description=&quot;SolARCameraOpencv&quot;&gt; (2)\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt;\n    &lt;interface uuid=&quot;5DDC7DF0-8377-437F-9C81-3643F7676A5B&quot; name=&quot;ICamera&quot; description=&quot;ICamera&quot;/&gt;\n  &lt;/component&gt;\n  &lt;component uuid=&quot;fa4a780a-9720-11e8-9eb6-529269fb1459&quot; name=&quot;SolARVideoAsCameraOpencv&quot; description=&quot;SolARVideoAsCameraOpencv&quot;&gt;\n      &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt;\n      &lt;interface uuid=&quot;5DDC7DF0-8377-437F-9C81-3643F7676A5B&quot; name=&quot;ICamera&quot; description=&quot;ICamera&quot;/&gt;\n  &lt;/component&gt;\n  &lt;component uuid=&quot;19ea4e13-7085-4e3f-92ca-93f200ffb01b&quot; name=&quot;SolARImageViewerOpencv&quot; description=&quot;SolARImageViewerOpencv&quot;&gt;\n    &lt;interface uuid=&quot;125f2007-1bf9-421d-9367-fbdc1210d006&quot; name=&quot;IComponentIntrospect&quot; description=&quot;IComponentIntrospect&quot;/&gt;\n    &lt;interface uuid=&quot;b05f3dbb-f93d-465c-aee1-fb58e1480c42&quot; name=&quot;IImageViewer&quot; description=&quot;IImageViewer&quot;/&gt;\n  &lt;/component&gt;\n&lt;/module&gt;\n&lt;factory&gt;\n    &lt;bindings&gt;\n      &lt;bind interface=&quot;ICamera&quot; to=&quot;SolARCameraOpencv&quot; properties=&quot;CameraProperty&quot;/&gt; (3)\n      &lt;bind interface=&quot;ICamera&quot; to=&quot;SolARVideoAsCameraOpencv&quot; name=&quot;VideoAsCamera&quot; properties=&quot;VideoAsCameraProperty&quot;/&gt; (4)\n    &lt;/bindings&gt;\n&lt;/factory&gt;\n\n&lt;properties&gt; (5)\n  &lt;configuration component=&quot;SolARCameraOpencv&quot; name=&quot;CameraProperty&quot;&gt;\n    &lt;property name=&quot;calibrationFile&quot; type=&quot;string&quot; value=&quot;camera_calibration.json&quot;/&gt;\n    &lt;property name=&quot;deviceID&quot; type=&quot;uint&quot; value=&quot;0&quot;/&gt;\n  &lt;/configuration&gt;\n  &lt;configuration component=&quot;SolARVideoAsCameraOpencv&quot; name=&quot;VideoAsCameraProperty&quot;&gt; (6)\n    &lt;property name=&quot;calibrationFile&quot; type=&quot;string&quot; value=&quot;camera_calibration.json&quot;/&gt;\n    &lt;property name=&quot;videoPath&quot; type=&quot;string&quot; value=&quot;path to video&quot;/&gt;\n    &lt;property name=&quot;delayTime&quot; type=&quot;int&quot; value=&quot;30&quot;/&gt;\n  &lt;/configuration&gt;\n  &lt;configuration component=&quot;SolARImageViewerOpencv&quot;&gt;\n    &lt;property name=&quot;title&quot; type=&quot;string&quot; value=&quot;Original Image&quot;/&gt;\n    &lt;property name=&quot;exitKey&quot; type=&quot;int&quot; value=&quot;27&quot;/&gt;\n    &lt;property name=&quot;width&quot; type=&quot;int&quot; value=&quot;0&quot;/&gt;\n    &lt;property name=&quot;height&quot; type=&quot;int&quot; value=&quot;0&quot;/&gt;\n  &lt;/configuration&gt;\n&lt;/properties&gt;\n\n&lt;/xpcf-registry&gt;\n\n\n\n\n\n1\nAdd the modules used by your pipeline. To get information concerning the modules, have a look to their registry file in the ${HOME_DIR}/.xpcf/SolAR/.\n\n\n2\nFor each module, add the components used by your pipeline. To get information concerning the components, have a look to the registry file of their module available in the ${HOME_DIR}/.xpcf/SolAR/.\n\n\n3\nThanks to the factory field, you can bind by default a specific component implementation (here a camera openCV) on a given SolAR interface (here the ICamera).\n\n\n4\nYou can give a name to a binding. In this case, in your code you will be able to ask for this specific binding when you will want to instantiate a ICamera component.\n\n\n5\nOptionally, add a configuration to your components. The name of your configuration parameters are generally the name of the variable in the class definition of the component without the prefix \"m_\". The type of the configuration parameters are simple types such as \"String\", \"Integer\", \"Float\", \"Double\", or array of these simple types.\n\n\n6\nA second configuration is defined for the SolARVideoAsCameraOpencv with a specific name if you want to instanciate a second component with a specific configuration\n\n\n\n\nExamples of such configuration files are available in the samples directory that is provided with the SolAR installation.\n\n\n\n\n\nImplement your pipeline\n\n\nUsing QT Creator Wizard\n\nYou can create a pipeline like a component using Qt creator wizard. This component has to inherit of one of the pipeline interfaces (have a look to SolAR Framework API in SolAR::api::pipeline).\n\n\nTo create a pipeline inheriting from IPoseEstimation Pipeline, follow below instructions.\n\n\nOpen QTCreator if not already done, and load the project of your pipeline. Be sure that your project MyPipeline is active.\n\n\nThen, select in the QTCreator menu File, New File or Project&#8230;&#8203;. Select in the Files and Classes menu on the left XPCF, and then select `XPCF Component Class and Choose&#8230;&#8203;.\n\n\n\n\n\n\n\nThen, set the name of your pipeline (e.g. MyPipeline) and specify a custom base class, here SolAR::api::IPoseEstimationPipeline. If you want your pipeline to be configurable, check the corresponding box.\n\n\n\n\n\n\n\nFinally, click on Finish.\n\n\nFor more information, refer to create a component to create your pipeline.\n\n\n\nPipeline API\n\nSome abstract classes named I???Pipeline have been defined in SolAR framework and are defined in  api/pipeline/I???Pipeline.h .\nAny SolAR pipeline should implement one of these classes, that means that all methods defined in these pipelines have to be implemented in the pipeline implementation. For example, for the IPoseEstimationPipeline:\n\n\n\n\ninit: Allows to execute some processes before starting the pipeline.\n\n\nstart: Starts the pipeline with a texture buffer that will be updated when a new frame will be processed and ready for display.\n\n\ngetCameraParameters: Provide third party applications with the calibration parameters of the camera. Useful when you want to set the parameters of the virtual camera with the ones from the real one.\n\n\nupdate: A method that provides the new pose of the camera.\n\n\nloadSourceImage: If there is no camera in your pipeline, you can feed it with an external image (for instance, an image capture by the Unity web camera).\n\n\n\n\nA full description of these methods is available in the api/pipeline/IPoseEstimationPipeline.h file.\n\n\n\nPipeline Template\n\nTo implement your pipeline, you have to fill in the header file MyPipeline.h and the source file MyPipeline.cpp. Next, we provide you with templates for these two files.\n\n\nMyPipeline.h\n\n/**\n * Information concerning the copyright and licence of your pipeline\n */\n\n#ifndef MYPIPELINE_H\n#define MYPIPELINE_H\n\n// Mandatory include file (1)\n#include &quot;MyPipelineAPI.h&quot;\n#include &quot;xpcf/core/traits.h&quot;\n#include &quot;xpcf/component/ConfigurableBase.h&quot;\n#include &quot;api/pipeline/IPoseEstimationPipeline.h&quot;\n#include &quot;xpcf/threading/BaseTask.h&quot;\n\n// Add here the header file for the component interfaces required by your pipeline (2)\n// e.g. #include &quot;api/input/devices/ICamera.h&quot;\n\nnamespace xpcf=org::bcom::xpcf;\n\nnamespace SolAR { (3)\nusing namespace datastructure;\nusing namespace api;\nnamespace PIPELINES {\n\n/**\n * @class MyPipeline\n * @brief A short description of your pipeline\n */\n(4)\nclass MyPipeline_EXPORT_API MyPipeline : public org::bcom::xpcf::ConfigurableBase,\n    public api::pipeline::IPoseEstimationPipeline\n{\npublic:\n    MyPipeline();\n    ~MyPipeline();\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n    /// @brief Provide the camera parameters\n    /// @return the camera parameters (its resolution and its focal)\n    CameraParameters getCameraParameters() const override;\n\n    /// @brief Starts the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the start succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n    /// @brief Starts the pipeline and provides a texture buffer which will be updated when required.\n    /// @param[in] textureHandle a pointer to the texture buffer which will be updated at each call of the update method.\n    /// @return FrameworkReturnCode::_SUCCESS if the start succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start(void* imageDataBuffer) override;\n\n    /// @brief Stop the pipeline.\n    FrameworkReturnCode stop() override;\n\n    /// @brief update the pipeline\n    /// Get the new pose and update the texture buffer with the image that has to be displayed\n    api::sink::SinkReturnCode update(Transform3Df&amp; pose) override;\n\n    /// @brief load the source image\n    api::source::SourceReturnCode loadSourceImage(void* sourceTextureHandle, int width, int height) override;\n\n    xpcf::XPCFErrorCode onConfigured() override;\n\n    void onInjected() override;\n\n    void unloadComponent () override final;\n\nprivate:\n  // Add here the declaration of the shared references on components required by your pipeline (e.g. SRef&lt;input::device::ICamera&gt; m_camera;)\n  (5)\n\n  // Threads (6)\n  bool pipelineLoop();\n  xpcf::DelegateTask* m_pipelineLoopTask = nullptr;\n\n  // optionally, add here some parameters of your module that can be configured\n  // int m_myPipelineParameter; (7)\n};\n\n}\n}\n\n\nXPCF_DEFINE_COMPONENT_TRAITS(SolAR::PIPELINES::MyPipeline, (8)\n                             &quot;&lt;yourUUID&gt;&quot;,\n                             &quot;My pipeline&quot;,\n                             &quot;A sample pipeline used for documentation&quot;);\n\n#endif // MYPIPELINE_H\n\n\n\n\n\n1\nThese include files are mandatory. They correspond to the XPCF traits file as here we declare the module traits directly in this file, to the XPCF configurableBase header file if you want to configure your pipeline through an external xml file, and to the SolAR IPipeline header file, as your pipeline implement this SolAR interface.\n\n\n2\nThe header files of the SolAR component interfaces required by your pipeline.\n\n\n3\nSolAR and XPCF namespaces directives to shorten the calls to SolAR api and data structures.\n\n\n4\nDeclare your pipeline class, and add the functions defined as abstract in the I????Pipeline interface.\n\n\n5\nDeclaration of the components used by the pipeline (thanks to their abstract interfaces). We are using a shared reference of the components to ease their use in multi-threaded pipelines.\n\n\n6\nDeclare a XPCF task and the function running the pipeline loop in a dedicated thread.\n\n\n7\nYou can declare parameters for the pipeline. Some of these parameters may be configurable.\n\n\n8\nFinally, add the trait of your pipeline. To generate an UUID, you can use a online UUID generator\n\n\n\n\nMyPipeline.cpp\n\n/**\n * Information concerning the copyright and licence of your pipeline\n */\n\n// Mandatory header file (1)\n#include &quot;xpcf/module/ModuleFactory.h&quot;\n#include &quot;core/Log.h&quot;\n\n// Header to your pipeline declaration (2)\n#include &quot;MyPipeline.h&quot;\n\n// Header to datastructure required to exchange data between your components (3)\n// e.g. #include &quot;datastructure/Image.h&quot;\n\n// Macro used by the XPCF factory to define a component/pipeline (4)\nXPCF_DEFINE_FACTORY_CREATE_INSTANCE(SolAR::PIPELINES::MyPipeline)\n\nnamespace xpcf=org::bcom::xpcf;\n\nnamespace SolAR { (5)\nusing namespace datastructure;\nusing namespace api::pipeline;\nusing namespace api::source;\nusing namespace api::sink;\nnamespace PIPELINES {\n\nMyPipeline::MyPipeline():ConfigurableBase(xpcf::toUUID&lt;MyPipeline&gt;())\n{\n   addInterface&lt;api::pipeline::IPipeline&gt;(this);\n   // Add here the declaration of the component implementation you want to inject in your interfaces\n   // e.g. declareInjectable&lt;input::device::ICamera&gt;(m_camera);\n   // If a sepcific named binding is needed\n   // e.g. declareInjectable&lt;input::device::ICamera&gt;(m_camera, &quot;VideoAsCamera&quot;);\n   (6)\n   // Add here the mapping of your pipeline properties with the variables of your pipeline class\n   // e.g. declareProperty(&quot;myPipelineProperty&quot;, m_MyPipelineVariable);\n   (7)\n}\n\nMyPipeline::~MyPipeline()\n{\n\n}\n\nvoid MyPipeline::onInjected()\n{\n    // ADD HERE: Things to do when injectable components have been inject in your pipline.\n}\n\nxpcf::XPCFErrorCode MyPipeline::onConfigured()\n{\n  // ADD HERE: Things to do when the variables of your pipeline class have just been set according to the properties defined in your configuration file.\n}\n\nFrameworkReturnCode MyPipeline::init()\n{\n  try {\n    // ADD HERE: Thinks to do to initialize your pipeline\n    //e.g. load a marker or set camera parameters for components requiring them.\n    (8)\n\n  }\n  catch (xpcf::Exception e)\n  {\n     LOG_WARNING(&quot;One or more components cannot be created: {}&quot;, e.what());\n     return FrameworkReturnCode::_ERROR_;\n  }\n\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nCameraParameters MyPipeline::getCameraParameters() const\n{\n  CameraParameters camParam;\n  // ADD HERE the code to return camera parameters\n\n  return camParam;\n}\n\nbool MyPipeline::pipelineLoop() (9)\n{\n  // ADD HERE: the code connecting the component of your pipeline\n\n  return true;\n}\n\nSourceReturnCode MyPipeline::loadSourceImage(void* sourceTextureHandle, int width, int height)\n{\n   // ADD HERE the code to take external image as input of your pipeline\n\n   return SourceReturnCode::_NOT_IMPLEMENTED;\n}\n\n\nFrameworkReturnCode MyPipeline::start(void* imageDataBuffer)\n{\n  // ADD HERE the code to start your pipeline\n  // e.g. start the camera\n\n  // create and start a thread for the loop of the pipeline (10)\n  auto pipelineLoopThread = [this](){;pipelineLoop();};\n  m_pipelineLoopTask = new xpcf::DelegateTask(pipelineLoopThread);\n  m_pipelineLoopTask-&gt;start();\n\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nFrameworkReturnCode MyPipeline::stop()\n{\n  // ADD HERE the code to stop your pipeline\n  if (m_pipelineLoopTask != nullptr)\n    m_pipelineLoopTask-&gt;stop(); (11)\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nSinkReturnCode MyPipeline::update(Transform3Df&amp; pose)\n{\n  // ADD HERE the code to update the pose of the camera\n\n  return SinkReturnCode::_NOT_IMPLEMENTED;\n}\n\n}\n}\n\n\n\n\n\n1\nRequired header file corresponding to the XPCF module factory.\n\n\n2\nObviously, add the header file corresponding to your pipeline declaration.\n\n\n3\nAdd the header of datastructures required to exchange information between the components of your pipeline.\n\n\n4\nAdd the definition of your pipeline to the XPCF component factory.\n\n\n5\nEmbed your pipeline in the SolAR/PIPELINES namespace, and add use namespace used in your pipeline.\n\n\n6\nIf you have declare the component interfaces required for your pipeline in its header file, you can map component implementatations on them. The choice of the component implementations to map on your interfaces will be solved at runtime according to what is described in your xml configuration file. By default, the resolution chooses the first component alias declared in the configuration file that fits your interface. If you have several components alias declared in your configuration file, you can force the choice by defining a specific binding. In your pipeline, if you need several instances of a component based on different implementations, you can name your binding and pass it to the DeclareInjectable function. Do not forget to add _autoalias=true\" at the beginning of your xml configuration file if you want to automatically create aliases for all declared components based on their names.\n\n\n7\nTo set a variable of the pipeline class as configurable and map on it a property defined in the xml configuration file, you have to declare the property. Here, if an xml element with the name \"myPipelineProperty\" is specified in the configuration file, when exiting the constructor, its value will be set to the variable m_myPipelineVariable.\n\n\n8\nIf required, add the code to initialize your pipeline (e.g. load a reference image, initialize some component with the intrinsic parameters of the camera, etc.).\n\n\n9\nImplement the loop of the pipeline by connecting components. This loop will run in a dedicated thread.\n\n\n10\nStart the pipeline loop thread when the start method is called.\n\n\n11\nStop the pipeline loop thread when the stop method is called.\n\n\n\n\n\nPipeline sample\n\nWe will present next the implementation of the simplest pipeline that consists in capturing an image from a camera and make it available for a third party application. For this implementation, we will need only one module: SolARModuleOpenCV.\n\n\nMyPipeline.h\n\n/**\n * Information concerning the copyright and licence of your pipeline\n */\n\n#ifndef MYPIPELINE_H\n#define MYPIPELINE_H\n\n// Mandatory include file\n#include &quot;MyPipelineAPI.h&quot;\n#include &quot;xpcf/core/traits.h&quot;\n#include &quot;xpcf/component/ConfigurableBase.h&quot;\n#include &quot;api/pipeline/IPoseEstimationPipeline.h&quot;\n#include &quot;xpcf/threading/BaseTask.h&quot;\n\n// Add here the header file for the component interfaces required by your pipeline\n// e.g. #include &quot;api/input/devices/ICamera.h&quot; (1)\n#include &quot;api/input/devices/ICamera.h&quot;\n#include &quot;api/sink/ISinkPoseImage.h&quot;\n#include &quot;api/source/ISourceImage.h&quot;\n\nnamespace xpcf=org::bcom::xpcf;\n\nnamespace SolAR {\nusing namespace datastructure;\nusing namespace api;\nnamespace PIPELINES {\n\n/**\n * @class MyPipeline\n * @brief A short description of your pipeline\n */\nclass MyPipeline_EXPORT_API MyPipeline : public org::bcom::xpcf::ConfigurableBase,\n    public api::pipeline::IPoseEstimationPipeline\n{\npublic:\n    MyPipeline();\n    ~MyPipeline();\n\n    /// @brief Initialization of the pipeline\n    /// @return FrameworkReturnCode::_SUCCESS if the init succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode init() override;\n\n    /// @brief Provide the camera parameters\n    /// @return the camera parameters (its resolution and its focal)\n    CameraParameters getCameraParameters() const override;\n\n    /// @brief Starts the pipeline.\n    /// @return FrameworkReturnCode::_SUCCESS if the start succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start() override;\n\n    /// @brief Starts the pipeline and provides a texture buffer which will be updated when required.\n    /// @param[in] textureHandle a pointer to the texture buffer which will be updated at each call of the update method.\n    /// @return FrameworkReturnCode::_SUCCESS if the start succeeds, else FrameworkReturnCode::_ERROR_\n    FrameworkReturnCode start(void* imageDataBuffer) override;\n\n    /// @brief Stop the pipeline.\n    FrameworkReturnCode stop() override;\n\n    /// @brief update the pipeline\n    /// Get the new pose and update the texture buffer with the image that has to be displayed\n    api::sink::SinkReturnCode update(Transform3Df&amp; pose) override;\n\n    /// @brief load the source image\n    api::source::SourceReturnCode loadSourceImage(void* sourceTextureHandle, int width, int height) override;\n\n    xpcf::XPCFErrorCode onConfigured() override;\n\n    void onInjected() override;\n\n    void unloadComponent () override final;\n\nprivate:\n  // Add here the declaration of the shared references on components required by your pipeline (e.g. SRef&lt;input::device::ICamera&gt; m_camera;)\n  SRef&lt;input::devices::ICamera&gt; m_camera; (2)\n  SRef&lt;sink::ISinkPoseImage&gt; m_sink;\n  SRef&lt;source::ISourceImage&gt; m_source;\n\n  // Threads\n  bool pipelineLoop();\n  xpcf::DelegateTask* m_pipelineLoopTask = nullptr;\n\n  // optionally, add here some parameters of your module that can be configured\n  // int m_myPipelineParameter;\n\n  // Other attributes\n  bool m_externalInputImageMode = false; (3)\n};\n\n}\n}\n\n\nXPCF_DEFINE_COMPONENT_TRAITS(SolAR::PIPELINES::MyPipeline,\n                             &quot;855c83b7-f4ec-48ab-8e89-56018ea9e169&quot;,\n                             &quot;My pipeline&quot;,\n                             &quot;A sample pipeline used for documentation&quot;);\n\n#endif // MYPIPELINE_H\n\n\n\n\n\n1\ninclude the component interface headers for a camera component, a sink component and a source component. A sink component handles an output buffer feed by the pipeline a read by external third parties. Reciprocally, a source component handles an input buffer feeds by external third parties and used by the pipeline.\n\n\n2\nDeclaration of the three components used by this pipeline, a camera, a sink component handling the pose and the output image, and a source component handling an input image.\n\n\n3\nAdd an attribute to know if the pipeline use as input an image coming from a third party.\n\n\n\n\nMyPipeline.cpp\n\n/**\n * Information concerning the copyright and licence of your pipeline\n */\n\n// Mandatory header file\n#include &quot;xpcf/module/ModuleFactory.h&quot;\n#include &quot;core/Log.h&quot;\n\n// Header to your pipeline declaration\n#include &quot;MyPipeline.h&quot;\n\n// Header to datastructure required to exchange data between your components\n// e.g. #include &quot;datastructure/Image.h&quot;\n#include &quot;datastructure/Image.h&quot; (1)\n\n// Macro used by the XPCF factory to define a component/pipeline\nXPCF_DEFINE_FACTORY_CREATE_INSTANCE(SolAR::PIPELINES::MyPipeline)\n\nnamespace xpcf=org::bcom::xpcf;\n\nnamespace SolAR {\nusing namespace datastructure;\nusing namespace api::pipeline;\nusing namespace api::source;\nusing namespace api::sink;\nnamespace PIPELINES {\n\nMyPipeline::MyPipeline():ConfigurableBase(xpcf::toUUID&lt;MyPipeline&gt;())\n{\n   addInterface&lt;api::pipeline::IPipeline&gt;(this);\n   // Add here the declaration of the component implementation you want to inject in your interfaces\n   // e.g. declareInjectable&lt;input::device::ICamera&gt;(m_camera);\n   // If a sepcific named binding is needed\n   // e.g. declareInjectable&lt;input::device::ICamera&gt;(m_camera, &quot;VideoAsCamera&quot;);\n   (2)\n   declareInjectable&lt;input::devices::ICamera&gt;(m_camera);\n   declareInjectable&lt;sink::ISinkPoseImage&gt;(m_sink);\n   declareInjectable&lt;source::ISourceImage&gt;(m_source);\n   // Add here the mapping of your pipeline properties with the variables of your pipeline class\n   // e.g. declareProperty(&quot;myPipelineProperty&quot;, m_MyPipelineVariable);\n}\n\nMyPipeline::~MyPipeline()\n{\n\n}\n\nvoid MyPipeline::onInjected()\n{\n    // ADD HERE: Things to do when injectable components have been inject in your pipline.\n}\n\nxpcf::XPCFErrorCode MyPipeline::onConfigured()\n{\n  // ADD HERE: Things to do when the variables of your pipeline class have just been set according to the properties defined in your configuration file.\n}\n\nFrameworkReturnCode MyPipeline::init()\n{\n  try {\n    // ADD HERE: Thinks to do to initialize your pipeline\n    //e.g. load a marker or set camera parameters for components requiring them.\n\n  }\n  catch (xpcf::Exception e)\n  {\n     LOG_WARNING(&quot;One or more components cannot be created: {}&quot;, e.what());\n     return FrameworkReturnCode::_ERROR_;\n  }\n\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nCameraParameters MyPipeline::getCameraParameters() const\n{\n  CameraParameters camParam;\n  // ADD HERE the code to return camera parameters\n  if (m_camera) (3)\n  {\n      camParam = m_camera-&gt;getParameters();\n  }\n  return camParam;\n}\n\nbool MyPipeline::pipelineLoop() (4)\n{\n  // ADD HERE: the code connecting the component of your pipeline\n  SRef&lt;Image&gt; image;\n\n  if (m_externalInputImageMode)\n  {\n    if (m_source-&gt;getNextImage(image) == SourceReturnCode::_NEW_IMAGE)\n      m_sink-&gt;set(image);\n  }\n  else\n  {\n    if (m_camera-&gt;getNextImage(image) == SolAR::FrameworkReturnCode::_ERROR_LOAD_IMAGE)\n      return false;\n    m_sink-&gt;set(image);\n  }\n  return true;\n}\n\nSourceReturnCode MyPipeline::loadSourceImage(void* sourceTextureHandle, int width, int height)\n{\n   // ADD HERE the code to take external image as input of your pipeline\n   m_externalInputImageMode = true; (5)\n   return m_source-&gt;setInputTexture((unsigned char *)sourceTextureHandle, width, height);\n}\n\n\nFrameworkReturnCode MyPipeline::start(void* imageDataBuffer)\n{\n  // ADD HERE the code to start your pipeline\n  // e.g. start the camera\n  m_sink-&gt;setImageBuffer((unsigned char*)imageDataBuffer); (6)\n\n  if (m_camera-&gt;start() != FrameworkReturnCode::_SUCCESS) (7)\n    return FrameworkReturnCode::_ERROR_;\n\n  // create and start a thread for the loop of the pipeline\n  auto pipelineLoopThread = [this](){;pipelineLoop();};\n  m_pipelineLoopTask = new xpcf::DelegateTask(pipelineLoopThread);\n  m_pipelineLoopTask-&gt;start();\n\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nFrameworkReturnCode MyPipeline::stop()\n{\n  // ADD HERE the code to stop your pipeline\n  if (m_pipelineLoopTask != nullptr)\n    m_pipelineLoopTask-&gt;stop();\n  return FrameworkReturnCode::_SUCCESS;\n}\n\nSinkReturnCode MyPipeline::update(Transform3Df&amp; pose)\n{\n  // ADD HERE the code to update the pose of the camera\n\n  return SinkReturnCode::_NOT_IMPLEMENTED;\n}\n\n}\n}\n\n\n\n\n\n1\nAdd the header file for the data exchange between your component. Here, an image exhnage between the camera and the image viewer.\n\n\n2\nWe declare the injection of the 3 components required for this pipeline.\n\n\n3\nA simple code to get camera parameters and return it.\n\n\n4\nImplementation of the pipeline loop. If an input image has been updated, we will access to it through the source component and we pass it to the sink component. If not, we read the last image captured by the camera and we pass it to the sink component.\n\n\n5\nSet the m_externalInputImageMode to true to inform the pipeline that it run with external input images, and put the image in the buffer of the source component.\n\n\n6\nSet the image buffer for the sink component.\n\n\n7\nStart the camera.\n\n\n\n\n\n\n\nAdd your pipeline to your module\n\n\nTo add your pipeline to your module, you have just to edit the file myPipeline_main.cpp:\n\n\nmyPipeline_main.cpp\n\n#include &lt;xpcf/module/ModuleFactory.h&gt;\n#include &quot;MyPipeline.h&quot; (1)\n#include &lt;iostream&gt;\n\nnamespace xpcf=org::bcom::xpcf;\n\n/**\n *  @ingroup xpcfmodule\n */\n/**\n  * Declare module.\n  */\nXPCF_DECLARE_MODULE(&quot;{df891348-4683-432d-beff-fb9ead08f020}&quot;,&quot;SolAR::PIPELINES::MYPIPELINE&quot;,&quot;MyPipeline module description&quot;);\n\n/**\n * This method is the module entry point.\n * XPCF uses this method to create components available in the module.\n *\n * Each component exposed must be declared inside a xpcf::tryCreateComponent&lt;ComponentType&gt;() call.\n */\nextern &quot;C&quot; XPCF_MODULEHOOKS_API xpcf::XPCFErrorCode XPCF_getComponent(const xpcf::uuids::uuid&amp; componentUUID,SRef&lt;xpcf::IComponentIntrospect&gt;&amp; interfaceRef)\n{\n    xpcf::XPCFErrorCode errCode = xpcf::XPCFErrorCode::_FAIL;\n    // Sample code to declare components instanciation\n    errCode = xpcf::tryCreateComponent&lt;SolAR::PIPELINES::MyPipeline&gt;(componentUUID,interfaceRef); (2)\n    /*\n    if (errCode != xpcf::XPCFErrorCode::_SUCCESS) {\n        errCode = xpcf::tryCreateComponent&lt;SolAR::PIPELINES::otherComponentType&gt;(componentUUID,interfaceRef);\n    }\n    */\n    return errCode;\n}\n\n/**\n  * The declarations below populate list of the components available in the module (it represents the module index).\n  * XPCF uses this index to introspect the components available in a module, providing the ability to generate the configuration file skeleton from the code.\n  */\nXPCF_BEGIN_COMPONENTS_DECLARATION\n// sample components declarations\nXPCF_ADD_COMPONENT(SolAR::PIPELINES::MyPipeline) (3)\n//XPCF_ADD_COMPONENT(SolAR::PIPELINES::otherComponentType)\nXPCF_END_COMPONENTS_DECLARATION\n\n\n\n\n\n1\nAdd the header file of your pipeline.\n\n\n2\nAdd this line to be able to let XPCF create your pipeline component.\n\n\n3\nAdd you pipeline to the XPCF components declaration.\n\n\n\n\nNow you can rebuild your module (do not forget if not on Windows with QTVS mode to add a make install for debug and release mode in the configuration of your QT project).\nYour pipeline is now ready to use, and should be available in your remaken packages folder.\n\n\n\n\nExport your pipeline for Unity\n\n\nGeneral\n\nFirst of all, you must follow the installation instructions of Unity and of the SolAR plugin for unity available in the section Use Unity.\n\n\nThen, add your pipeline in the packagedependencies.txt available in the root folder of the SolARUnityProject:\n\n\n\nSolARPipeline_FiducialMarker|0.11.0|SolARPipeline_FiducialMarker|SolARBuild@github|https://github.com/SolarFramework/Sample-FiducialMarker/releases/download\nSolARPipeline_NaturalImageMarker|0.11.0|SolARPipeline_NaturalImageMarker|SolARBuild@github|https://github.com/SolarFramework/Sample-NaturalImageMarker/releases/download\nSolARPipeline_SLAM|0.11.0|SolARPipeline_SLAM|SolARBuild@github|https://github.com/SolarFramework/Sample-Slam/releases/download\nSolARPipelineManager|0.11.0|SolARPipelineManager|SolARBuild@github|https://github.com/SolarFramework/SolARPipelineManager/releases/download\nSolARModuleOpenCV|0.11.0|SolARModuleOpenCV|SolARBuild@github|https://github.com/SolarFramework/SolARModuleOpenCV/releases/download\nSolARModuleTools|0.11.0|SolARModuleTools|SolARBuild@github|https://github.com/SolarFramework/SolARModuleTools/releases/download\nSolARWrapper|0.11.0|SolARWrapper|SolARBuild@github|https://github.com/SolarFramework/SwigWrapper/releases/download\nSolARModuleFBOW|0.11.0|SolARModuleFBOW|SolARBuild@github|https://github.com/SolarFramework/SolARModuleFBOW/releases/download\nSolARModuleG2O|0.11.0|SolARModuleG2O|SolARBuild@github|https://github.com/SolarFramework/SolARModuleG2O/releases/download\nMyPipeline|0.11.0|MyPipeline|@github|https://github.com/SolarFramework/MyPipeline/releases/download (1)\n\n\n\n\n\n1\nThe line to add your pipeline to the packagedependencies.txt available in teh root folder of the SolARUnityProject. Ideally, the binaries of your pipeline should be uploaded in the release of GitHub or on an artifactory.\n\n\n\n\nThen, run the install.bat available in the root folder of the SolARUnityProject (available only on Windows). This script should install all required binaries in the Assets/Plugins folder of the unity project.\n\n\nCopy also your configuration file MyPipelineConfiguration.xml in the Assets/SolAR/Pipelines folder of your unity project, and replace the opencv module path $XPCF_MODULE_ROOT/SolARBuild/SolARModuleOpenCV/0.10.0/lib/x86_64/shared by /Assets/Plugins as now you will use the module libraris copy locally in your Unity project (do it for all modules in your configuration file).\n\n\nNow, in the Unity editor, you can select again your pipeline folder in the inspector of your SolARPipelineLoader, and select in the pipeline list your pipeline.\n\n\nIf you want to export your pipeline, create a new Unity package with:\n\n\n\n\nThe shared binaries stored in the Unity plugin folder relative to:\n\n\n\nYour pipeline\n\n\nThe modules used by your pipeline\n\n\nThe third parties of the modules used by your pipeline\n\n\n\n\n\nThe pipleline configuration file stored under Assets/SolAR/Pipelines/.\n\n\n\n\n\nUnity Android Deployment\n\nFor Unity platform we proceed to a change in the path of the pipeline configuration file to match applications public path. Your pipeline configuration will be available from the device and could be edited directly to manage its components, parameters, etc.\n\n\nDeployment details\n\n/!\\ Only arm64-v8a architecture are supported by SolAR.\n\n\nWhile you launch build for Android platform :\n\n\n\n\nOn Unity\n\n\n\nUnity pre-build process will be called to generate ./Assets/StreamingAssets/SolAR/Android/android.xml. This file will list every file in under your ./Assets/StreamingAssets/.\n\n\nPath of pipelines xml in ./Assets/StreamingAssets/SolAR/Pipelines/ are set to match Android filesystem to the public application directory. This will let your xml available on your device and you could edit them.\n\n\nThis file will be read to clone every asset included in the Android private JAR to Android application public path\n\n\nAPK is built and it includes Unity StreamingAssets and libraries\n\n\n\n\n\nOn Android\n\n\n\nWhile the application is launched for the first time, it will look for android.xml in the private application JAR. Then if this file doesn&#8217;t exist in the device application public path (/storage/emulated/0/Android/com.bcom/SolARDemo/files/StreamingAssets/SolAR/Android) it will be read and all of his content lists will be cloned into this path. Otherwise, the already present one will be read and cloned. If you don&#8217;t want to overwrite an asset at each application launching you can set the overwrite attribute for a dedicated file to false.\n\n\nThe pipeline selected by the application will be load (from public application path) and his path to ./Assets/Plugins will be matched with Android private JAR path (/data/app/com.bcom/SolARUnityPlugin-[only-known-on-running]==/lib/arm64/).\n\n\nApplication is initialized correctly. You can change the pipeline selected with SolARMenu in the right-hand corner.\n\n\n\n\n\n\n\n\n\n\nFigure 1. Android pipeline\n\n\n\n\n\n\nTest your pipeline in C++\n\n\nTo test your pipeline in C++, have a look to the section use in C++.\n\n\n",
      "id": 26
    });
    
  

  

  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Search",
      "content": "Search results\n\n\n",
      "id": 27
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "services",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: services\n:page-description: services\n:page-layout: default\n:page-category: use\n:page-liquid:\n:toc:\n\n== Services\n\n== Introduction\n\nThis quick installation guide explains how to get, install and run ARCloud services locally, on your computer, and how to test them with test client applications.\n\n== Install the Docker engine\n\nhttps://docs.docker.com/engine/install/\n\n== Configure the Docker engine\n\nAdd our repository to the \"insecure-registries\" of your Docker Engine configuration, like this:\n\n[source,xml]\n{\n  \"builder\": {\n    \"gc\": {\n      \"defaultKeepStorage\": \"20GB\",\n      \"enabled\": true\n    }\n  },\n  \"debug\": true,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": true\n  },\n  \"insecure-registries\": [\n    ...\n    \"solar-docker-local.artifact.b-com.com\"\n  ],\n  \"registry-mirrors\": []\n}\n\n== Pull the service images\n\nIMPORTANT: check the available versions of the ARCloud Services on our public repository to be able to pull their images to your local Docker engine: +\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-docker-local\n\nThen, use the `docker pull` command.\n\n[underline]*Services without GPU use:*\n\n[source,cmd]\ndocker pull solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-markers-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/mappingandrelocalizationfrontend-service:w.x.y.z\n\n[underline]*Services with GPU use:*\n\n[source,cmd]\ndocker pull solar-docker-local.artifact.b-com.com/mapupdate/0.11.0/mapupdate-cuda-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/relocalization/0.11.0/relocalization-cuda-service:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/mapping/0.11.0/mapping-multi-cuda-service:w.x.y.z\n\n== Pull the test client images\n\nIMPORTANT: check the available versions of the ARCloud test clients on our public repository to be able to pull their images to your local Docker engine: +\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-docker-local/tests/\n\nThen, use the `docker pull` command.\n\n[source,cmd]\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-client:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/map-update-displaymap-client:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-producer:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mapping-multi-viewer:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mappingandrelocalizationfrontend-client:w.x.y.z\ndocker pull solar-docker-local.artifact.b-com.com/tests/0.11.0/mappingandrelocalizationfrontend-relocviewer:w.x.y.z\n\n== Get the launch scripts for services\n\nThose scripts are available on our public repository, for Windows (\".bat\" files) and for Linux (\".sh\" files):\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/services\n\nWARNING: *At the end of the `docker run` command (last line of the scripts), you should replace the version of the service image by the current one if needed.*\n\n== Get the launch scripts for test clients\n\nThose scripts are available on our public repository, for Windows (\".bat\" files) and for Linux (\".sh\" files):\n\nhttps://artifact.b-com.com/webapp/#/artifacts/browse/tree/General/solar-generic-local/solar-service-scripts/clients\n\nWARNING: *At the end of the `docker run` command (last line of the scripts), you should replace the version of the client image by the current one if needed.*\n\n== Launch the services\n\nWARNING: ARCloud Services depend on each other and you must launch them in a specific order to test them all: +\nMap Update -> Relocalization -> Relocalization Markers -> Mapping -> Front End\n\nAs some ARCloud Services depend on others, sometimes you need to give some services URL (*IP:port*) as script parameters. To check if a script needs such parameters, just run it and you will see a help message if needed.\n\nFor example:\n\n[source,cmd]\nlaunch_relocalization.sh\nYou need to give Map Update Service URL as parameter!\n\nA specific message will be prompted for each necessary parameter.\n\nTo give the local URL of a service, use the *Docker bridge IP address (172.17.0.1)* and *the port defined in the launch script*. \n\nFor example:\n\n[source,cmd]\nlaunch_relocalization.sh 172.17.0.1:50053\n\n*The default ports currently defined for the ARCloud Services are:*\n\n* [underline]*Services without GPU (no Cuda)*:\n  - Map Update: 50053\n  - Relocalization: 50052\n  - Relocalization Markers: 50050\n  - Mapping/Mapping No Drop: 50051\n  - Front End: 50055 / 5000->5009 (*for Unity clients*)\n\n* [underline]*Services with GPU (Cuda)*:\n  - Map Update: 60053\n  - Relocalization: 60052\n  - Mapping/Mapping No Drop: 60051\n  - Front End: 60055 / 5100->5109 (*for Unity clients*)\n\n== Check that the services are running\n\nYou can use the \"docker container ls\" command to check if the services are correctly running, and to see the ports used by each one.\n[source,cmd]\ndocker container ls\nCONTAINER ID   IMAGE                                                                   COMMAND               CREATED              STATUS              PORTS                                                       NAMES\nd313d8945c02   artwin/solar/services/mappingandrelocalizationfrontend-service:latest   \"./start_server.sh\"   14 seconds ago       Up 12 seconds       0.0.0.0:5000-5009->5000-5009/tcp, 0.0.0.0:50055->8080/tcp   solarservicemappingandrelocalizationfrontend\n3ffcd5baf375   artwin/solar/services/mapping-multi-service:latest                      \"./start_server.sh\"   36 seconds ago       Up 35 seconds       0.0.0.0:50051->8080/tcp                                     solarservicemappingmulti\nb634466b4c50   artwin/solar/services/relocalization-markers-service:latest             \"./start_server.sh\"   51 seconds ago       Up 50 seconds       0.0.0.0:50050->8080/tcp                                     solarservicerelocalizationmarkers\n43a631d40498   artwin/solar/services/relocalization-service:latest                     \"./start_server.sh\"   About a minute ago   Up About a minute   0.0.0.0:50052->8080/tcp                                     solarservicerelocalization\n7e8b740da874   artwin/solar/services/map-update-service:latest                         \"./start_server.sh\"   About a minute ago   Up About a minute   0.0.0.0:50053->8080/tcp                                     solarservicemapupdate\n\n== Display the logs of the services\n\nYou can display the logs of each service using its Docker name:\n\n[source,cmd]\ndocker logs [-f] [service_name]\n\nFor example:\n[source,cmd]\ndocker logs -f solarservicemapupdate\n\n== Install and launch a X-server\n\nYou must have a X-server running on your system to manage the graphical outputs of the test applications.\n\nFor example, you can download and install a X server for Windows from this web site: https://sourceforge.net/projects/vcxsrv/\n\nThen, use the [underline]#XLaunch# shortcut installed on the Windows desktop.\n\nSelect the following options on the screens that appear successively:\n\n[underline]#Display settings# : _'Multiple windows'_ +\n[underline]#Client startup# : _'Start no client'_ +\n[underline]#Extra settings# : check _'Disable access control'_ +\n[underline]#Configuration complete# : _'Finish'_\n\n*Warning* : [underline]#if the Windows Firewall is displayed, you must allow VcXSrv connections from Public and Private networks#\n\n== Launch the test clients\n\nYou can now launch the client Docker containers using the script files, giving *the URL of the service you want to test* and *your computer local IP address (to export graphical display)* as parameter: \n\n[source,command]\nlaunch_service.sh [service_url] [host_IP_address]\nlaunch_service.bat [service_url] [host_IP_address]\n\nFor example: \n[source,command]\nlaunch_mapping_multi_viewer.bat 172.17.0.1:50053 172.17.0.1:50052 192.168.56.1\n\nThen, you can verify that the application is running correctly by looking at its traces, with this Docker command:\n\n[source,command]\ndocker logs [-f] clientcontainername\n\nCAUTION: On Windows host, do not forget to start the X Server manager before running the test application\n\n== to know more...\n\nAll the precise and detailed information you want to know about ARCloud services is available in the link:../../documentation/arcloud_services/#Services[ARCloud Services documentation].",
      "id": 28
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "standalone pipeline",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: standalone pipeline\n:page-category: assemble\n:page-description: standalone pipeline\n:page-liquid:\n:page-layout: default\n:toc:\n\n== Assemble a standalone {cpp} pipeline\n\n== What is standalone {cpp} pipeline ?\nA standalone {cpp} pipeline is a SolAR pipeline running in a standalone application. This pipeline will be defined in a _main.cpp_ class, and is very useful for pipeline debugging. We recommend starting with this approach to familiarize yourself with pipeline assembly.\n\n== Initialize your pipeline with the QTCreator wizard\n\n=== Download QTcreator wizards for XPCF\nCreating a new pipeline from scratch can be a little bit tricky. To help you, a QTcreator wizard is available and will make the task much easier. +\nStart by installing this wizard by launching the _install.bat_ on Windows or _install.sh_ on Linux (in your ${XPCF_MODULE_ROOT}/xpcf/[version]/wizards/qtcreator).\n\n=== Create a standalone SolAR pipeline in QTCreator\n\nOpen QTCreator and create a new project (in File menu).\n\nSelect `_XPCF project_` and `_XPCF Application template_` and click on `_Choose_` button.\n\nimage::images\\QTProjectAppCreation.png[create an XPCF C++ application in QT,600,600, align=\"center\", title=\"Create an XPCF C++ application in QT\"]\n\nThen, set the name of your application embedding your pipeline, and its location (Create a dedicated folder with all pipeline projects is recommended).\n\nimage::images\\QTProjectAppCreation2.png[Set pipeline project name in QT,600,600, align=\"center\", title=\"Set pipeline project name in QT\"]\n\nNext provide the details concerning your application. You can set your package version. You can define if your dependencies are static or shared. We highly recommend to use the shared library for modules. Also, for installation, all dependencies of your application can be copied in a recursive mode in your installation folder. Thus, when you will install your application, you will be sure that all required third parties will be also there to run it. For the link step, you can let the SolAR build pipeline find automatically the dependencies recursively. Finally, if you want to load your project in visual studio, check the box QTVS. Thus, you will be able to load the QTProject in visual studio via the plugin https://download.qt.io/development_releases/vsaddin/[QT Visual Studio Tools].\n\nimage::images\\QTProjectAppCreation3.png[set application details in QT,600,600, align=\"center\", title=\"Set application details in QT\"]\n\nFor the next step, you have to enter the directory where the XPCF binaries are located. Normally, you will find it in your USER_HOME, in the folder _.remaken\\packages\\_.\n\nimage::images\\QTProjectAppCreation4.png[Set XPCF version directory,600,600, align=\"center\", title=\"Set XPCF version directory\"]\n\n\nChoose your development kits. We recommend to use _MSVC 2017 64bit_ on Windows or _GCC_ on Linux.\n\n\nThen, no subproject to add, if you want to add a version control select it, and click on Finish. Your project is now created.\n\nThree files have been created. First the project file _MyStandalonePipeline.pro_, secondly a C++ file _MyStandalonePipeline_main.cpp_, and finally a _packagedependencies.txt_ to manage the dependencies of your application.\n\nBut let's take a closer look at these files.\n\n=== QTCreator project file\n\n.MyStandalonePipeline.pro\n[source]\n----\nQT       -= core gui\nCONFIG -= app_bundle qt\n\nTARGET = MyStandalonePipeline\nTARGET = %{ModuleName}\nFRAMEWORK = $${TARGET}\nVERSION=0.11.0\nDEFINES +=  $${TARGET}VERSION=\\\"$${VERSION}\\\"\n\nCONFIG += c++1z\nCONFIG += console\nCONFIG += shared\n\n# Uncomment following line to add more verbose information from builddefs-qmake rules\n# CONFIG += verbose\n# Uncomment following line to prepare remaken package\n# CONFIG += package_remaken\n\nCONFIG += with_qtvs\n\nwith_qtvs {\n    PROJECTCONFIG = QTVS\n}\n\ninclude(findremakenrules.pri) \n\nDEPENDENCIESCONFIG = sharedlib\nREMAKEN_PKGSUBDIR=shared\nDEPENDENCIESCONFIG += install_recurse \n\nCONFIG(debug,debug|release) { \n    DEFINES += _DEBUG=1\n    DEFINES += DEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/debug\n}\n\nCONFIG(release,debug|release) {\n    DEFINES += NDEBUG=1\n    REMAKEN_PKGSUBDIR=$${REMAKEN_PKGSUBDIR}/release\n}\n\npackage_remaken {\n    message(\"Preparing remaken package installation in $${REMAKEN_PKGSUBDIR}\")\n    INSTALLSUBDIR=$${REMAKEN_PKGSUBDIR}\n}\n\n## Configuration for Visual Studio to install binaries and dependencies. Work also for QT Creator by replacing QMAKE_INSTALL\nPROJECTCONFIG = QTVS \n\n#NOTE : CONFIG as staticlib or sharedlib, DEPENDENCIESCONFIG as staticlib or sharedlib and PROJECTDEPLOYDIR MUST BE DEFINED BEFORE templatelibbundle.pri inclusion\ninclude ($${QMAKE_REMAKEN_RULES_ROOT}/templateappconfig.pri)\n\n#DEFINES += BOOST_ALL_NO_LIB\nDEFINES += BOOST_ALL_DYN_LINK\nDEFINES += BOOST_AUTO_LINK_NOMANGLE\nDEFINES += BOOST_LOG_DYN_LINK\n\nHEADERS += \nSOURCES +=     MyStandalonePipeline_main.cpp\n\nunix {\n    LIBS += -ldl\n    QMAKE_CXXFLAGS += -DBOOST_LOG_DYN_LINK\n}\n\nmacx {\n    QMAKE_MAC_SDK= macosx\n    QMAKE_CXXFLAGS += -fasm-blocks -x objective-c++\n}\n\nwin32 {\n    QMAKE_LFLAGS += /MACHINE:X64\n    DEFINES += WIN64 UNICODE _UNICODE\n    QMAKE_COMPILER_DEFINES += _WIN64\n    QMAKE_CXXFLAGS += -wd4250 -wd4251 -wd4244 -wd4275\n\n    # Windows Kit (msvc2013 64)\n    LIBS += -L$$(WINDOWSSDKDIR)lib/winv6.3/um/x64 -lshell32 -lgdi32 -lComdlg32\n    INCLUDEPATH += $$(WINDOWSSDKDIR)lib/winv6.3/um/x64\n }\n\nINCLUDEPATH += $${PWD}\n\nDISTFILES +=     Makefile\n\nOTHER_FILES +=     packagedependencies.txt\n\nwith_qtvs {\n#NOTE : Must be placed at the end of the .pro\n    include ($${QMAKE_REMAKEN_RULES_ROOT}/remaken_install_target.pri) \n}\n----\n [white]#This .pri file has been installed by the wizard. It will allow to find the remaken folder depending on the OS you are using.#\n [white]#The dependencies of your application will be installed recursively. More details are available on the https://github.com/b-com-software-basis/builddefs-qmake/[builddefs-qmake] project on GitHub.#\n [white]#By default, your executable will be installed in the packages folder of remaken. If you want to install it locally in a common bin folder, add these 10 lines.#\n [white]#The installation of your application will also work with Visual Studio. Warning, in QTCreator, this will replace the usual QMAKE_INSTALL.#\n [white]#Place at the end the .pri file to install your application.#\n\nFinally, click on `_Projects_` in the left menu of QTcreator, click on _Run_, add a _Custom Executable_ run configuration set your working directory to the bin folder of your project, and check the box _Add build library search path to PATH_ if not already done.\n\n=== Package Dependencies file\n\ninclude::_dependencies_file.adoc[]\n\nRight now, your project is configured.\n\n== What about Visual Studio ?\n\nYou can load your _.pro_ file created with the Wizard in Visual Studio (see above for more details how to configure the _.pro_ file). If you do not want to install QTCreator, create manually your _.pro_ based of the above description.\n\nMicrosoft Visual Studio provides a Qt Visual Studio Tools. This enables developers to import QT project files (.pro) into Visual Studio.\n\nInstall QT Visual Studio Tools:\n\n* In Visual Studio, select *Tools > Extensions and Updates > Online* to install and update QT Visual Studio Tools.\n\nImport the _.pro._ file into Visual Studio:\n\n* Select *Qt VS Tools > Open Qt Project File (.pro)* and choose your _.pro_ file.\n\nRight now, your project is configured.\n\n== Create the configuration file\n\ninclude::_configuration_file.adoc[]\n\nIf you need to use new modules please refer to the > and to the > section to install them. If you need more third parties, please refer to the > section.\n\n== Implement your pipeline\n\nThis section describes how to assemble different components to build a vision pipeline.\nFirst as already mentioned in the previous sections, it is supposed that the required components have been identified. And consequently, a configuration file, a dependencies file and a project file have been created.\n\n=== Main template of a standalone pipeline\n\nTo create a standalone {cpp} pipeline, you can start by replacing the main.cpp code with the following one:\n\n.main.cpp\n[source, cpp]\n----\n///**\n * Add your header with the copyright and license information concerning your pipeline\n */\n\n// Common headers // \n#include \"xpcf/xpcf.h\"\n#include \"core/Log.h\"\n#include \n\n// ADD HERE: Component interfaces header. e.g. #include \"api/input/devices/ICamera.h\"\n// \n\n\n// Namespaces // \nusing namespace SolAR;\nusing namespace SolAR::datastructure;\nusing namespace SolAR::api;\n\nnamespace xpcf  = org::bcom::xpcf;\n\n// Main function\nint main(int argc, char *argv[])\n{\n\n#if NDEBUG // \n    boost::log::core::get()->set_logging_enabled(false);\n#endif\n    LOG_ADD_LOG_TO_CONSOLE(); // \n\n// Instantiate component manager and load the pipeline configuration file // \n    SRef xpcfComponentManager = xpcf::getComponentManagerInstance();\n    if(xpcfComponentManager->load(\"MyPipelineConfiguration.xml\")!=org::bcom::xpcf::_SUCCESS)\n    {\n        LOG_ERROR(\"Failed to load the configuration file MyPipelineConfiguration.xml\")\n        return -1;\n    }\n\n// ADD HERE: instantiate concrete components and bind them to abstract component interfaces\n    // e.g. SRef camera = xpcfComponentManager->resolve();\n    // \n\n// ADD HERE: Declare here the data structures used to connect components\n    // \n\n// ADD HERE: The pipeline initialization\n    // \n\n// ADD HERE: The pipeline processing\n    while (true)\n    {\n      // \n    }\n\n    return 0;\n}\n----\n [white]#The _xpcf_ header is required to instantiate components. _log_ header is recomended if you want to log your pipeline.#\n [white]#Add the component interface header files of the components used by the pipeline.#\n [white]#Add SolAR and XPCF namespaces directives to shorten the calls to SolAR api and datastructures.#\n [white]#Add this line to remove irrelevant logs in release mode.#\n [white]#Add this line to push logs in the console. You can also push logs to a log file by using the macro _LOG_ADD_TO_FILE(\"path/logfilename.log\", \"r\")_.#\n [white]#Create an instance of an XPCF ComponentManager and use it to load the configuration file of your standalone pipeline.#\n [white]#Instantiate concrete components embedded into modules thanks to the XCPF Component Manager. The implementation of the component is automatically resolved according to the factory field defined in your configuration file. Here, an OpenCV camera will be instantiated if you request a component of type ICamera. Thanks to that, swapping a component by another one will just consists of editing your configuration file.  More details are given in the next section >#.\n [white]#Declare all the data structures used to exchange data between components. Have a look to the data structures defined in the SolARFramework in the > section.#\n [white]#if required, add the code to initialize your pipeline (e.g. start a camera, load a reference image, etc.).#\n [white]#Create the loop of your pipeline by calling the different processing functions of your components with data structure as input and/or output attributes. More details on how to call a function of a component are given in the next section >.#\n\n=== Instantiate a component [[InstantiateComponent]].\n\nThanks to XPCF, instantiation of a component is very easy and this operation is done at run-time. The configuration of the components will be initialized with values declared in the configuration file.\nThe syntax is the following for e.g. a component that display an image in a window:\n\n[code, cpp]\n----\nSRef imageViewer = xpcfComponentManager->resolve();\n----\nAs you have only one implementation of a _IImageViewer_ in your configuration file (defined in the OpenCV module), it will be instantiate automatically.\n\nIf you want to use several implementations of the same component in your pipeline, you can define in your configuration file different binding with corresponding names.\nYou can then ask to instantiate a dedicated component implementation thanks to this binding name:\n[code, cpp]\n----\nSRef videoAsCamera = xpcfComponentManager->resolve(\"VideoAsCamera\");\n----\n\n=== Use a component.\n\nOnce a component is created, any public function  described in its API can be used to build your pipeline.\nFor instance :\n```\nif (viewerConfImage->display(image) == FrameworkReturnCode::_STOP )\n```\n\n=== Pipeline sample\n\nWe will present next the implementation of the simplest pipeline that consists in capturing an image from a camera and display it in a window. For this implementation, we will need only one module: _SolARModuleOpenCV_.\n\n- as the example contains components that implement virtual interfaces, include the corresponding header files\n```\n#include \"api/input/devices/ICamera.h\"\n#include \"api/display/IImageViewer.h\"\n```\n\n- as the modules/components to be used are listed in a configuration file (xml), don't forget to load it via XPCF :\n```\nxpcfComponentManager->load(\"MyStandalonePipelineConfiguration.xml\")\n```\n\nHere the full sample code for this standalone pipeline:\n\n\n[source,c]\n----\n///**\n * Add your header with the copyright and license information concerning your pipeline\n */\n\n// Common headers\n#include \"xpcf/xpcf.h\"\n#include \"core/Log.h\"\n#include \n\n// Component interfaces header. e.g. #include \"api/image/IImageLoader.h\"\n#include \"api/input/devices/ICamera.h\" // \n#include \"api/display/IImageViewer.h\"\n\n// Namespaces\nusing namespace SolAR;\nusing namespace SolAR::datastructure;\nusing namespace SolAR::api;\n\nnamespace xpcf  = org::bcom::xpcf;\n\n// Main function\nint main(int argc, char *argv[])\n{\n\n#if NDEBUG\n    boost::log::core::get()->set_logging_enabled(false);\n#endif\n    LOG_ADD_LOG_TO_CONSOLE();\n\n// Instantiate component manager and load the pipeline configuration file\n    SRef xpcfComponentManager = xpcf::getComponentManagerInstance();\n    if(xpcfComponentManager->load(\"MyStandalonePipelineConfiguration.xml\")!=org::bcom::xpcf::_SUCCESS)\n    {\n        LOG_ERROR(\"Failed to load the configuration file MyStandalonePipelineConfiguration.xml\")\n        return -1;\n    }\n\n    // declare and instantiate components // \n    auto camera = xpcfComponentManager->resolve();\n    auto imageViewer = xpcfComponentManager->resolve();\n\n    if (!camera || !imageViewer)\n    {\n        LOG_ERROR(\"One or more component creations have failed\");\n        return -1;\n    }\n\n// Declare here the data structures used to connect components\n    SRef image; // \n\n// The pipeline initialization\n\n    // start the camera // \n    if (camera->start() != FrameworkReturnCode::_SUCCESS)\n    {\n        LOG_ERROR (\"Camera cannot start\");\n        return -1;\n    }\n\n// The pipeline processing // \n    while (true)\n    {\n        if(camera->getNextImage(image)==SolAR::FrameworkReturnCode::_ERROR_)\n            break;\n        if (imageViewer->display(image) == FrameworkReturnCode::_STOP )\n            break;\n     }\n    return 0;\n}\n----\n [white]#As the example assemble two components, a camera and an image viewer, we include the corresponding component interface header files.#\n [white]#We instantiate our two components based on openCV implementations and we bind them to their abstract component interfaces.#\n [white]#We declare a shared reference of a SolARImage that will be used to pass the image captured by the camera to the image viewer.#\n [white]#We initialize the pipeline by starting the camera. In our configuration file, you will find for the camera first an ID corresponding to the camera you want to start, and secondly a path to a json camera description file defining its intrinsic parameters. This file can be generated by calibrating the camera (see > section for more information).#\n [white]#Finally, we implement the core of the pipeline by capturing the current image from the camera, and by passing this image to the image viewer.#\n\nIMPORTANT: Don't forget to re-run qmake before building your pipeline for QT Creator and re-import _.pro_ file for Visual Studio.\n\nAs there is no generic way to implement a pipeline, we encourage the readers to take a look at the many examples provided by SolAR in the Samples directory that comes with the installation of SolAR.\nFor each sample, you will find a configuration file, a dependencies file and a project file to help you to build your own pipeline together with sources codes.\n",
      "id": 29
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "Tags",
      "content": "Tags\n\n{% capture site_tags %}{% for tag in site.tags %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endfor %}{% endcapture %}\n\n\n{% assign tag_words = site_tags | split:',' | sort %}\n\n\n\n\n\n  {% for item in (0..site.tags.size) %}{% unless forloop.last %}\n    {% capture this_word %}{{ tag_words[item] }}{% endcapture %}\n    \n      {{ this_word }}\n        ({{ site.tags[this_word].size }})\n      \n    \n  {% endunless %}{% endfor %}\n\n\n\n\n  {% for item in (0..site.tags.size) %}{% unless forloop.last %}\n    {% capture this_word %}{{ tag_words[item] }}{% endcapture %}\n    {{ this_word }}\n    {% for post in site.tags[this_word] %}{% if post.title != null %}\n      \n        \n          {{ post.title }}\n        \n        \n          {{ post.date | date_to_string }}\n        \n      \n      \n    {% endif %}{% endfor %}\n  {% endunless %}{% endfor %}\n\n",
      "id": 30
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "module tests",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: module tests\n:page-description: module tests\n:page-layout: default\n:page-category: create\n:page-liquid:\n:toc:\n\n== Tests\n\n== Tests\n\nDefine unit tests, based on boost framework .\nYou can know more about boost with this link : http://www.boost.org/[http://www.boost.org,role=\"external\", window=\"_blank\"]\n\nYour Unit tests for a specific component, should be placed in the component directory/unittest.\nIf you have used the SolARComponent template, this directory should be already there.\n\nimage::images\\400px-Unittestdirectory.png[Unit test directory]\n\n* Open \\{yourcomponent}/unittest/\\{yourcomponent}unittest.pro\n\n* You have to describe your unit tests in the file  \\{yourcomponent}unittest.cpp\n\nFor example\n\n[source,cpp]\n----\n#define\nBOOST_TEST_MODULE \\{Yourcomponent}UnitTest\n\n#include  \n#BOOST_AUTO_TEST_CASE(TestLoadImage) \n{ // test execution instructions\n}\n----\n\n [white]#Please note your code contains include of boost#\n [white]#You have to define the *name* of your test thanks to the boost macro \"BOOST_AUTO_TEST_CASE\"#.\n\nIn this example , the definition of the test case \"TestLoadImage\" for your component.\n\nIt means, that when you will execute the unit test, it will executes this test \"TestLoadImage\" following the instructions in this declaration.\nYou can define several test cases.\n\nInside your test, please write a kind of demo main function, but where you check results of your component function thanks to macro BOOST CHECK and/or BOOST_TEST.\n\nYou will find easily information about BOOST macro on Internet http://www.boost.org/doc/libs/1_64_0/libs/test/doc/html/index.html [boost.org information,role=\"external\", window=\"_blank\"] .\n\nNOTE: There is no \"main\" function, as it is automatically generated by boost (used in unit tests).\n\n* Please ensure that it contains sufficient tests cases to verify your code is OK (normal case, error cases).\n\nExample here : // Case normal, with an existing image file.\n\nWARNING: remplacer ici par le nouveau code source SolAR\n[source,cpp]\n----\nBOOST_AUTO_TEST_CASE(TestLoadImage)\n{ // To simplify this example test, let's suppose we'll test 'float'.\n // Some test are stupid, but all should pass.\n int result= 0;\n std::shared_ptr myArgoImage0 = getArgoImageInstance();\n\n....\nBOOST_CHECK( myArgoImage0 !=  NULL);\n....\n\n// getArgoImageInstance should not return a null pointer result\n\nmyArgoImage0->LoadImage(\"../test.jpg\");\nBOOST_TEST( result= = 0,\"ARGO ERROR: Load Image should return 0\");\n// As the image indicated exists, loadImage should return 0, as a normal case\n\n}\n\nBOOST_AUTO_TEST_CASE(TestLoadImageInexistante)\n{\n// Some test are stupid, but all should pass.\nint result= 0; std::shared_ptr\nmyArgoImage0 =  getArgoImageInstance();\n\n....\nBOOST_TEST(( myArgoImage0 !=  NULL),&quot;ARGO ERROR: ArgoImage should not return null pointer&quot;);\n\nresult=  myArgoImage0-&gt;LoadImage(&quot;../test2.jpg&quot;);\nBOOST_TEST( result= = -1,&quot;ARGO ERROR: Load Image should return -1&quot;);\n....\n\n// As the image indicated does not exist, loadImage should return -1, an error\n\n }\n----\n",
      "id": 31
    });
    
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "tools",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: tools\n:page-description: tools\n:page-layout: default\n:page-category: use\n:page-liquid:\n:toc:\n:toclevels: 1\n\n== Tools\n\n\n== Camera Calibration [[CameraCalibration]]\n\n=== Introduction\n\nSolAR provides a program based on  https://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html[\"OpenCV\",role=\"external\", window=\"_blank\"] that can be used to calibrate your camera. The source code is available in the https://github.com/SolarFramework/SolARModuleOpenCV[\"SolARModuleOpenCV repository\",role=\"external\", window=\"_blank\"] (in the tools folder), and a executable is available in the https://github.com/SolarFramework/SolARModuleOpenCV/releases[\"releases\",role=\"external\", window=\"_blank\"] of this OpenCV module.\n\nThis tutorial is a simplified version of the one provided by OpenCV, feel free to visit OpenCV website to get details on the actual implementation. The calibration process uses a chessboard pattern like the one below. \n\nimage::..\\use\\images\\chessboard.png[chessboard,400,400, align=\"center\"]\n\n===  Tutorial\n\n==== Installation + \nDownload and unzip the camera calibration tool for https://github.com/SolarFramework/SolARModuleOpenCV/releases/download/CalibrationTool_0_10_0/CameraCalibration_win.zip[\"windows\",role=\"external\", window=\"_blank\"] or https://github.com/SolarFramework/SolARModuleOpenCV/releases/download/CalibrationTool_0_10_0/CameraCalibration_linux.zip[\"linux\",role=\"external\", window=\"_blank\"]\n\n==== Configuration +\nIn the unzipped folder, you find a _calibration_config.yml_ file that contains information about the chessboard size, its square size, aspect ratio, number of frames used for calibration, OpenCV's flags and delay time between each frame. You can configure all these parameters corresponding to your use case. + \n[source,yml]\n----\n# the number of inner corners on board width\nchessboard_width: 9\n# the number of inner corners on board height\nchessboard_height: 6\n# square size in some user-defined units\nsquare_size: 0.026\n#fix aspect ratio (fx/fy)\naspect_ratio: 1\n# number of frames to calibrate the camera\nnb_frames: 10\n# OpenCV Flags for camera calibration\nflags: 0\n# delay between each frame in milliseconds\ndelay: 2000\n----\n\n\n==== Input preparation +\nFirstly, print the chessboard pattern file in the installation folder, and update the _calibration_config.yml_ according to the realse size of your printed chessboard. +\nThen this tool allows to calibrate a camera by using its index when plugged in, a video file or an image sequence. Please configure your input data in the _SolARTool_CameraCalibration_conf.xml_ file. You can also choose the resolution of the camera that you want to calibrate, if this resolution is supported, by modifying `_width_` and `_height_` parameters in the _camera_calibration.json_ file. \n\n==== Execution +\nRun the execution file.\n************\nWindows:\n....\nSolARTool_CameraCalibration.exe\n....\n\nLinux:\n....\n./run.sh ./SolARTool_CameraCalibration\n....\n************\n\nWARNING: Parameters are passed with an equal operator (e.g. `_SolARTool_CameraCalibration -i=1`).\n\nWhen launching this tool, it will show the following window. Press the `_g_` key to start the process.\n\nimage::images/CameraCalibration/run.jpg[]\n\nA number of positive detections will be taken to calibrate.\nA positive detection is verified when the chessboard is correctly detected. This is illustrated with a frozen picture displaying corners and lines as below.\n\nimage::images/CameraCalibration/detect.jpg[]\n\nPlease, notice the bottom-right counter that indicates the number of positive detections so far.\n\nWhen all the positive detections are obtained, the calibration is performed and the process is completed, then a bottom-right message indicates `_Calibrated_`.\n\nimage::images/CameraCalibration/completed.jpg[]\n\nBy default, an output calibration file has been generated in _json_ format (_camera_calibration.json_) in the same folder. We illustrate an example of the calibration file as follows.\n\n[source,json]\n----\n{\n    \"CameraParameters\": {\n        \"name\": \"Logitech C920\",\n        \"id\": 0,\n        \"type\": 0,\n        \"resolution\": {\n            \"width\": 640,\n            \"height\": 480\n        },\n        \"intrinsic\": [\n            6.2358844756875726e+02, 0, 3.19501379528701e+02, \n            0, 6.2510924611650637e+02, 2.395453191051286e+02, \n            0, 0, 1\n        ],\n        \"distortion\": [\n            5.0406145631272294e-03, -7.3194070034412229e-01, 8.8401137738982200e-03, \n            -4.1912068994392751e-03, 2.7609935737342024e+00\n        ]\n    }\n}\n----\n\nThis file can be used directly in SolARModuleOpenCV tests and samples if a calibration file is required.\n\n\n=== Illustration Video\nTo better calibrate, the calibration process requires capturing the chessboard pattern at different poses of the camera. This is illustrated in the following video.\n\nvideo::../videos/CameraCalibration/CameraCalibration.mp4[width=640px, options=\"\"]\n",
      "id": 32
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "unity",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: unity\n:page-description: unity\n:page-layout: default\n:page-category: use\n:page-liquid:\n:toc:\n:imagesdir: ../images\n\n== Unity [[Unity]]\n\n== Unity\n\nIntegrating SolAR pipelines in Unity Engine gives opportunities to developers of\nAR services to get access to fully configurable SolAR-based solutions to design AR applications. +\n\nSolAR has now a *Plugin for Unity*\n\n== Install Unity\n\n* Refer to https://store.unity.com/download?ref=personal\n* *Download the last version of Unity Hub*. The hub gives you the possibility to get access to different versions of Unity.\n* Our current version (our tests are based on) is **2019.4.15f1**\n\n== Create a new Unity project\n\n* *New project*\n\n[#img-newunity]\nimage::../use/images/newunity.png[NewUnityProject,450,250,align=\"center\"]\n\n[.text-center]\n*Interface to create a new Unity project*\n\nWARNING: It's recommended that you use a 3D project set-up.\n\n* *Documentation*\n\n** All documentation for Unity is available here : https://docs.unity3d.com/2019.4/Documentation/Manual/\n\nNOTE: If you are new to Unity, check online tutorials to help you.\n\n== SolAR Plugin for Unity\n\n=== For whom ?\n\nFor users who want to simply design AR services: Choose a pipeline, configure it in the inspector, and experience AR at run-time. +\n\n    Easy to use, understandable for everyone.\n\n=== Download\n\n* go on https://github.com/SolarFramework/SolARUnityPlugin/releases/tag/0.9.1\n* click on _SolARUnityPackage.unitypackage_\n* the file is downloaded, all you have left to do is to import it into Unity\n\nThis package provides user with the minimum requirements to load a SolAR pipeline in Unity and includes three pipelines (Fiducial marker, natural image marker, and a SLAM).\n\n=== Project Structure\n\nThe plugin version will add the following hierarchy in the Assets folder of your unity project:\n\n* `_Plugins_`: Host the binaries of the SolAR Framework, of the the pipeline manager that will handle the load of SolAR pipelines as well as the relative third parties (boost and xpcf).\n* `_SolAR_`\n** `_Demos_`\n*** `_Materials_`: Materials used to display the video stream of the camera in the background.\n*** `_Objects_`: Some 3D objects to augment the scene.\n*** `_Prefabs_`: All the prefabs required to simply use SolAR in Unity.\n*** `_Scenes_`: Sample scenes for simply testing the SolAR plugin.\n*** `_Scripts_`: UI-related scripts for the demo.\n** `_Editor_`: scripts to control the pipeline manager appearance in the Inspector and customize build process for various platforms.\n** `_Pipelines_`: The xml configuration files for the different SolAR pipelines. It is mandatory to put these pipeline configuration file in or under this folder.\n** `_Scripts_`: various script to be able to use SolAR pipelines in Unity.\n*** `_Swig_`: C# wrapper classes to expose SolAR native API, generated from C++ with SWIG.\n* `_StreamingAssets_`\n** `_SolAR_`\n*** `_Android_`: contains a file that lists all assets. Used by Android build to install app properly.\n*** `_CameraCalibration_`: Some camera calibration files used by the pipelines handling their own camera.\n*** `_FBOW_`: vocabulary files used by the SLAM pipeline.\n*** `_Markers_`: files describing the markers. You can edit these files to provide the exact size of the markers defined in meters, or create them to add new markers.\n\n=== How does the SolAR Plugin for Unity work ?\n\nThe SolAR plugin for Unity is a simple interface to load any SolAR pipelines assembled by the SolAR pipeline assemblers and use them to design AR services with Unity. +\n +\nTo load a SolAR pipeline in Unity, simply drag and drop the _SolARPipelineLoader_ prefab (in SolAR/Prefabs/novice) in your scene hierarchy. The _SolARPipelineLoader_ has the following parameters:\n\n* *Camera*: is the virtual camera whose position and orientation will be automatically updated at runtime according to the camera pose computed by the SolAR pipeline.\n* *Use Unity Webcam*: Check it if you want to replace the camera used by the pipeline by a Unity camera. If checked, you will be able to select a camera among those available on your computer. For a better experience, you can set the intrinsic parameters of the camera. For that, you will need to calibrate it (see >).\n* *Custom_Canvas*: For video see-through AR, the system has to display the video stream of the camera on background. If unchecked, a Canvas with the video stream of the selected camera will be automatically created when the application will start. If you want to manage your canvas by yourself, you can check this option and drag and drop your canvas and its material.\n* *Select Pipeline Folder*: A button to select the folder where the available SolAR pipelines are stored in your Unity project. These pipelines must be stored anywhere under the folder _Assets/SolAR/Pipelines/_.\n\nSet your _SolARPipelineLoader_ by dragging and dropping your main camera from your scene hierarchy to the _Camera_ parameter in the _SolARPipelineLoader_ inspector. +\n\nThen you can put any object in your scene hierarchy to visualize it in augmentation (you can drag and drop the SolAR can available in the SolAR objects). The camera will automatically move relatively to the coordinate system of your marker (positionned in the center of the marker, x-axis pointing to the left, y-axis pointing to the top, z-axis pointing backward), so place your object in (0,0,0) if you want it to be positionned in the center of your marker. +\n\nIMPORTANT: For each object with a renderer, add the TAG _SolARObject_ to make it disappear when the pose provided by the pipeline is wrong.\n +\n\n=== Test SolAR pipelines\n\nSimply load the sample scene _Assets/SolAR/Demos/Scenes/NoviceVersion_.\n\nNow you can choose a SolAR pipeline among the fiducial, the natural image or the SLAM:\n\n* Select the _SolARPipelineLoader_ object in your scene hierachy.\n* Select the _PipelineFiducialMarker_ in the drop-down menu under the button _Select Pipelines Folder_.\n\nYou will see a list of parameters relative to the different components of the selected pipeline. A tooltip provides user with information relative to each parameter when he move the pointer over the parameter (only if the configuration file of the pipeline defines descriptions for the configuration parameters).\n\nFinally, you can print the following fiducial marker, play the application, and present the marker in front of the camera. That's it, you should visualize the SolAR robot appearing on your fiducial marker.\n\nimage::../use/images/FiducialMarker.gif[FiducialMarker,450,250,align=\"center\"]\n\nNow, stop the application, select the _SolARPipelineLoader_ object in your scene hierarchy, and select the _PipelineNaturalImageMarker_. Print the following image marker, and play the application. Changing pipeline is as simple as that ! +\n\nimage::../use/images/graf1.png[NaturalImageMarker,450,250,align=\"center\"]\n\nFinally, you can test the SLAM pipeline. To initialize the SLAM, you will need to target the fiducial maker with your camera. When it is done, the SolAR robot should appear. Here, as the SLAM pipeline build a 3D map to localize your camera, it does not matter if the fiducial marker does not remain in its field of view.\n\nWARNING: For a better AR experience, it is preferable to enter the actual size of the marker in meters. You can do it by editing the marker files available in the folder _Assets/StreamingAssets/Markers/_. By editing these files, you will also be able to change the fiducial marker pattern or the image of reference for the natural image marker pipeline.\n\n////\n// Hide deprecated documentation about export mode\n\n== Full SolAR C# wrapper (deprecated)\n\n=== For whom ?\n\nFor vision expert who want to directly assemble SolAR pipeline in Unity thanks to a full C# wrapping of SolAR.\n\n* go on https://github.com/SolarFramework/SolARUnityPlugin/releases\n* click on _Assets_\n* click on _SolARPlugin_win.unitypackage_\n\n=== Project Structure\nThe unitypackage adds our project’s hierarchy in your Unity project. +\n\n\n* `_Objects_` : our 3D object\n* `_Plugins_` : our DLLs from SolAR\n* `_Scenes_` : Expertversion scene\n* `_SolAR_`\n*** `_Editor_`\n**** `_SolARPluginExpert_' : own Editor inspector for our PipelineLoader\n*** `_Materials_`\n*** `_Pipelines_`\n*** `_Scripts_`\n**** `_SolARPluginExpert_` : all our C# scripts\n***** `_Samples_` : pipelines examples\n*** `_Shaders_`\n*** `_Swig_`\n**** `_SolARPluginExpert_` : full wrapping of SolAR\n* `_StreamingAssets_`\n*** `_CameraCalibration_` : `_design for our cameras_`\n*** `_Configuration_` : xml files for our pipelines\n*** `_Markers_` : files describing our markers. Print our markers or make yours.\n\n[.text-center]\n**Our Scene, our project's folders and some explanations*#\n[#img-expertEditor]\nimage::../use/images/expertEditor.png[NewbieScene,800,300, align=\"center\"]\n\n=== Make your own pipelines\n\nWARNING: You have to be in Unity Editor with Expert version package imported.\n\nNOTE: We give you pipelines examples in Scripts/SolARPluginExpert/Samples +\n\n*To create a new pipeline :* +\n +\n\n* Create a new script C* \"MyPipeline.cs\"\n* Add *_using properties_* in your file that give you components available. (see example code)\n\n using SolAR.Datasctructure  //our Datasctructure\n using XPCF.Api              //XPCF is the heart of SolAR\n\n* Put your class in *_namespace_* *SolAR.Samples*\n\n namespace SolAR.SAmples {\n  class MyPipeline\n }\n\n* Your class have to base upon AbstractPipeline (C# interface)\n\n class MyPipeline : AbstractPipeline\n\n** This inheritance means you must use different methods.\n\n*** *A constructor* taking a IComponentManager as parameter. +\n    This constructor is the place where you instantiate your components.\n\n*** *GetMarkerSize()* method to define your marker size and use it at the basis of your pipeline.\n\n*** *SetCameraParameters(...)* method to give intrinsic and distorsion parameters to certain components.\n\n*** *Proceed* function. Core function of a pipeline where you have to connect components to each others.\n\nNOTE: Components have to be declare with *_readonly_* property\n\nWARNING: You have to create a new XML files to fix your pipeline configuration. +\n         Examples in StreamingAssets/Configuration.\n\n// Hide deprecated documentation about export mode\n////\n\n== Troubleshooting\n\n=== I have selected another pipeline but the application still uses the old one\n\nThis might mean your selection hasn't been saved. This is a known issue that remain to be fixed.\n\nA workaround is to select a value in a pipeline parameter, erase it, write the same value again, and then save.\n\n=== The camera viewport is blank on Android\n\nWhen targetting Android, you *MUST* check the *Use Unity Webcam* box. Let the camera selected by default in the dropdown menu.",
      "id": 33
    });
    
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be includedt items that are from subpages (title ==null), because we don't know in wich (potentially) multiples page(s) these supages can be included
    index.add({
      "title": "on windows",
      "content": ":page-layout: _auto\n:showtitle:\n:page-title: on windows\n:page-category: install\n:page-description: on windows\n:page-liquid:\n:page-layout: default\n:toc:\n:solar-version: 0.11.0\n:solar-version_: 0_11_0\n\n== Install On Windows\n\n== Install development environment [[InstalDevEnv]]\n\nThe SolAR framework uses a dedicated pipeline to link and compile code as well as internal tools for third party downloads which should make your job much easier.\nAs you will see later, the SolAR framework is based on _QMake_ originally created by the Qt Company, but is compliant with most IDE including Visual Studio.\n\n=== Install your IDEs\n\n==== Visual Studio\n\nYou will have to install Visual Studio to get the corresponding compiler and debugger on your machine. The community version 2017 has been tested and is available on the following link: link:++https://my.visualstudio.com/Downloads?q=visual%20studio%202017&wt.mc_id=o~msft~vscom~older-downloads++[Visual Studio installer]\n\nWe recommend to install at least the following features:\n\n* Desktop Development C++\n* Game development with Unity (useful to develop and debug Unity applications)\n* Mobile development in C++ (to build for Android)\n\n==== QT Creator\n\nAs SolAR is using QMake file to setup the projects, we highly recommend to install _Qt Creator_. Moreover, you will have access to wizards which will help you create new SolAR modules, components and pipelines.\n\nDownload the latest version of QTCreator on the following link (scroll down to choose \"Downloads for open-source users\" for open-source development):\n\nhttps://www.qt.io/download[Qt installer download]\n\nCreate a _Qt_ account if not already done, select where you want to install _Qt_. You don't need to install _Qt_ SDK, just the _Qt Creator_. So just check the following components (if not available during installation, search for `QT maintenance tool` or `QT uninstall` after installation to launch the QT tools to upgrade it):\n\n* Preview\n** QT creator\n** QT Creator CDB Debugger Support\n* Qt \n** Qt\n*** with MSVC 2017 64-bit\n*** Android (if you want to build SolAR for Android)\n** Developer and Designer Tools\n*** Qt Creator CDB Debugger Support\n*** Debugging Tools for Windows\n*** CMake 64-bit\n\nand start the installation.\n\n==== QT Visual Studio Tool\n\nIf you are more comfortable with Visual Studio, you will have to install the Visual Studio plugin called _QT Visual Studio Tool_ which will allow you to open Qt projects. To do so, Open Visual Studio, click on `_Tools_` menu, then on `_Extensions and Updates_`. Search for _Qt Visual Studio Tools_, and install it. To complete this installation, you must restart Visual Studio.\n\n=== Install dependencies\n\nTo download dependencies, the SolAR framework uses the meta dependencies management tool https://github.com/b-com-software-basis/remaken[Remaken] supporting https://conan.io/[Conan], https://github.com/microsoft/vcpkg[VCPKG] and its native C++ packaging structure based on https://www.freedesktop.org/wiki/Software/pkg-config/[pkg-config] description files.\n\n==== Install Remaken [[InstallRemaken]]\n\n    * Start by downloading and installing the latest version of _Remaken_ available on the following link: https://github.com/b-com-software-basis/remaken/releases/[Remaken installer]\n    * Launch the remaken installer *with administrator rights*. By default, the dependencies will be installed in your `C:/Users/%Username%/.remaken/packages` directory, but you can change it by setting another location for your _REMAKEN_PKG_ROOT_ environment variable. By default, Remaken will install Conan, pkg_config and CMake which will be used afterwards. Conan will check these remotes for searching your dependencies.\n    * Set XPCF_MODULE_ROOT environment variable to `C:/Users/%Username%/.remaken/packages/win-cl-14.1` (if _REMAKEN_PKG_ROOT_ is set to its default value)\n    * SolAR has its own conan remote to store pre-built dependencies for the common configurations used by the SolAR Framework. You will need to add this conan remote with the following command in a command prompt:\n\n----\nconan remote add conan-solar https://artifact.b-com.com/api/conan/solar-conan-local --insert 0\nconan remote add conan-bcom https://artifact.b-com.com/api/conan/bcom-conan-local --insert 1\n----\n\nYou should obtain a list of conan remotes that looks like this:\n----\n> conan remote list\nconan-solar: https://artifact.b-com.com/api/conan/solar-conan-local [Verify SSL: True]\nconan-bcom: https://artifact.b-com.com/api/conan/bcom-conan-local [Verify SSL: True]\nconancenter: https://center.conan.io [Verify SSL: True]\n----\n\n    * Then, open a command prompt *with administrator rights* and run:\n\n----\nremaken init -f -e\n----\n\nThis command will install the latest the qmake building rules in your `C:/Users/%Username%/.remaken/rules` folder.\n\nFinally, you need to configure your remaken profile according to your development environment. To do so, run the following command in a command prompt:\n\n----\nremaken profile init --cpp-std 17 -b cl-14.1 -o win -a x86_64\n----\n\n// TIP: You can install another version of the qmake rules by passing it with _--tag_ parameter, but we recommend to install the default one..\n\n==== Set conan profile\n\nAs we are using conan to download dependencies, you need to configure your default conan profile. To do so, open the default file available in your `C:/Users/%Username%/.conan/profiles`, and replace the configuration by the following one before saving it:\n----\n[settings]\nos=Windows\nos_build=Windows\narch=x86_64\narch_build=x86_64\ncompiler=Visual Studio\ncompiler.cppstd=17\ncompiler.version=15\nbuild_type=Release\nceres-solver:build_type=Release\nceres-solver:compiler.runtime=MD\n[options]\n[build_requires]\n[env]\n----\n\nNOTE: ceres_solver package has problems with glog in debug mode, that is why Release build_type is forced for it in the conan profile.\n\nNow, you are ready to download your dependencies with Remaken.\n\n==== Install CUDA\n\nIf you want to build or run SolAR pipelines with CUDA optimizations, you will need to install CUDA.\n\nFollow the Nvidia instructions available here: https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\n\nYou will also need to install cuDNN to run deepl learning inference by following the Nvidia instructions: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows\n\n==== Use Remaken\n\nRemaken uses a file called _packagedependencies.txt_ to describe which depedencies to install, in which version, where to install them, where to download them, with which package manager and with which configuration. +\n\nA global _packagedepedencies.txt_ defining the common dependencies with specific _packagedependencies-.txt_ files defining dependencies which are specific for each os are available in the parent GIT repository _SolAR_, and can be downloaded on the following link:\n\n* https://raw.githubusercontent.com/SolarFramework/SolAR/release/0.11.0/packagedependencies.txt[packagedependencies.txt] \n* https://raw.githubusercontent.com/SolarFramework/SolAR/release/0.11.0/packagedependencies-win.txt[packagedependencies-win.txt]\n\nCopy these files where you want on your computer, open a terminal in the folder where you have copied the _packagedependencies.txt_ and _packagedependencies-.txt_ files, and run remaken with the following command:\n\n----\nremaken install packagedependencies.txt\n----\n\nThis command will install all SolAR dependencies in release mode in your _C:/Users/%Username%/.remaken/packages_ folder. You can go and have a cup of coffee while it's downloading.\n\nTo download the dependencies in debug mode, run the following command:\n\n----\nremaken install -c debug packagedependencies.txt\n----\n\nCAUTION: You do not need to launch `remaken install` on `packagedependencies-win.txt`. `remaken install packagedependencies.txt` will automatically load it depending on the platform you are executing it.   \n\nThis is done, all your dependencies are downloaded and ready to use !\n\nSome of the module will download and build third parties using Conan which requires CMake (minimum version 3.10).\n\n== Build SolAR\n\ninclude::installClone.adoc[]\n\n=== Build with scripts\n\nFor Windows, build scripts are available in the _scripts_ folder.\nJust run _build_all.bat_ to rebuild the SolAR framework, the modules and their tests, the pipelines and their tests, the samples, the services and their tests. If you want to rebuild them seprately, use the corresponding script. Check with _build_all.bat --help_ the different options such as the cross-build for Android, the number of processors to use, the version of Qt, the path to SolAR root folder, etc.\n\nCAUTION: _build_all.bat_ will not build modules PopSift, FBOW and OpenCV with CUDA optimization. To do it, launch _build_allModulesCuda.bat_.\n\n=== Build on Qt Creator\nCounting the framework, the pipeline manager, modules, module tests, samples, pipelines, pipeline tests, there are more than 60 QT projects on GitHub. In order to ease the building of all these projects, they are grouped in the following parent QT projects available in the root folder of SolAR:\n\n* SolARCore\n* SolARAllModules\n* SolARAllModulesTests\n* SolARAllSamples\n* SolARAllPipelines\n* SolARAllPipelineTests\n* SolARAllServices\n* SolARAllServicesTests\n\nYou can open one of them or all in QT Creator.\n\nCheck by clicking on the `_Projects_` tab, and then on the `_Manage Kits..._` button that your _QT MSVC2017 64bit_ kit is well configured (Microsoft Visual C++ Compiler should be in version _15.x.x.x (x86_amd64)_ if your are using Visual Studio 2017). +\n\nCAUTION: If you have not yet run a qmake on each project, some error could appear concerning the fact that `conanbuildinfo.pri` files do not exist. This is normal, `run qmake` on each project will create these files, and the error message will no longer appear. \n\nWARNING: Since QT Creator 4.14.0, we highly recommand to set the project option `qmake system() behavior when parsing` to `Ignore` to highly reduce the project loading time. But do not forget to `run qmake` on projects before building them.\n\nimage::images\\Install_args_Windows.png[Add install argurment,800,800, align=\"center\", title=\"Set `qmake system() behavior when parsing` to `Ignore` for Debug and Release mode.\"]\n\ninclude::projectDependencies.adoc[]\n\nThen, in the `Build` menu, click on `Run qmake` on each project (right click on the parent project in the projects view) , and when it is done, click on `Rebuild All Projects for All Configurations`, and go get a cup of coffee.\n\nNOTE: When using visual compiler on QT creator, you do not need to add a make step to install built binaries. In the children project files (.pro), you will find a line with `PROJECTCONFIG = QTVS`. This configuration will automatically manage installation step according to the argument set to variable `DEPENDENCIESCONFIG`, and it also works for visual studio. For more information, see the documentation of the builddefs-qmake project on https://github.com/b-com-software-basis/builddefs-qmake[GitHub].\n\n=== Build on Visual Studio\n\nYou have first to set the path of Qt for _Qt Visual Studio Tool_. Click on the `_Qt Vs Tools_` tab, and then on `_Qt Options_`. Add a new Qt Version (for instance name: _5.14.1 MSVC207_64_, path: _C:\\Qt\\5.14.1\\msvc2017_64_). +\nWhen this is done, simply open the Qt parent projects by cliking on the `_Qt VS Tools_` tab, and on `_Open Qt Project File (.pro)_`. Then build your projects. It's done!\n\n== Q&A\n\n=== Conan Error when installing dependencies: PermissionError: [WinError 5]\nSome Conan recipes are renaming the unzipped folders embedding the source code of the dependence as soon as they are downloaded locally. Unfortunately, your antivirus or the Windows Search Indexing can lock it. We recommend to deactivate Windows Search Indexing on Conan folders, and check your antivirus to avoid any control after unzipping the conan package. If you still have a problem, you will have to add __time.sleep()__ before renaming your unzipped folder in your problematic conan recipes (see the following thread for more details: https://github.com/conan-io/conan/issues/5205).\n\n=== Remaken does not work\nBe sure to install, init and use Remaken from a command prompt with admin rights.\nYou can check that _cmake_, _pkg-config_ and conan are well installed with the following command:\n\n----\ncmake -version\n----\n\n----\npkg-config --version\n----\n\n----\nconan -v\n----\n\nIf not, sometimes the remaken setup encounters conflicts with a version of Python already installed on your machine. If it is the case, install cmake by your own with pip:\n\n----\npip install cmake\n----\n",
      "id": 34
    });
    
  

  

 
// builds reference data
var store = [];
 

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "GIT",
      "link": "/community/GIT/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "About",
      "link": "/about/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "for Android development",
      "link": "/install/android/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "api",
      "link": "/create/api/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "services",
      "link": "/documentation/arcloud_services/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "best practises",
      "link": "/community/best_practices/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "ARCloud platform",
      "link": "/documentation/cloud_architecture/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "create a component",
      "link": "/create/component/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "contribution workflow",
      "link": "/community/contribution_workflow/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "C++",
      "link": "/use/cplusplus/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "SolAR Framework architecture",
      "link": "/documentation/framework_architecture/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "get modules",
      "link": "/assemble/get_modules/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "get Samples",
      "link": "/use/get_samples/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Install",
      "link": "/install/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Community",
      "link": "/community/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Create",
      "link": "/create/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Use",
      "link": "/use/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Documentation",
      "link": "/documentation/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Assemble",
      "link": "/assemble/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Home",
      "link": "/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "How to install",
      "link": "/install/installassemble_back/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "install",
      "link": "/install/installcreate_back/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "on linux",
      "link": "/install/linux/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Made with SolAR",
      "link": "/made_with_solAR/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "create module",
      "link": "/create/module/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "module api",
      "link": "/assemble/moduleapi/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "pipeline",
      "link": "/assemble/pipeline/"
    });
  

  

  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Search",
      "link": "/search/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "services",
      "link": "/use/services/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "standalone pipeline",
      "link": "/assemble/standalone_c_plus_plus/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "Tags",
      "link": "/tags/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "module tests",
      "link": "/create/tests/"
    });
  

  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "tools",
      "link": "/use/tools/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "unity",
      "link": "/use/unity/"
    });
  

  
    // do not take into account items that are from subpages (title == null),
    // because we don't know in wich (potentially) multiples page(s) these supages can be included
    
    store.push({
      "title": "on windows",
      "link": "/install/windows/"
    });
  

  

 
function searchAndDisplay(query){
  var resultdiv = $('#results');
  var result = index.search(query);
  // Remove previous results
  resultdiv.empty();
  // Add status
  resultdiv.prepend('<p>Found '+result.length+' result(s) for keyword: '+query+'</p>');
  // Loop through, match, and add results
  result.forEach(function(item) {
    var ref = item.ref;
    var searchitem = '<div class="result"><div class="result-body"><li><a href="'+store[ref].link+'" class="post-title">'+store[ref].title+'</a></li></div>';
    resultdiv.append(searchitem);
  });
}
 
// builds search
$(document).ready(function() {
  var query = (decodeURI(location.search).split('q=')[1] || '').split('&')[0];
  var formattedQuery = query.split('+').join(' ');
  searchAndDisplay(formattedQuery);
 
  $('input#search').val(formattedQuery).on('keyup', function () {
    var query = $(this).val();
    searchAndDisplay(query);
    console.log("inputsearch query="+query);
  });
});